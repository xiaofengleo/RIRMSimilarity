the objectives of envri wp3 task t3 is to examine the design of the esfri environmental research infrastructures ris icos
euro argo eiscat 3d lifewatch epos and emso in order to identify common computational characteristics of them and to develop
an understanding of the specific requirement through observations throughout the study standard model the open distributed processing odp is chosen
to use to interpret the design of the research infrastructures and place their requirements into the odp framework for analysing
the document reports the initial results from this study briefly from the aspect of the odp engineering viewpoint the architectural
characteristics of the ris have been examined and common sub systems have been identified sub systems of data acquisition curation
access processing and community support secondly from the aspect of the odp computational viewpoint we looked at each of the
ris in details and identified the common functions and embedded computations they provided matrices has been used for comparison definitions
of functionalities have been provided finally from the aspect of the odp enterprise viewpoint we have identified common communities and
derived the community roles the contribution of this work to the environmental science research infrastructures is threefold it investigates esfri
ris which is collection of representative research infrastructures for environmental sciences and provides projection of europe wide requirements they have
identifying in particular those they have in common it experiments with odp as an approach in requirement analysis to serve
as common language for interpretation and discussion to ensure unifying understanding the results from this study can be used as
an input to design or implementation model common services can be provided in the light of the common analysis which
can be widely applicable to various environmental research infrastructures and beyond copyright members of the envri collaboration see www envri
eu for details of the envri project and the collaboration envri òcommon operations of environmental research infrastructuresó is project co
funded by the european commission as coordination and support action within the 7th framework programme envri began in october and
will run for years this work is licensed under the creative commons attribution noncommercial license to view copy of this
license visit http creativecommons org licenses by or send letter to creative commons second street suite san francisco california and
usa the work must be attributed by attaching the following reference to the copied elements òcopyright members of the envri
collaboration see www envri eu for details of the envri project and the collaborationó using this document in way and
or for purposes not foreseen in the license requires the prior written permission of the copyright holders the information contained
in this document represents the views of the copyright holders as of the date such views are published this document
is formal deliverable for the european commission applicable to all members of the envri project beneficiaries and joint research unit
members as well as its collaborating projects frontier environmental research increasingly depends on wide range of data and advanced capabilities
to process and analyse them the envri project òcommon operations of environmental research infrastructuresó is collaboration in the esfri environment
cluster with support from ict experts to develop common science components and services for their facilities the results will speed
up the construction of these infrastructures and will allow scientists to use the data and software from each facility to
enable multi disciplinary science the target is on developing common capabilities including software and services of the environmental infrastructure communities
while the envri infrastructures are very diverse they face common challenges including data capture from distributed sensors metadata standardisation management
of high volume data workflow execution and data visualisation the common standards deployable services and tools developed will be adopted
by each infrastructure as it progresses through its construction phase two use cases led by the most mature infrastructures will
focus the development work on separate requirements and solutions for data pre processing of primary data and post processing toward
publishing the project will be based on common reference model created by capturing the semantic resources of each esfri env
infrastructure this model and the development driven by the test bed deployments result in ready to use systems which can
be integrated into the environmental research infrastructures the project puts emphasis on synergy between advanced developments not only among the
infrastructure facilities but also with ict providers and related science initiatives these links will facilitate system deployment and the training
of future researchers and ensure that the inter disciplinary capabilities established here remain sustainable beyond the lifetime of the project
this report presents the initial finding from the study t3 analysis of the requirements of data processing the open distributed
processing odp is used as the framework for the analysis from the aspect of the odp engineering viewpoint the physical
structuring mechanism for the envri research infrastructures are analysed and common sub systems are identified data acquisition data curation data
access data processing and user community support secondly from the aspect of the computational viewpoint set of operations and embedded
computations commonly provided by the infrastructures are identified finally from the aspect of the odp enterprise viewpoint common communities are
identified data acquisition data management data service provision and data user the roles behaviours and policies for each community are
described the objective of this study is to analyse and develop an understanding of the specific requirements of each esfri
environmental env research infrastructure ri with respect to common short term priorities the envri background papers and t3 assessment of
the state of the art provide useful surveys and evaluations of env ris common requirements emerge this study intends to
make further step towards common model for envri the study will not produce common model however it will serve as
an input to such model in this study we use standard approach open distributed processing odp to interpret the design
of representative environmental research infrastructures icos1 epos2 emso3 eiscat 3d4 lifewatch5 and euro argo6 and place their requirements into the
odp framework for further analysis odp is an iso iec standard which provides an overall conceptual framework for building distributed
system it defines five specific viewpoints which are abstractions that yield specifications of the whole system related to particular sets
of concerns the five viewpoints are the enterprise viewpoint which concerns the organisational situation in which design activity is to
take place the information viewpoint which concerns the modelling of the shared information manipulated within the infrastructure of interest the
computational viewpoint which concerns the development of the high level design of the processes and applications supporting the infrastructure the
engineering viewpoint which tackles the problems of diversity in infrastructure provision it gives the prescriptions for supporting the necessary abstract
computational interactions in range of different situations the technology viewpoint which concerns with managing real world constraints such as restrictions
on the facilities available to implement the system to the existing application platforms on which the applications must run the
added value of using odp to analyse the requirements for envri is threefold to use standardised language to interpret the
design of research infrastructures which helps to unify understanding to provide gentler pathway to link real world systems to odp
model world since the odp framework is created to help designers deliver practical architecture which leads to concrete implementations using
odp concepts for requirements analysis can help us to drill down to details and identify essential problems the analysis presented
in this reports are based on partial knowledge of snapshot of current state of env ris this is because the
documentation of ris is often incomplete and inconsistent and the designs evolve over time and subject to change for example
the eiscat 3d design study finished in and submitted the final design as deliverable to the eu commission its succeeding
project the eiscat 3d preparatory phase eiscat 3d pp started in examines the feasibility of the design and prepares for
implementation starting in during the eiscat 3d pp many parts of the design are likely to be re evaluated and
re designed due to infeasibility for implementation this is common issue for most if not all other ris the investigation
of envri should be based on the existing knowledge of the design provided by ris meanwhile keep up to date
with the development of any new activities there is one key issue of how to denote in the odp context
known unknowns we should tolerate schemas descriptions with many of these at first and progressively push the unknowns towards detail
or boundaries later the rest of the report presents initial findings from the study the analysis covers odp viewpoints enterprise
computational and engineering because most of the env ris provide documentation which describes the architectural features of their infrastructure it
is straightforward to start with the engineering viewpoint where we identify the common physical structuring mechanism for the system infrastructures
secondly we identify common functions and embedded computations provided by the env ris this in essence is to analyse the
ris from the aspect of the odp computational viewpoint finally we look at the real world systems from the aspect
of the odp enterprise viewpoint and identify the common communities and their roles matrices are used to visualise the results
of comparison where columns are the names of env ris and rows are odp elements in odp the purpose of
the engineering viewpoint is to identify and specify the structuring mechanisms for distributed interactions and the functional elements it concerns
the architectural features of an infrastructure the structures of the studied ris can be divided into sub systems based on
functions and locations of computational elements for the purposes of this document each sub system is defined as set of
capabilities that collectively are defined by set of interfaces with corresponding operations that can be invoked by other sub systems
an interface in odp is an abstraction of the behaviour of an object that consists of subset of the interactions
of that object together with set of constraints on when they may occur sub systems are disjoint from each other
five common sub systems are identified data acquisition data curation data access data processing and community support the order of
these sub systems is irrelevant the data acquisition sub system collects raw data from sensor arrays various instruments or human
observers and brings the measures data streams into the system note envri is concerned with the computational aspects of an
infrastructure thus by definition the data acquisition sub system starts from the point of sensor signals being converted into digital
values and received by the system there are many related activities including defining data acquisition protocols design and deployment of
the sensor instruments and configuration and calibration devices which are crucial tasks for data acquisition nevertheless beyond the scope of
the envri investigation the data acquisition sub system is typically operated at observatories or stations data in the acquisition sub
system are normally non reproducible the so called raw data or primary data consistent time stamps are assigned to each
data object there are the cases that the raw data may be generated by simulation model in which situation the
raw data may be reproducible in terms of being regenerated the real time data streams sometimes are temporarily stored in
computer clusters then sampled filtered or processed based on applied quality control criteria control software is often provided to allow
the execution and monitoring of data flows the data collected at the data acquisition sub system are transmitted to the
data curation sub system to be maintained and archived there the data curation sub system facilitates quality control and preservation
of scientific data it is typically operated at data centre data handled at the curation sub system are often reproducible
in term of being able to be re processed operations such as data quality verification data identification annotation cataloguing and
long term preservation are often provided various data products are generated and provided for users which need to be accessed
through data access sub system there is usually an emphasis on non functional requirements for data curation sub system including
the need for satisfying performance criteria in availability reliability utility throughput responsiveness security and scalability the data access sub system
enables discovery and retrieval of data housed in data resources managed by data curation sub system data access sub systems
often provide facilities such as data portals as well as services to present or deliver the data products search facilities
including both query based and navigation based searching tools are provided which allow users or services to discover interesting data
products discoveries based on metadata or semantic linkages are most common data handled at the access sub system can be
either structurally and semantically homogeneous or heterogeneous when supporting heterogeneous data different types of data often pulled from variety of
distributed data resources may be converted into uniform representations with uniform semantics which can be resolved by data discovery and
access service services allowing harvesting of metadata and or data as well as services enhancing the performance by compression and
packaging methods and encoding services for secure data transfer are often part of the data access sub system data access
can be open or controlled enforced by authentication and authorisation policies it is notable that data access sub system usually
does not provide write operations for end users although such operations may be provided for an administrator of data resource
the data processing sub system aggregates the data from various resources and provides computational capabilities and capacities for conducting data
analysis and scientific experiments data handled by the data processing sub system are typically derived and recombined via the data
access sub system data processing sub system normally offers operations for statistical and or mining functions for analysis facilities for
conducting scientific experiments modelling simulation and scientific visualisation performance requirements for processing scientific data tend to be concerned more about
scalability issue which may also be necessary to address at the infrastructure level for example to make use of the
grid or cloud technology in this case functionalities to interact with the physical infrastructure should be provided finally the community
support sub system manages controls and tracks users activities and supports users to conduct their roles in communities data handled
by community support sub system typically are user generated data control and communications community support sub system normally supports for
interactive visualisations authentication authorisation and accounting aaa as well as for managing virtual organisations the community support is orthogonal to
and cross cutting the other sub systems there may be other ways to group the functional elements above provides one
possible solution the main purpose for the classification is to identify the common structural characteristics of the environmental research infrastructures
as shown in figure below the five sub systems map well to the architectures of the ris studied as shown
in table different ris emphasise the design and implementation of different sub systems by the time of writing this report
ris such as icos eiscat 3d euro argo and emso mainly focus on data acquisition curation and access they are
typical large scale observatory systems some others ris such as epos and lifewatch are built on existing systems having limited
control over data resources and focus more on data access and processing they are comprehensive integration infrastructures for domain data
and computations it worthy of mentioning that generic computational ris such as eudat and egi are general purpose large scale
infrastructures for data management and processing eudat tends to focus more on the functionalities related to the data curation sub
system and egi tends to focus more on the functionalities related to the data processing sub system both eudat and
egi provide generic operations and services which can be used in various domains of research within either infrastructure the community
support sub system is commonly requested by users of new ris for example in eiscat 3d there is need to
allow users to control remote radar systems to collect their own data both euro argo and icos request the expertise
of users to verify the quality of data emso pangaea system facilitates user to submit metadata whilst epos and lifewatch
plan to allow users to share their data and experiments workflows however currently we observe only few ris actively designing
or implementing this sub system it is likely because of resource limitations egi offers more experience of supporting its user
community operations such as virtual organizations management service voms and accounting are provided through egi infrastructure services however there are
no generic tools or software available to support many newly emerged requirements such as community coordination collaboration policy making and
user collaborative work and publication of results we expect the ris with greater maturity and sufficient resources will lead the
way to explore this area in the near future in the rest of the study we will not examine the
requirements for this sub system intensively for the same sub system different ris provides different facilities in the following we
examine each ri and identify the common computational functions in each sub system the odp computational viewpoint focuses on the
functionality of an infrastructure and the service it offers dividing the structures of ris into sub systems helps to break
down the complexity in analysis within each sub system we use data oriented approach which follows the life cycle of
data creation transmission transformation modification processing and visualisation to identify key functions and embedded computations the analysis is based on
the materials or information from the following sources envri background papers envri deliverable d3 assessment of the state of the
art published papers deliverables white papers and websites of investigated ris and interviews with ris the objective of eiscat 3d
is to design and construct new generation incoherent scatter research radar which provides long term upper atmospheric science capability for
studies of the atmosphere and near earth space the system design of eiscat 3d explored many different areas including the
construction of antennas arrays the signal processing the network and the data distribution system the investigation of envri scopes into
data acquisition processing and archiving aspects of eiscat 3d the design of eiscat 3d data archiving and distribution can be
summarised as follows there is two stage system for handling data the beam formed sample level data together with data
from the interferometry system and some high volume data from supporting instruments are streamed to large ring buffer designed to
hold up to few days after which these low level data will be over written the ring buffer allows the
low level data to be stored for long enough to allow it to be optimally processed in terms of subsequent
auto correlation and integration in time and range the latency time of the buffer must therefore be long enough to
allow multiple processing strategies to be applied before the low level data are over written the final optimally derived data
products which are typically at least an order of magnitude smaller are then transferred to the permanent data archive at
the same time second copy of the incoherent scatter data is separately passed through default signal processing strategy in order
to produce the quick look data needed for control of experiments eiscat 3d will provide visualisation means to present its
data products and system status we consider group of functions that support eiscat 3d to collect raw data as data
acquisition sub system group of functions that support of data storing and archiving as data curation sub system group of
function that support data discovery and deliver to end users as data access sub system in the data acquisition sub
system eiscat 3d collects types of data incoherent scatter data interferometric data and data from supporting instruments which are not
among eiscat 3d main data products we analyse the life cycles of incoherent scatter data and the interferometric data as
follows incoherent scatter data are collected from the antenna elements then beam formed to generate the actual receiver beams the
data stream will be split and the two replicated streams of beam formed data are handled in two different ways
one of the streams will be directed to standard signal processing software or an analysis program to automatically correlate the
data for some data analysis experiments this will largely be an automated process using default time integration strategy so that
each experiment produces continuous stream of auto correlated data at standard time resolution with standard gating strategy the other stream
of sample level beam formed data will flow into cyclic buffer where it can be stored for some finite time
until it is over written interferometric data are set of incoherent scatter data with sub beam width resolution these data
will be delivered to separate interferometry buffer where these data will be continuously tested against the coherency threshold criteria if
the threshold is not exceeded the data will be over written if the coherences on given number of baselines begin
to exceed the threshold the content of the interferometry buffer will be continuously transferred to the central site ring buffer
interferometry data will continue to be transferred to the ring buffer until the coherences have fallen persistently below threshold values
for more than user specified period providing ample time to examine the decay of the scatters in this way the
raw data from the interferometry system can be stored in the central site ring buffer for long enough for it
to be optimally processed into visibility patterns and brightness functions which will then become part of the permanent archive sufficient
metadata will be generated to describe which data sets were used and how the interferometry calculation were carried out provides
full list of functions provided at this sub system the main request is control software to provide the following computational
functions process control scheduler is designed which is control process that takes input from time allocation committee and any geophysical
alert events and determines the current observing programme that should be executed control system generates the low level logic and
drives the hardware and software components that do the actual work the control system is distributed over the various sites
whereas the scheduler is located centrally instrument monitoring data collection parameter visualisation and instrument calibration monitoring system is responsible for
collecting and collating all the auxiliary data that emanates from the eiscat 3d radar system this means all the engineering
data that do not form part of the main data stream but are essential for calibrating and interpreting that data
in addition the data are presented to any operator who is controlling the system instrument access and real time data
visualisation an operator is combination of software and human interaction that drives the radar via the control system and makes
tactical decisions regarding the operation of the radar experiment the operator can see incoming data via the visualisation system in
real time and react appropriately to scientifically interesting events temporary data storing the raw data is stored in the raw
data buffer store for either reprocessing by data selector or for directly dumping it into data archive raw data accept
process accepts commands from the control system and when required extracts data from the raw data buffer store and forwards
it directly to the data archive noise reduction an integrator time integrates the signals and reduce the noise variance and
the total data throughput of the system that reached the data archive here the names of the functions are given
as the abstractions of the requirements described in the eiscat 3d design documents or related information materials the original requirements
from the eiscat 3d design documents are used as examples to illustrate the meanings of the functions the same principle
applies to the rest of the analysis more formal definitions of the functions will be provided in sub section in
the data curation sub system eiscat 3d will archive data delivered from the ring buffer for the long term the
data preservation and distribution component of the overall eiscat 3d system is the one that essentially acts as an ingest
facility with provision for adaptive storage data location management and the ability to reprocess data within certain time limitations the
data handled at this sub system at typical time associated data in file formats the key functions include fault tolerant
data buffering to allow the system to cope with anomalies in the regularity of the data flow for instance to
allow catch up grace period for data which are temporarily disrupted such disruptions in addition to the òstandard operationó disruptions
listed above would also include more major incidents such as rebooting down stream computer the system would provide sufficient latency
to recover with time the computer outage as it would be possible to process the backlog of data faster than
the incoming data rate this would not apply in the case of the interferometric system data replication automated secure remote
backup data preservation the data archive receives scientific data from the receiver signal processing streams and auxiliary data from the
monitoring system and serves it to users and the visualisation system eiscat 3d designs for the following functions for the
data access sub system data discovery the data archive consists of store query process that accepts commands from users and
instructs the data stores to send data to data aggregator the data aggregator merges science and auxiliary data when required
and converts the data into format suitable for transmission to the user data access throughput intensive mode of operation for
access of data in file formats in high volumes is requested data visualisation eiscat 3d will provide facilities for visualisation
of metadata for event search or maintenance purposes access control secure access for users data conversion for experimental purposes the
low level format raw bit streams would need to be convert to more usable format and transferred to conventional media
for example automatic detection and excision of spurious signals and the automatic recognition of scientifically interesting data may occur before
post integration eiscat 3d will provide facilities for data processing the focus is on visualisation and the functions to be
provided include data visualisation and scientific visualisation real time visualisation of analysed data will be provided with figure of updating
panels showing electron density temperatures and ion velocity to those data for each beam in addition non real time post
experiment visualisation of the physical parameters of interest will also be provided by standard plots using three dimensional block to
show to spatial variation in the user selected cuts using animations to show the temporal variation allow the visualisation of
or higher dimensional data using the cut up and stack technique to reduce the dimensionality that is take one or
more independent coordinates as discrete or volume rendering technique to display 2d projection of 3d discretely sampled data set to
support its community eiscat 3d will provide the following functionality interactive visualisation eiscat 3d wish to allow users to combine
the information on several spectral features by using colour coding and to provide real time visualisation facility to allow the
users to link or plug in tailor made data visualisation functions and more importantly functions to signal for special observational
conditions eiscat 3d introduces significant challenges in data handling that how to cope with large scale data in real time
the requirements are documented in detail and solutions are proposed however the system is not yet implemented and there are
many uncertainties in realisation in the next we will look at more mature system euro argo analysis of euro argo
argo is global ocean observing system comprising of large network of robotic floats distributed across the world oceans and supporting
infrastructure it is unique system to monitor heat and salt transport and storage ocean circulation and global overturning changes and
to understand the ability of the ocean to absorb excess carbon dioxide from the atmosphere euro argo6 is the european
contribution to argo as an european infrastructure the objectives of the new euro argo research infrastructure include to provide deploy
and operate an array of around floats contributing to the global array european contribution of of the global array to
provide enhanced coverage in the european regional seas and to provide quality controlled data and access to the data sets
and data products to the research climate and oceanography and operational oceanography gmes marine core service communities the robotic floats
are operated as follows after being released floats dive to programmable depth currently metres drifting freely in currents every days
float dives to metres then rises to the surface to send data by satellite link more than cycles can be
performed during the float year lifespan the data collected by argo include heat salt transport storage ocean circulation and global
overturning changes in order to understand amongst other things the ocean absorption of excess carbon dioxide the life cycle of
argo data are as follows the national data assembly centres dacs receives data from satellite operators decode and perform quality
control according to set of real time automatic tests erroneous data are flagged corrected if possible and then passed to
the global data assembly centres gdac and to the world meteorological office global telecommunication system gts the gdac located at
coriolis france and usgodae usa collect data from the dacs and provide unique access both in real time within 48hrs
after transmission and delayed mode months after transmission data available in netcdf format in ftp and internet the gdacs synchronise
every day gdacs also deliver data to several argo regional centres arcs where the expertise on specific geographical ocean regions
will provide comprehensive data sets including non argo data data from gdacs will be long term archived at data centre
located in nodc us the architecture of euro argo is depicted in figure we consider group of functions that supports
the dacs to collect the raw data from the floats and standardise the collection process as data acquisition sub system
group of functions that supports the gdacs to check the data quality and to archive the data as data curation
sub system and group of functions that support data distribution and access as data access sub system euro argo provides
limited functions for data processing and community support it links with external systems such as myocean7 an ocean monitoring and
forecasting system which provides products and services for all marine applications and the seadatanet8 pan european infrastructure for ocean marine
data management to provide such functionalities in the data acquisition sub system euro argo includes the following functions data collection
to receive data from satellite data conversion to convert the standard exchange formats noise reduction to apply standardised real time
quality control data transmission delivering data through ftp to the gts and gdacs within 24hrs of surfacing and to principle
investigators pis on more relaxed schedule process control to allow the coordination of argo data handling for the floats under
their control in the data curation sub system the gdacs keep the master copies of the argo global dataset metadata
profiles trajectories and technical information the data are long term archived at the nodc centre in us the key functions
provided include replica synchronisation storages of the gdacs centres are synchronised everyday data preservation nodc will long term archive all
argo data data quality verification the argo regional data centres receive data from the gdacs and look at data from
ocean basins to verify the consistency of float data and generate products feedback will send to pis data product generation
comprehensive data sets including non argo data are provided by the argo regional data centres in the data access sub
system euro argo provides the following functions data publication gdacs distributes data to users the argo information centre the nodc
archive centre and the rdacs via ftp and the internet the euro argo system is relatively mature and capable of
supporting the whole life cycle of argo data from acquisition to preservation however only necessary and basic operations are provided
many processes have not yet been automated and comprehensive functionalities in particular for data access and data processing have not
yet been considered compared to euro argo icos offers more implementation experiences in such functions icos1 the integrated carbon observing
system is world class research infrastructure to quantify and understand greenhouse gas fluxes the objectives of icos community are to
monitor greenhouse gases ghg over the long term through atmospheric ecosystem and ocean networks headquarters which coordinates the research infrastructure
at the european level this mainly involves human activities the icos network of atmospheric ecosystem and ocean observation sites which
include atmospheric ecosystem and ocean stations central analytical laboratory for calibration and air samples analyses including radiocarbon for the entire
network an atmospheric thematic centre atc which is responsible for the coordination of atmospheric measurements instrument development servicing and online
data processing an ecosystem thematic centre etc which is responsible for total ecosystem flux measurements and component fluxes and carbon
pools including data processing and instrument development and an ocean thematic centre otc which is responsible for coordinating continuous marine
observations initial data processing from marine network we consider group of computational functions that facilitates the collections of the observations
of greenhouse gases ghg from the hundred plus stations of icos atmospheric ecosystem and ocean networks as data acquisition sub
system the design describes the requirements for control software which offers the following functions instrument access to provide the access
and control of the stations locally and remotely parameter visualisation to display the parameters and measured variables in real time
instrument configuration to allow the configuration of the stations instrument configuration logging and instrument monitoring to log the state of
the station configuration and possible problems warnings and alarms and to implement the security routines process control to include central
sequencer for all stations instruments and to allow performing action on all controllable components of the stations to permit the
execution of measurement sequences and to allow the handling of the data flow and transfer data collection and instrument calibration
to automate the operation such as measurement and calibration message handling to allow the handling of the intra and extra
station communication instrument integration to permit the addition of further instruments to the station in the future data transmission and
data transmission monitoring to allow transferring data from all icos stations in real time protocol will be set up to
check for the correct transfer of the expected daily data files automatic emails will be sent to the stations principle
investigators pis in case of problem in the data transmission gathering process noise reduction quality check will be applied based
on the information given by each instrument temperature flow rates and on statistical algorithm to identify suspicious signals we consider
group functions that support the icos three thematic centres atc etc and otc to receive observations from stations check data
quality and to archive data as the data curation sub system icos designed or implemented the following functions in this
sub system data quality verification interactive tools are provided to the station pis in order to flag the data which
correspond to malfunctions of the instruments or to local contaminations data preservation the raw data received from stations will be
archived metadata about the sites the variables and the instruments is provided when archiving data the data archiving will be
dynamic complete and robust over time years to allow for an automatic reprocessing of the whole dataset for instance when
primary scale changes will occur every years on average data product generation based on level raw data atc will produce
level data which are expressed in geophysical units and usable by modellers to calculate fluxes and level data which are
in time series data versioning which tracks changes in data and preserves different versions of data products level data products
will be organised into versions resulting from the regular re processing of level data raw data data identification the implementation
of dataset id system for icos data such as digital object identifier doi is important for tracking and referencing data
resources http datacite org attaching doi to data set will be achieved by òfreezingó the database at regular intervals at
least annually and feeding data streams to the icsu world data system group of functions which supports the publication and
access of icos data products is considered as data access sub system icos designs for carbon portal to distribute its
data products for example the atc has implemented the following functions in its web portal9 icos web portal https icos
atc demo lsce ipsl fr data visualisation graphic products are provided including the time series data plots of the measurements
and daily automated diagnostic plots for station and instruments monitoring simple php browser with thumbnail capacity is available data publication
the atc provides on request pipes and hourly carbon dioxide or methane measurements are made available by sftp server or
https server the carbon portal will also support discovery and integration of the data from its thematic centres and the
following functionalities are planned data annotation and data publication interface for metadata description and data release publications and web service
for external data access data discovery the carbon portal will generate and provide effective tools to discover find extract and
collocate observations according to user needs semantic harmonisation and data conversion the harmonisation of data and metadata standards together with
graphical formats and links to new products will be coordinated between the tcs the carbon portal and the station principal
investigators the carbon portal will offer different options to meet user needs with online offline automatic conversion tools we consider
group of computational functions that supports analysing and mining of icos data as data processing sub system icos will provide
the following functions through its carbon portal data mining and data extraction icos carbon portal will provide interfaces for data
mining and data extraction scientific modelling icos will provide level data the generation of level data is research process and
it is desirable in this process that several models are applied to the same level or level data in order
to obtain ensemble estimates of level data using models prescribed with different parameter values and or of different structure this
is similar to ipccõs combination of results from several climate models forced by the same scenario hence giving range of
uncertainty on modelõs projections data assimilation icos will provide access to quality controlled long term observational data and data products
for data assimilation and modelling icos designs the following tools functions to be provided by the carbon portal to support
the icos user community this group of functions can be considered as community support sub system data citation tracking processed
and quality controlled dataset offered via the carbon portal will be frozen on yearly or semi annual basis and be
published in specialized geosciences data journals citation and referencing of such papers will offer simple bibliometric means of tracing and
measuring the data usage through its referencing in the scientific literature the carbon portal will keep track of published scientific
papers using icos data and provides links to them any other outcome of the use of the icos data will
also be documented on the carbon portal relevant information on data usage and icos visibility will be collected authentication the
carbon portal will develop an overarching registration system for all icos data streams and implement for itself and the thematic
centres single id concepts openid or geo id that allow logging in with one id at multiple sites the log
in procedure should be minimally intrusive and require only the acceptance of the data use policy license indication of purpose
and affiliation data description publication the carbon portal will coordinate with the thematic centres on the peer reviewed publication of
descriptions of the ensemble of the databases this publication ensures bibliometric recognition of principal investigatorsõ and thematic centresõ work besides
the atmospheric thematic centre icos is constructing the similar information systems for the other two thematic centres however in long
term icos encounters the challenges of aggregating and integrating data across the thematic centres and to conduct scientific analysis and
experiments upon the integrated data in such areas emso is step ahead analysis of emso emso3 european multidisciplinary seafloor observatory
is european network of sea floor observatories for the long term monitoring of environmental processes related to ecosystems climate change
and geo hazards the objectives of emso community are to ensure the technological and scientific framework for the investigation of
the environmental processes related to the interaction between the geosphere biosphere and hydrosphere and for sustainable management by long term
monitoring also with real time data transmission emso observatories will include common set of sensors for basic measurements and further
sensors for specific purposes defined by users the common set of instruments comprises seismometers hydrophones for geophysics magnetometers gravity meters
ctd conductivity temperature and depth current meters chemical sensors pressure sensors and hydrophones for bio acoustic monitoring additionally laboratory studies
are performed on material collected at these sites by sampling devices water samplers sediment cores traps etc the following activities
are carried out at emso individual observatories they are likely supported by computational facilities in order to design measurements and
monitoring models based on geographical location scientific requirements operational requirements and available resources develop and test the sensor detectors deploy
the instruments into selected locations of the deep sea adapt the infrastructure to new deployed instruments emso eric scientific and
technological advisory board and the executive board will establish general and detailed requirements and standard in order to fulfil both
cabled node and stand alone node integration into unique research infrastructure recover and reset the sensors update with new technology
using cabled observatory send data from sensors to surface buoys boats to satellites then forward to shore stations emso data
collected in experiments at regional sites are locally stored and organized in catalogues or relational database and run by the
institutions involved some of emso observatories data from some distributed sites are harvested and long term archived at data archives
ifremer eurosites10 unihb pangaea11 and ingv moist12 central archive hosting web service access to all the databases is planned for
the near future we consider group of functions provided by moist pangea and eurosites that support data quality control and
preservation as data curation sub system key functions include but are not limited to data identification assigning persistent identifiers emso
partially already assigns dois to some of its data sets from the hausgarten site and will use dois for further
data products data cataloguing emso collects metadata on both the physical sensors and observatories as well as on the data
observatories are intended to be described by sensorml metadata is provided by the regional nodes of emso moist12 is data
management system for multi parametric observatories aiming at hosting multidisciplinary data and metadata the core part is the database that
indexes data and keep track of the data source moist supports emso sites in organising indexing and transforming data into
compatible data scheme moist is developed to adopt the most common standards ogc nasa inspire for organising its information system
it offers access to data in nasa dif and dublin core via oai pmh interface as well as an opensearch
interface the information system pangaea offers variety of services for scientific project data management long term data archiving and data
publication pangaea provides access to metadata in several formats such as dc nasa dif iso19139 or darwincore access to metadata
is provided via oai pmh interface opensearch as well as digir ifremer offers access to several emso sites via their
eurosites data management system which offers access to data in netcdf format via ftp emso has implemented prototype common data
catalogue13 which uses the opensource panfmp14 software to harvest and index the metadata records pangaea and moist metadata are harvested
via their oai pmh interfaces while for ifremer an additional service has been implemented which extracts metadata from the netcdf
records which then are harvested via http the emso common data catalogue offers common opensearch interface as well as metadata
transformation service which offers metadata in dclite4g format compliant with the genesi requirements used for the envri opensearch client data
preservation in pangaea curator is responsible for the data archiving and publication15 moist will adopt reference model developed inside scidip
es ec project the pangaea data library and publisher retrieves data from emso resources and make them publically accessible we
consider group of functions that facilitates the publication and access of emso data as data access sub system this sub
system includes the following functions data conversion pangaea provides the following tools software pan2applic16 which converts files or folders of
files ascii tab separated data files with or without metaheader downloaded from pangaea via the search engine or the data
warehouse to formats as used by applications for visualization or further processing pantool17 which is used for data conversion and
recalculation written to harmonize individual data collections to standard import format used by pangaea split2events18 which splits one file with
data from several events into several files one for each event pangaea as well as moist are planning to provide
netcdf transformation services moist provides the following tools software moist plot multi parametric plot in an interactive area of an
online web page for first immediate data analysis by the user before download selected data data publication pangaea offers doi
resolution service and makes internally use of doi registration service to register the doi metadata records at datacite in cooperation
with publishers such as elsevier pangaea provides cross linking service which allows ôreciprocal linkingõ automatically linking research data sets deposited
at pangaea to corresponding articles in elsevier journals on its electronic platform sciencedirect and vice versa data import data import
tools22 are provided by pangaea data discovery google like advanced metadata discovery is provided for public access23 also the common
emso data catalogue and data portal data citation pangaea supports for data citation24 each data point is fully citable with
doi and can be cross reference with journal articles it also supports of pre publication peer review process and support
of data citation quality verification pangaea data policy25 describes the quality assurance for data submission qc procedures are maintained within
the pangaea data curatorial process26 during which quality flags can be assigned to indicate the quality of each measurement metadata
harvesting pangaea oai pmh for esonet data in emso sites harvesting test integration into envri metadata catalogue etc pangaea georss
embedding georss feed ifremer sos sensor observation service for eurosites oceanographic data in emso sites getcapabilities getobservation check format pangaea
sos for ingv data in emso sites via moist moist rm ingv it getcapabilities getobservation check format moist opensearch for
ingv data and metadata in emso sites data and metadata search according to time or space or parameter common netcdf
metadata extraction and transformation service moist oai pmh for harvesting ingv data and metadata in emso sites identifier registration catalogue
of emso dois is being planned in order to register emso dois metadata registration centralised metadata catalogue to store metadata
harvested from distributed sites is implemented within the prototype emso data catalogue and data portal see above sensor registration emso
aims to implement core standards of the open geospatial consortium ogc sensor web enablement swe suite of standards namely the
ogc standards sensorml sensor registry catalogue service for web cs sensor observation service sos and observations and measurements sensor registry
is available at esonet27 we consider group of functions that support emso users to conducts various tasks as community support
sub system we identified the following functions accounting the statistics of the portal accesses is planned to replace the user
registration which can track resource consumption by users for the purpose of capacity and trend analysis metadata submission pangaea provides
online metadata data submission service28 which supports metadata standards such as iso19115 dublin core dif darwincore and datacite metadata curation
editor pangaea also provides online curation editor29 to be used by curators for the administration of metadata and the import
of data event notification real time access of the sensor data to identify new phenomena and events occurring to provide
geo hazard warning emso provides advanced technology in data publication and citation through the pangaea system emso also offers capabilities
for data access standardisation harmonisation and visualisation via moist data infrastructure presently in dec regional sites data are integrated in
moist and one regional site is integrated in pangaea which additionally offers data from several related or preparatory studies for
other emso sites in addition ifremer offers access to data from all eurosites sites which are shared with emso emso
has integrated all its operational sites within common data portal in the next step emso plans to continue to harmonize
its vocabularies and terminologies according to seadatanet standards and aims to offer access to data via common netcdf format which
is compliant with seadatanet further emso plans to improve standardised access to real time data via sos in the next
we look at epos which has special emphasis on the integration and interoperability problem and tackles the problem by new
infrastructure design analysis of epos epos2 the european plate observing system is research infrastructure and science for data and observatories
on earthquakes volcanoes surface dynamics and tectonics the objectives of epos community are to integrate the existing research infrastructures ris
in solid earth science in order to increase the accessibility and usability of multidisciplinary data from seismic and geodetic monitoring
networks volcano observatories laboratory experiments and computational simulations epos aims to enhance worldwide interoperability in earth science by establishing leading
integrated european infrastructure and services since only the seismic network of epos is relatively mature when writing this report in
dec the following analysis is limited to the requirements of this discipline epos focuses on integration and interoperability of existing
earth science systems it does not actively design or implement functionalities for data acquisition and curation such functionalities are already
available in the existing systems for example the real time seismic waveform data from more than broadband stations in europe
are collected by the virtual european broadband seismograph network vebsn using seismic data acquisition systems such as antelope seiscomp seedlink
and scream number of data centres such as orfeus and emsc respond to data quality control and archiving data are
archived using archive protocols arclink and mseed2dmc all data is openly available to the research community through variety of means
such as web services direct access and interactive tools in the long term the data will be preserved via eudat
nodes using grid data technology such as irods which store and replicate the data providing also unique and persistent id
pid to data granules through federated handle systems we consider group of computational functions provided by vebsn to support data
collection as data acquisition sub system the key functions include real time data collection the vebsn facilitates real time collection
locations and quantifications of important seismic events waveform data real time data transmission vebsn data are exported in real time
by seedlink protocol vebsn facilitates rapid centralized data exchange between european observatories and the iris dmc data centre real time
data extraction the vebsn also provides rapid accurate locations of large to medium sized earthquakes complete with automatic picks and
magnitude estimates to the emsc and observatories in the region emsc combines the vebsn picks with additional data picks to
produce an improved location and magnitude estimate with certain delay the event waveform data are also used for routine rapid
moment tensor determinations we consider group of computational functions provided by orfeus to support data quality control and archiving as
data curation sub system the key functions include quality checking orfeus processes raw data according to various quality parameters the
continuous seismic waveform data from the virtual european broadband seismograph network vebsn are monitored at orfeus data center odc in
various ways to ensure high quality of waveform data and metadata the following monitors are in place near real time
monitor of the power spectral density psd versus time for selected frequencies network latency monitor monitors latency per network and
days history near real time monitor of the psd through probability density function pdf histograms for magnitude time residuals and
qc plots data identification assignment of persistent identifiers pids to data collections data cataloguing to associate metadata to data collection
and update metadata when associated data collections change data preservation repositories for observational and experimental raw data pre processed data
products and modelling simulation data including the respective metadata data replication epos distinguishes safe replication which supports bit stream preservation
optimal data curation and accessibility data staging dynamic replication which is the replication of data between storage resources and hpc
staging areas workflow enactment to automate set of operations provenance tracking to track data sources and processes non functional requirements
emphasise on performance aspects including security consistency productivity responsibility reliability accessibility availability scalability and load balance eida serves in the
epos data infrastructure as consortium of waveform data centres that share common agreement on issues related to data formats metadata
transfer protocols and interfaces within the consortium we consider group of functions provided by the eida data centre which supports
of data exchange and discovery as data access sub system the technical architecture consists of the arclink middleware which is
installed at each node of the consortium each node synchronises its network station location channel metadata everyday on top of
arclink each node has built its own infrastructure to exchange the waveform data within the consortium through the tcp ip
protocol this presents peer to peer communication federated security is being planned for each individual institution within epos so that
each institute can maintain its own security infrastructure but single sign on process is desired probably making use of some
combination of x509 certificates shibboleth and ldap in order to make an apparently seamless aai authentication and authorisation infrastructure to
summarise the functions and embedded computations provided by the data access sub system include data publication data and metadata associated
are made publically accessible by data centres web services which provide access by stand alone clients to download bulk data
using command line batch systems mail based data request services such as netdc brequfast and autodrm data transmission network protocols
for moving large or small amounts of data and for moving real time or non real time data data discovery
arclink technology that is used to query server for seismological data in certain time window and region which can handle
requests of metadata waveform quality control and routing access control externally expert users will likely be permitted to interact with
resources via the command line using standard grid credentials for data processing epos data centres such as orfeus have established
long lasting tradition for data analysis and mining orfeus maintains repository of software tools for specific interest to the seismological
community with emphasis on free software the required functions for data processing mainly include the following areas data conversion to
convert waveform formats data analysis for routine analysis of signals or hazard analysis data visualisation statistical plots scientific simulation and
scientific modelling provide simulation and modelling of solid stress and strain due to elastic static response to an earthquake geomechanic
modelling and wave propagation modelling scientific visualisation visualisation and analysis of seismograms the functionality provided by epos to support its
community which can be considered as community support sub system such functionality includes but is not limited to authentication federated
security mechanism by using shibboleth based aai or openid for web based access interactive visualisation orfeus provides the following stand
alone services via web clients event selection from catalogues based on user defined regions and or magnitude thresholds such as
wilber ii stream selection on network station and channel level based on geographical region and epicentral distance such as owi
direct and automatic waveform harvesting seed from eida time window adjustment using configurable phase arrival times event notification the emsc
operates an earthquake notification service30 which implements software named qwids quake watch information distribution system to provide quick and robust
data exchange system through permanent tcp connections qwids disseminates email sms fax to the registered end users within minutes on
average after the earthquake occurrence epos designs for new generation earth science system by applying the most advanced science technologies
on existing well developed seismic and other earth science systems however the project is still in its early stages and
design work is not yet completed in the next section we will look at lifewatch which addresses similar challenges to
epos in many areas and has provided some solutions through its own design study lifewatch5 is an science and technology
infrastructure for biodiversity and ecosystem research to support the scientific community and other users in the public commercial and policy
sectors the main objective of lifewatch is to put in place the essential infrastructure and information systems necessary to provide
an analytical platform for the use of both existing and new data on biodiversity different from an observatory system such
as eiscat 3d or euro argo lifewatch is an comprehensive integration infrastructure for domain specific scientific data and computation the
emphasis is on network of services providing secure access across multiple organisations to biodiversity and related data and to relevant
analytical and modelling tools to collaborative groups of researchers the guidelines for the specification and implementation of the lifewatch ict
infrastructure is given by the lifewatch reference model which is built on the orchestra reference model an architectural framework for
distributed processing and geospatial computing which itself is based on odp the lifewathc reference model describes the lifewatch architecture which
consists of function domains as shown in figure they the resource layer the infrastructure layer the composition layer and the
user layer the resource layer contains the data from sites and collections but also contains catalogue services analysis tools and
processing resources that already exist at external networks the infrastructure layer provides mechanisms for uniform access and integration of heterogeneous
resources in the resource layer functional components in the lifewatch infrastructure layer are implemented as services the composition layer provides
the tools for intelligent selection and orchestration of services including workflows semantic metadata for the discovery of components and the
storage of additional attributes such as provenance and version information and the user layer provides domain specific presentation environments and
tools for community collaborations which is generic portal with extended domain and application specific portlets with the data acquisition sub
system being absent the functional domains defined in lifewatch are approximate to the common sub systems identified in envri the
mapping is provided in table provides list of services to be provided by lifewatch unfortunately the mapping between these services
to the lifewatch architectural layers are missing from the document for the purpose of analysis we examine the specification of
each service and distribute them into the appropriate sub systems in the data curation sub system the lifewatch consider those
data processing tools and instruments managed by multiple organisations and in general lifewatch cannot dictate their location or configuration the
main function provided from within lifewatch is group of source integration services service delegation which provide an encapsulation of external
resources to be used by the infrastructure in the data access sub system lifewatch provides the following functions for data
discovery and access in particular upon the heterogeneous resources resources annotation an annotation service which automatically generation of specific meta
information from various sources and relation with semantic descriptions data publication catalogue service which supports the ability to publish query
and retrieve descriptive information for resources data independent of specific meta information standard data discovery and data retrieval feature access
service which allows interoperable read and write access on feature instances available in an service network document access service which
is specialization of the feature access service supporting access without manipulation to documents of any type taxonomy access service which
is specialization of feature access service that allows to read and write taxonomic information provenance service which is specialization of
the feature access service for accessing provenance data generated through annotation services among others and data discovery sensor access service
which is specialization of the feature access service for accessing sensor data configuring sensor and publishing sensor data semantic harmonisation
thesaurus access service which provides read and write access to multi lingual thesaurus for the vocabulary used on service network
ontology access service which provides read access to the specification of logical ontology and export or import of complete specifications
into an ontology store and schema mapping service which is the mapping of features into target schema through the transformation
of each data instance from one data structure into another one preserving the original meaning data conversion format conversion service
which allows the conversion of data given in one format to the corresponding data given in another format coordinate operation
service which changes coordinates from feature locations from one coordinate reference system into another and calendar service which provides the
transformation comparison and arithmetical operations on data time functions data compression compression service which performs data compression the services provided
at the data processing sub system include service naming name service which encapsulates the implemented name policy for service instances
in service network scientific workflow enactment process service chain access service which supports the creation of an executable service instance
based on an explicit description of service chain and workflow enactment service which is specialization of processing service that allows
the execution and monitoring of workflow or service chain data processing processing service which is common interface for services offering
processing operations by initiating the calculation and managing the outputs to be returned to the client process monitoring service monitoring
service which provides an overview about service instances currently running within service network scientific modelling modelling services which is specialization
of processing services allows the user to discover specify input for and control execution of variety of models for simulation
scientific visualisation portrayal service map and diagram service which visualizes symbolizes and enables geographic clients to interactively visualise geographic and
statistic data by providing graphical representation of the data data association geolinking service which allows establishing virtual join between data
having spatial location and data without spatial location but referring to the same feature through common properties geocoder service which
allows adding geographic information to address data and gazetteer service which allows to relate geographic location instance identified by geographic
names with an instance identified by coordinates data extraction generalisation service which allows to create spatial temporal and other generalisation
of features according to given hierarchy and an occurrence distribution service which allows creating distribution maps from particular specie or
specie group authentication an authentication service which verifies genuineness of principals using set of given credentials authorisation an authorisation service
which gives compliance value as response to given authorisation context user registration user management service which creates and maintain subjects
including groups of principals as entities that need authentication instant messaging communication service which provides the harmonized access to direct
user to user communication means based on multi media technology and data exchange between users like chat teleconference sms template
generation reporting service which creates reports using actual information from other services according to template wrapper interface for existing products
data editor an interpolation service which allows interpolation of spatial locations event notification notification service which allows the sending of
messages to client which previously has been registered to listen for certain events lifewatch investigated the possibility of integrating various
state of the art standardised technologies to provide generic services and operations to support biodiversity research this on the other
hand results in high level of abstractions of the design which is likely to introduce difficulties in interpretation and realisation
to summarise the above observations table lists functions and embedded computations provided by the existing research infrastructures each function is
defined as an interface which encapsulates set of required operations or services that act upon an object recall the definition
of an object in odp which is model of real world entity characterised by its behaviour and its state the
interactions that occur between those objects at their interfaces lacking of sufficient resources the analysis here is very brief we
leave the unfilled spaces for future explorations the goal of this investigation was to identify the common requirements of the
env ris throughout the study odp has been used as the analysis framework which serves as uniform platform for interpretation
and discussion to ensure unified understanding from the aspect of the odp engineering viewpoint the architectural characteristics of the ris
have been examined and common sub systems have been identified sub systems of data acquisition curation access processing and community
support secondly from the aspect of the odp computational viewpoint we looked at each of the ris in details and
identified the common functions and embedded computations they provided matrices has been used for comparison definitions of functionalities have been
provided finally from the aspect of the odp enterprise viewpoint we have identified common communities and derived the community roles
the results from this study can be delivered as an input to design or an implementation model common services can
be provided in the light of the common analysis which can be widely applicable to various environmental research infrastructures there
are several elements which could be extended in future work due to time limitation only odp viewpoints have been explored
in future work analysis from the aspect of the odp information viewpoint can be conducted where the requirements for common
information model can be investigated and from the odp technology viewpoint shared technologies can be identified the analysis from the
aspect of odp enterprise viewpoint has only examined the communities and their roles it will be useful to further derive
the community behaviours in term of activity processes and to address community policy issues more substantially how the findings from
this study can be better applied to support env ris and other environmental research infrastructures is still an open question
much help and resources have been obtained from the following people and projects great thanks to them esa turunen anders
tjulin and ingemar haggstrom from eiscat 3d project thierry carval from euro argo project me tarniewicz from icos project robert
huber from pangaea project laura beranzoli from emso project alex hardisty nicola fiore and giuseppe corriero from lifewatch project and
malgorzata krakowian and gergely sipos from egi project
