submission dissemination of relevant material etc tools like indico are currently popular website the purpose of the website is to
disseminate community relevant information to all stakeholders the website should not contain reserved material but only publicly accessible material documents
and presentations for external or internal stakeholders images for press review the website should also include news and interactions from
social networks the website should be simple enough to allow almost anyone with basic it skill to add pages articles
images simple cms content management system is the most reasonable solution wordpress joomla teleconferencing tools communication with all stakeholders internal
and external is also carried on through teleconferencing for this purpose good quality tools screen sharing multi user document exchange
private chat etc are needed popular tools include adobe connect web ex gotomeeting google hangout and skype helpdesk technical support
for example the data products that icos produces are complex and often require experience of and detailed knowledge about the
underlying methods and science to be used in an optimal way technical support must be available to solve any problem
the icos thematic centres for atmosphere ecosystems and ocean are ready to provide information and guidance for data users if
needed requests for information may also be forwarded to the individual observation stations the mission of icos also comprises responsibility
to support producers of derived products typically research groups performing advanced modelling of greenhouse gas budgets by providing custom formatted
data packages as final remark at the moment it is difficult to find pre existing software package with the aforementioned
features on the other hand it would be better to re use tools that community members are familiar with or
are already offered by other infrastructures the best approaches could be to provide toolkit available for ris dataone57 or to
manually build an internal environment with single sign on which gives access to bundle of tools but this second option
would need strong efforts in community uptake appropriation and maintenance non functional requirements the non functional requirements of the ris
that were most frequently referred to were performance ris need robust fast reacting systems which offer security and privacy moreover
they need good performance for high data volumes data policy and licensing constraints the data produced by some communities has
licensing constraints that restrict access to certain group of users for example while icos will not require its users to
register in order to use the data portal or to access and download data it plans to offer an enhanced
usage experience to registered users this will include automatic notifications of updates of already downloaded datasets access to additional tools
and the possibility to save personalised searches and favourites in workspace associated with user profile everyone who wishes to download
icos data products must also acknowledge the icos data policy and data licensing agreement58 registered users may do so once
while others must repeat this step every time training training activities within envriplus communities can be categorised as follows no
training plan the majority of envriplus ri communities do not have common training plan at the moment no community wide
training activities for example within sios many organisations have their own training activities training is provided to students or scientists
for example the university centre in svalbard unis has its own high quality training programme on arctic field security how
to operate safely in an extreme cold climate and in accordance with environmental regulations for students and scientists within actris
each community has its own set of customised training plans courses and documentation are made available online for example for
training on how to use the data products their preferred methods for delivering training are through the community website or
through targeted sessions during community specific workshops actris also considers organising webinars icos does not have common training plan at
the moment the carbon portal organises occasional training events on alfresco dms the document management system used by icos ri
the different thematic centres periodically organise training for their respective staff and in some cases also for data providers station
pis icos also co organises and or participates in summer schools and workshops aimed at graduate students and post docs
in the relevant fields of greenhouse gas observational techniques and data evaluation representatives of icos have participated in training events
organised by eudat on pid usage and data storage technology the method of delivering training through one or two day
face to face workshops concentrated on given topic and with focus on hands on activities is probably the most effective
this should also be backed up by webinars including recordings from the workshops and written materials community training plan is
under development number of communities are in the process of developing community training plan for example lter plans the development
of community training plan within lter europe59 the expert panel on information management is used to exchange information on personal
level and to guide developments such as deims to cater for user needs lter europe also provides dissemination and training
activities to selected user groups training activities will enhance the quality of the data provided by applying standardised data quality
control procedures for defined data sets for epos training is part of its communication plan an advanced system is in
place for training activities within is enes2 workshops are organised from time to time also communities communicate about the availability
of training courses and workshops organised by hpc centres or european projects prace egi etc within embrc training web portal
is provided offering support to training organisers to advertise and organise courses review of technologies technology review methods task is
also involved in performing review of the state of the art technologies provided by data and computational infrastructures the technology
review has two important purposes informing the other tasks in theme including the six pillars supporting the data lifecycle the
three cross cutting topics to make them work together and the provision of computational resources on which the envisaged services
and systems will run see figure for their contribution to envriplus and their relationships this will ensure that those working
on data infrastructure tasks in envriplus work packages or use cases will have access to up to date and relevant
information they would still be well advised to refresh this information with close focus on the work they are undertaking
inevitably the technology review is broader analysis than they will require and as technology in this context evolves rapidly an
update is always wise advising the ris in envriplus when they decide to implement or upgrade their infrastructures again the
information gathered here and in the corresponding wiki60 has long standing value as review of the issues to be considered
in each context and current set of entry points to sources of information this should be revisited and re analysed
focussing on the specific technological issues an ri or group of ris are considering this will also refresh the information
as the available solutions may have changed dramatically these technology review results are publicly available and publicly updateable to contribute
the information to others addressing similar issues and to act as virtual whiteboard where those with good solutions can contribute
evidence of their value figure six pillars and crosscutting mechanisms to make them work together61 start in the direction of
considering and discussing current relevant technological trends was made by envriplus through the organisation of the it4ris workshop in conjunction
with the ieee science conference munich in september they contain key inputs and initial insights from an international cohort of
experts in planning the survey of candidate technologies it was agreed to partition them to match the six pillars of
theme and three crosscutting topics for each area the open issues should be clarified and candidate solutions should be investigated
and evaluated the following is the full list of topics data identification and citation section data curation including quality control
section data method process and resource cataloguing section data processing including transformations for data integration section provenance including tools and
mechanisms exploiting provenance section optimisation of data handling to reduce human effort or resource use section architecture including mechanisms to
meet non functional requirements section semantic linking models section envri reference model with extensions to ontologies section provision of compute
storage and network resources platforms section for each topic team was formed to identify issues identify relevant information sources investigate
these as far as time permitted and to develop discuss and refine the material to be made available via the
wiki and as snapshot in this report each team has leader and at least one independent member to ensure breadth
table outlines the contributors for each topic table contributors to the technology review per topic topic topic leader independent member
identification and citation margareta hellström alex vermeulen and ari asmi curation keith jeffery data curation centre and rda metadata group
cataloguing thomas loubrieu gergely sipos alex hardisty and malcolm atkinson processing leonardo candela rosa filgueira optimisation paul martin zhiming zhao
provenance barbara magagna malcolm atkinson margareta hellström and alessandro spinuso architecture keith jeffery malcolm atkinson alex hardisty linking model paul
martin zhiming zhao reference model alex hardisty keith jeffery markus stocker and abraham nieva provision of compute storage and networking
yin chen damien lecarpentier as for requirements gathering wiki space was specifically created for the purpose of the technology review63
page of candidate technologies64 was developed where members of the teams outlined items that need to be considered or are
being considered and reviewed as part of the technology review the items refer to general areas that need to be
covered specific technologies specific examples of implementations of those technologies or specific examples of the application of those implementations the
page proposes structure for reporting the items it had the purpose of recording progress and avoiding duplication the wiki also
includes pages for each of the areas of investigation where the teams entered their reviews for curation65 each of the
technology review sections adopted similar structure that was developed to aid readers an introduction to the topic summary of the
primary sources used to guide follow up investigations short term analysis of the available choices what should we do today
the analysis tried to consider the next two to five years the duration of envriplus direct actions longer term vision
attempted to assess the ways in which the topic will evolve inevitably these are not detailed and cannot be used
without further thought but such visions may stimulate useful long term planning existing relationships within the envriplus project and its
members that may be informed by and inform this technology review topic known open issues and their implications for this
technology topic the aspect regarding existing relationships item above included work packages and tasks within those wp it also included
current use cases these are intended to focus on well defined target issue in order to develop deeper understanding and
if possible devise implementation strategies and exemplar prototypes to validate those solution strategies and communicate the potential value of investing
along these lines they will involve agile methodologies in most cases they are organised by wp9 and their current status
can be found in the relevant wiki space66 it was not always possible to cover all of these aspects for
each technology topic however very high standard was achieved the overall achievements of the technology review and some individual aspects
are assessed in section page onwards identification and citation technologies margareta hellström and alex vermeulen icos ri and lund university
introduction context and scope general comment it is important to keep in mind that there are many different actors involved
in data identification and citation as there are in all of the technology review topics that follow data producers ris
agencies individuals data centres community repositories university libraries global or regional data centres publishers specialised on data or with traditional
focus and data users diverse ecosystem from scientists experts to stakeholders and members of the public technologies should reflect needs
and requirements for all of these here the focus is on ris that typically involve all of those viewpoints time
constants for changing old practices and habits can be very long especially if they are embedded in established cultures or
when capital investment is required for these reasons updating or implementing totally new technology alone does not improve usage performance
as the behaviour of the designated scientific community will influence the discoverability and ease of reuse of research data scientific
traditions and previous investments into soft or hardware can lead to large time constants for change adopting new database technology
quickly could on paper provide large benefits to the data providers like lower costs and easier administration and curation but
may de facto be unacceptably lowering overall productivity for significant parts of the user community over long period of time
while the transition is achieved unequivocal identification of resources and objects underlies all aspects of today research data management the
ability to assign persistent and unique identifiers pids to digital objects and resources and to simultaneously store specific metadata url
originator type date size checksum etc in the pid registry database provides an indispensable tool towards ensuring reproducibility of research
duerr stehouwer almas not only do pids enable us to make precise references in reports and literature but it also
facilitates recording of object provenance including explicit relationships between connected objects data and metadata parent and child predecessor and successor
as well as unambiguous descriptions of all aspects and components of workflows moreau tilmes pervasive adoption of persistent identifiers in
research is expected to contribute significantly to scientific reproducibility and efficient re use of research data by increasing the overall
efficiency of the research process and by enhancing the interoperability between ris ict service providers and users almas background identification
number of approaches have been applied to solve the questions of how to unambiguously identify digital research data objects duerr
traditionally researchers have relied on their own internal identifier systems such as encoding identification information into filenames and file catalogue
structures but this is neither comprehensible to others nor sustainable over time and space stehouwer instead data object identifiers should
be unique labels registered in central database that contains relevant basic metadata about the object including pointer to the location
where the object can be found as well as basic information about the object itself exactly which metadata should be
registered and in which formats is topic under discussion see weigel environmental observational data pose special challenge in that they
are not reproducible which means that also fixity information checksums or even content fingerprints should be tied to the identifier
socha duerr et al duerr provide comprehensive summary of the pros and cons of different identifier schemes and also assess
nine persistent identifier technologies and systems based on combination of technical value user value and archive value dois digital object
identifiers provided by datacite scored highest for overall functionality followed by general handles as provided by cnri and dona and
arks archive resource keys dois have the advantage of being well known to the scientific community via their use for
scholarly publications and this has contributed to their successful application to geoscience data sets over the last decade klump general
handle pids have up to now mostly been used to enable referencing of data objects in the pre publication steps
of the research data lifecycle schwardmann they could however in principle equally well be applied to finalised publishable data persistent
identifiers systems are also available for other research related resources than digital data metadata articles and reports it is now
possible to register many other objects including physical samples igsn software workflow processing methods and of course also people and
organisations orcid isni in the expanding open data world pids are an essential tool for establishing clear links between all
entities involved in or connected with any given research project dobbs background citation the force11 data citation principles martone state
that in analogy to articles reports and other written scholarly work also data should be considered as legitimate citable products
of research although there is currently discussion as to whether data sets are truly published if they haven undergone standardised
quality control or peer review see parsons thus any claims in scholarly literature that rely on data must include corresponding
citation giving credit and legal attribution to the data producers as well as facilitating the identification of access to and
verification of the used data subsets data citation methods must be flexible which implies some variability in standards and practices
across different scientific communities martone however to support interoperability and facilitate interpretation the citation should preferably contain number of metadata
elements that make the data set discoverable including author title publisher publication date resource type edition version feature name and
location especially important the data citation should include persistent method of identification that is globally unique and contains the resource
location as well as links to all other pertinent information that makes it human and machine actionable in some sensitive
cases it may also be desirable to add fixity information such as checksum or even content fingerprint in the actual
citation text socha finding standards for citing subsets of potentially very large and complex data sets poses special problem as
outlined by huber et al huber as granularity formats and parameter names can differ widely across disciplines another very important
issue concerns how to unambiguously refer to the state and contents of dynamic data set that may be variable with
time because new data are being added open ended time series or corrections introduced applying new calibrations or evaluation algorithms
rauber rauber both these topics are of special importance for environmental research today finally number of surveys have indicated that
the perceived lack of proper attribution of data is major reason for the hesitancy felt by many researchers to share
their data openly uhlir socha gallagher this attitude also extends to allowing their data to be incorporated into larger data
collections as it is often not possible to perform micro attribution to trace back the provenance of an extracted subset
that was actually used in an analysis to the individual provider through the currently used data citation practices sources of
state of the art technology information used web sites of cnri crossref datacite dataone ddi data documentation initiative alliance dona
digital object numbering authority epic european persistent identifier consortium eudat eurocris responsible for cerif development icsu codata icsu committee on
data for science and technology igsn international geosample number isni international standard name identifier mdc making data count project oknf
open knowledge foundation openaire orcid pangaea data publisher for earth environmental science rda research data alliance and the web pages
of its many active interest and working groups including bibliometrics working group active data citation working group finished data fabric
interest group active data publishing interest group active in collaboration with icsu world data system data type registries working group
finished phase starting phase metadata interest group active pid information types working group finished persistent identifiers interest group active research
data collections working group active taverna workflow management system thomsonreuters thor and its precursor odin project w3c world wide web
consortium wf4ever project webinars organised by rda openaire thor proceedings from some recent conferences ieee etc articles in scientific literature
see bibliography discussions with colleagues and experts from envriplus partners and other organisations two to five year analysis of state
of the art and trends as evident from the large number of on going initiatives for applying identifiers to and
subsequently providing linkages between all components of research from individual observation values to the people making them it is very
difficult task to even try to envisage how the data intensive research landscape will look in few years from now
here we list some of the issues and ideas that are being worked on now and which we feel will
continue to be of importance in the coming years majority of starting up ris adapt data curation strategies that are
fully capable of handling dynamic data both versioned static files and truly dynamic databases centred around persistent identifiers for both
data metadata objects and queries standards for unambiguous referencing of subsets of data sets in citations and in workflow contexts
will become widely adopted by scientists and publishers alike enabling both efficient human and machine extraction of slices of data
as well as detailed micro attribution of the producers of the data subset more complex data objects will become common
including data collections research objects containing both data and related metadata and other virtual aggregates of research information from multitude
of sources this will require new strategies for content management and identification at both producer and user level systems for
allocating persistent identifiers will become more user friendly by development of apis and human oriented uis that are common to
all major identifier registries this will have profound positive impacts on the administration and reproducibility of scientific workflows to enable
efficient automation of data discovery and processing it will become common to store an enhanced set of metadata about the
objects directly in the pid registries data bases related to fixity versioning basic provenance and citation the current trend to
implement an ever tighter automated information exchange between publishers data repositories and data producers will continue and become the norm
in many fields including environmental and earth sciences more effective usage tracking and analysis systems that harvest citation information not
only from academic literature but from wide range of sources will be developed individual envriplus ris are engaged in number
of the above mentioned developments through the activities outlined in the description of work of several work packages in themes
and there is also active participation by individual envriplus ris in projects such as eudat2020 or as use cases in
rda groups however the relatively short lifetimes and limited number of members of this type of project often has several
negative consequences firstly there may not be enough diversity within the use cases to encourage the development of broad solutions
that cover the needs and requirements of wider range of communities secondly the knowledge and experience gained through such work
often ends up benefiting only small number of ris if there is any long lasting application at all envriplus could
therefore make difference by setting up platform for informing practitioners about on going initiatives especially those that involve envriplus members
but not as part of envriplus itself collection of ri use cases for passing on to the technology developers and
finally promoting the dissemination implementation and uptake of effective examples details underpinning the above analysis in this section we present
more background for the topics listed above for each topic some specific examples of relevant technologies are listed together with
brief narrative discussion and suggestions for further reading either links to the bibliography or to organisations whose web site addresses
are listed under majority of starting up ris adapt data curation strategies that are fully capable of handling dynamic data
both versioned static files and truly dynamic databases centred around persistent identifiers for both data metadata objects and queries main
technology needs versionable databases to support time machine retrieval of large datasets also sensor data that are dynamic sources rauber
and personal communications with asmi there exist already today several different technical database solutions that support versioning of database records
both sql and nosql based both approaches have advantages and disadvantages but with optimised and well planned schemas for storing
all transactions and their associated timestamps it is possible to achieve time machine like extraction of data and metadata as
they existed at any given time without significant losses in performance at least for moderately sized databases but challenges remain
for databases required to store long time series of high frequency sensor data for data stored as flat files it
is mainly the metadata that must be stored in database supporting versioning database to allow identification of what file represent
the current state of the data at given point in time connections to cataloguing and maintenance of provenance records supporting
automated metadata extraction and production for machine actionable workflows sources tilmes duerr see example in the article supplement on going
work in rda metadata interest group rda research data provenance interest group and eudat2020 work package in order for data
driven research to be reproducible it is an absolute requirement that not only all analysis steps be described in detail
including the software and algorithms used but that the input data that were processed are unambiguously defined ideally this is
achieved by minting persistent identifier for the data set as the basis for the citation and then adding details about
the date when the data was extracted the exact parameters of the subset selection if used version number if applicable
and some kind of fixity information like checksum or content fingerprint optimally at least one of the citation itself the
pid record metadata and or the resource locator associated with the pid will provide all this information in machine actionable
format thus allowing workflow engines to check the validity and applicability of the data of interest currently majority of the
envriplus ris and their intended user communities haven yet started to implement the outlined practices in consistent manner as consequence
the reproducibility of research based on data from these ris could be called into question what is needed to change
this situation are good examples and demonstrators that can be easily adopted by the ris without much investment in time
and software such best practices need to be developed in cooperation across the work packages of theme standards for unambiguous
referencing of subsets of data sets in citations and in workflow contexts will become widely adopted by scientists and publishers
alike allowing both efficient human and machine extraction of slices of data as well as detailed micro attribution of the
producers of the data subset query centric citations for data allowing for both unambiguous and less storage resource intensive handling
of dynamic data sets sources duerr huber rauber data sets from research may undergo changes in time as result of
improvements in algorithms driving re processing of observational data errors having been discovered necessitating new analysis or because the data
sets are open ended and thus being updated as new values become available unless great care is taken this dynamic
aspect of data sets can cause problems with reproducibility of studies undertaken based on the state of the data set
at given point in time the rda working group on data citation has therefore produced set of recommendations in steps
for implementing query based method that provides persistently identifiable links to subsets of dynamic data sets the wg have presented
few examples of how these recommendations can be implemented in practice but there is great need for continued work towards
sustainable and practical solutions that can easily be adopted by ris with different types of data storage systems more complex
data objects will become common including data collections research objects containing both data and related metadata and other virtual aggregates
of research information from multitude of sources this will require new strategies for content management and identification at both producer
and user level systems for cataloguing and handling more complex collections both of data sets and metadata research objects sources
okfn wf4ever the rda data collections wg just starting rda data type registries wg concluded with recommendations the increasing complexity
of research data and metadata objects adds more challenges firstly in contrast to printed scholarly records like articles or books
data objects are often in some sense dynamic updates due to re analysis or discovered errors or new data are
collected and should be appended the content can also be very complex with thousands of individual parameters stored in single
data set furthermore there is growing trend to create collections of research related items that have some common theme or
characteristic in the simplest form collections can consist of lists of individual data objects that belong together such as daily
observations from given year similarly it may be desirable to combine data and associated metadata into packages or to create
even more complex research objects that may also contain annotations related articles and reports etc collections can be defined by
the original data producers but may also be collated by the users of the data and may thus contain information
from large variety of sources and types this diversity is prompting work on providing tools for organising and managing collections
using apis that are able to gather identity information about collection items through their pids as well as minting new
pids for the collections themselves there is also need for sustainable registries for data type definitions that can be applied
to tag content in way that is useful and accessible both to humans and for machine actionable workflows however the
use of data types varies greatly between different user communities making it difficult task to coordinate both the registration of
definitions as well as sustainable operation of the required registries especially if these are set up and operated by ris
here more work is needed in collaboration with number of ris each with differing data set structures and catalogue organisations
in order to provide clear recipes for data typing systems for allocating persistent identifiers will become more user friendly by
development of apis and human oriented uis that are common to all major identifier registries this will have profound positive
impacts on the administration and reproducibility of scientific workflows adoption of common api for pid minting applicable across registries and
methods sources duerr socha klump work by the rda pid information types wg concluded and the rda pid interest group
starting now although number of systems for persistent identification of scientific publications have been available for over decade relatively few
researchers are consistently applying these systems to their research data there is at the same time pressing need to encourage
data producers to mint pids for any digital items belonging in the research data lifecycle that should be referable including
also raw data and datasets produced during analysis and not just finalised and published data sets surveys have indicated that
the reasons for the slow adoption rate include lack of knowledge about the existing opportunities confusion over their relative differences
and merits and difficulties related to the identifier minting process especially when it needs to be performed on large scale
as often the case for data the latter problem is to large extent due to the large variety in design
and functionality of pid registry user interfaces and apis and there are now several initiatives looking into how the registration
and maintenance of pid records can be streamlined and simplified however the proposed inclusive user and programmatic interfaces will need
extensive testing by wide range of different user communities there are also institutional issues concern over intellectual property rights may
inhibit the adoption of working practices or the delegation of authority to allocate pids to enable efficient automation of data
discovery and processing it will become common to store an enhanced set of metadata about the objects directly in the
pid registries databases related to fixity versioning basic provenance and citation handle registries also need to become federated and allow
users to add community or project specific metadata to the handle records see recommendations of the rda wg on pid
information types including those required for identity and fixity verification sources rda pid information types wg final new rda data
collections wg presentations from the epic datacite pid workshop in paris mainly motivated by desire to speed up and facilitate
the automation of data discovery and processing there are calls for the centralised handle and other pid system registries to
also allow data producers and curators to store more types of metadata about the objects directly in the registries data
bases examples include information related to data content type fixity versioning basic provenance and citation this would speed up data
processing since the requesting agent workflow process would be able to collect all basic metadata via just one call to
the pid registry instead of needing to first call the registry and then follow the resource locator pointer to landing
page which data would need to be harvested and interpreted some pid management organisations such as datacite and the doi
foundation already support relatively broad range of metadata fields but other registries are more restrictive the technology for storing the
metadata is already in place but database systems would need to be upgraded to allow for more pid information types
also registry servers capacity to handle the expected large increase in lookup query requests must be upgraded optimal performance will
require the pid information types themselves to be defined and registered in persistent way using data type registry the current
trend to implement an ever tighter automated information exchange between publishers data repositories and data producers will continue and become
the norm in many fields including environmental and earth sciences expanding the application of persistent unique identifiers for people and
institutions in research data object management including metadata and pid registry records sources orcid and datacite thor web site and
webinar series driven by demands from large scientific communities biochemistry biomedicine and high energy physics publishers and funding agencies there
is strong movement towards labelling everything and everyone with pids to allow unambiguous and exhaustive linking between entities currently it
quite common for individual researchers to register an orcid identity and subsequently use this to link to articles in their
academic publications record this could be equally well applied to published research data for example by entering orcid ids in
the relevant author metadata fields of the datacite doi registry record and allowing this information to be harvested by crossref
or similar services connected with this is growing trend to implement tighter information exchange primarily links to content between publishers
data repositories and data producers there are several on going initiatives looking into how to optimise and automate this including
the thor project operated by cern which involves amongst others openaire orcid datacite and pangea it is expected that the
outcomes of these efforts will set the norm however to be fully inclusive and consistent from data curation and cataloguing
point of view this practice should be extended to all relevant personnel categories involved in the research data lifecycle including
technicians collecting data data processing staff curators etc not just principal investigators and researchers this would allow both complete record
of activities for individuals suitable for inclusion in cv but conversely can also be seen as an important source of
provenance information for linked data sets more effective usage tracking and analysis systems that harvest citation information not only from
academic literature but from wide range of sources will be developed discovering and accounting for micro attribution of credit to
data producers and others involved in the processing management of data objects especially in the context of complex data objects
sources uhlir socha huber rda research data collections interest group there is strong encouragement from policy makers and funding agencies
for researchers to share their data preferably under open access policies and most scientists are also very interested in using
data produced by others for their own work however studies show that there is still widespread hesitancy to share data
mainly because of fears that the data producer will not receive proper acknowledgement and credit for the original work these
apprehensions become stronger when discussing more complex data containers how to give proper credit if only parts of an aggregated
data set or collection of data sets were actually used in later scientific works indeed many scientists deem it inappropriate
or misleading to attribute collective credit to everyone who contributed to collection proposed solutions now under investigation by various projects
focus on two approaches making the attribution information supplied together with data sets both more detailed and easier to interpret
for end users and providing means for data centres and ris to extract usage statistics for collection members based on
harvested bibliometric information available for the collections the first of these could be achieved by labelling every individual datum with
code indicating the producer or minting pids dois for the smallest relevant subsets of data from given researcher group or
measurement facility based on such information data end user can provide detailed provenance about data sets used at least in
article text the second approach may combine tracing downloads and other access events at the data centre or repository level
with bibliometry with the aim to produce usage statistics at regular intervals or on demand from data producer however handling
each file records individually would quickly become cumbersome so methods of reliably identifying groups of files should be considered organisation
of ri operated metadata systems that will allow fast and flexible bibliometric data mining and impact analysis sources socha epic
and datacite pid workshop paris make data count project crossref openaire thor by analysing information about the usage of research
data through collecting citations and references from variety of academic sources it is possible to extract interesting knowledge of what
subsets of data sets are of interest who has been accessing the data and how and in what way they
have been used and for what purpose traditionally this data usage mining is performed based on searching through citation indices
or by full text searches of academic literature applying the same methods as for articles crossref scopus web of science
sometimes also augmented by counting downloads or searches for data at repositories and data portals however up till recently citations
of data sets were not routinely indexed by many publishers and indices and such services are still not comprehensively available
across all science fields at least partly this is due to limits in the design of citation record databases and
the insufficient capacity of lookup services here updated technologies and increased use of semantic web based databases should bring large
improvements however it is important to cover also non traditional media and content types such altmetric sources include mendeley citeulike
and scienceseeker as well as facebook and twitter indeed while references to research data rather than research output in social
media may not be very common in earth science yet it may become more prevalent where inferences from digital media
activity complement direct observations in poorly instrumented regions there are already examples from astronomy data are in any case already
being referred to in many other forms of non peer reviewed science related content such as wikipedia articles reddit posts
and blogs since authors using these alternative information outlets are less likely to use pids or other standard citation formats
it is great challenge to bibliometry mining systems to identify and properly attribute such references discovery and sharing especially of
data contained in complex data objects may be enhanced by the use of data type registries that facilitate subset identification
and retrieval sources rda data type registries working group eudat data sharing requires that data can be parsed understood and
reused by both people and applications other than those that created the data ideally the metadata will contain exhaustive information
about all relevant aspects measurement units geographical reference systems variable names etc however even if present such information may not
be readily interpretable it may be expressed in different languages or contain non standard terminology there is need for support
system that allows for precise characterisation of the parameter descriptions in way that can be accessed and understood by both
human users and machine actionable workflows registries containing persistently and uniquely identified data type definitions offer one solution that is
highly configurable and can be adapted to needs of specific scientific disciplines and research infrastructures in addition to the basic
properties listed above the type registry entries can also contain relationships with other types parent and child or more complex
ones pointers to services useful for processing or interpretation or links to data convertors data providers can choose to register
their own data types possibly using their own namespace apply definitions provided by others or apply mix of these approaches
the pids of the applicable data types are then inserted into the data objects metadata and can also be exposed
via cataloguing services and search interfaces the rda data type registry working group has designed prototype registry server which is
currently being tested by number of ris and organisations in second phase the rda group will continue the development of
the registry concept by formulating data model and expression for types designing functional specification for type registries and investigating different
options for federating type registries at both technical and organisational levels the adoption of unambiguous and clear annotation of data
as offered by data types should go long way towards allaying researchers concerns that their data will be misused either
in an erroneous fashion or for inappropriate purposes longer term horizon as discussed in recent report from the rda data
fabric interest group balmas both the increasing amounts of available data and the rapidly evolving ecosystem of computing services there
will have to be an intensifying focus on interconnectedness and interoperability in order to make best use of the funding
and resources available to scientists and society tools and technologies including cloud based processing and storage and increasing application of
machine actionable workflows including autonomous information searches and data analyses will all rely on sustainable and reliable systems for identification
and citation of data based on this we have identified couple of likely trends for the period up to the
year move towards automation of those aspects of the research data lifecycle that will involve basic tasks like assigning identifiers
and citing or referring to all kinds of resources including data and metadata objects software workflows etc evolution towards more
complex collections of research resources like research objects that will necessitate more flexible approaches towards both strategies for identification and
detailed unambiguous citation or referencing parts of such objects much more tightly integrated systems for metadata provenance identification and citation
will evolve pushed by data producers publishers and data centres offering rapid and trusted feedback on data usage and impact
relationships with requirements and use cases requirements there are strong connections between the ri requirements gathered for identification and citation
with those related to other topics including cataloguing curation processing and provenance majority of ris are very concerned with how
to best encourage and promote the use of their data products in their designated scientific communities and beyond but at
the same time it is considered high priority to implement mechanisms and safeguards that can ensure that the data producers
especially principal investigators and institutes in charge of data collecting and processing receive proper credit and acknowledgments for their efforts
here it seems obvious that consistent allocation of persistent identifiers and the promotion of standards for using these when citing
data use in reports and publications will go long way to fulfil these needs in addition efforts to standardise the
practices and recipes for identifying subsets of complex data collections and subsequent extraction of micro attribution information related to these
subsets would ensure fair distribution of professional credit asked for by researchers and funding agencies alike work packages the overarching
objective of the envriplus work package is to improve the efficiency of data identification and citation by providing recommendations and
good practices for convenient effective and interoperable identifier management and citation services wp6 will therefore focus on implementing data tracing
and citation functionalities in environmental ris and develop tools for the ris if such are not otherwise available use cases
of the proposed envriplus case studies66 those of interest from an perspective are mainly ic_01 dynamic data citation identification citation
ic_06 identification citation in conjunction with provenance and ic_09 use of dois for tracing of data re use at the
time of writing these are under review or preparation with some likelihood of merger of the three the primary aim
of ic_01 is to provide demonstrators of the rda data citation working group recommendation rauber for query centric approach to
how retrieval and subsequent citation of dynamic data sets should be supported by the use of database systems that track
versions this may be combined with support also for collections of data sets which can be seen as sub category
of dynamic datasets thus addressing also the goals of ic_09 ic_06 is aimed at identifying good practices for using pids
for recording provenance throughout the data object lifecycle including workflows and processing summary of analysis highlighting implications and issues tools
and services now under development that will allow seamless linking of data articles people etc are likely to have large
impact on individual researchers institutions publishers and stakeholders by allowing streamlining of the entire data management cycle virtually instantaneous extraction
of usage statistics and facilitation of data mining and other machine actionable workflows while dois for articles and orcid identifiers
for researchers are now an accepted part of the scientific information flow publishing of data may not even consider identifiers
for other resources except for publications for which dois are well established to speed up the adaptation both current and
future technologies for data identification and citation must not only be flexible enough to serve wide range of existing research
environments but they also have to be shown to provide clear benefits to both producers curators and end users indeed
while some research communities and infrastructures have fully embraced the consistent use of pids for data metadata and other resources
throughout the entire data lifecycle many others are only beginning to think about using them important reasons for this hesitancy
or tardiness include substantial knowledge gap perceived high investment costs both for personnel hardware and software and lack of support
from the respective scientific communities to change engrained work practices envriplus is expected to play an important role in defining
best practices for first applying identifiers to data and other research resources including the researchers themselves and secondly how use
them for citations and provenance tracking this will be achieved by designing and building demonstrators and implementations based on concrete
needs and requirements of envriplus member ris and providing documentation and instructional materials that can be used for training activities
further discussion of the data identification and citation technologies can be found in section this takes longer term perspective and
considers relations with strategic issues and other technology topics curation technologies keith jeffery british geological survey bgs introduction context and
scope digital curation is the selection preservation maintenance collection and archiving of digital assets digital curation establishes maintains and adds
value to repositories of digital data for present and future use this is often accomplished by archivists librarians scientists historians
and scholars wikipedia it should be noted that cataloguing curation and provenance are commonly grouped together since the metadata workflow
processes and legal issues associated with each have more than intersection and therefore rather than generating independent systems common approach
is preferable moreover there are strong interdependencies with identification and citation with aaai with processing with optimisation with modelling and
with architecture sources of state of the art technology information used relevant sources are the data curation centre dcc open
archival information system oais both discussed below and research data alliance rda which has several relevant groups notably preservation69 but
also active data management plans70 and reproducibility71 short term analysis of state of the art and trends the ideal curation
state is aimed to ensure the availability of digital assets through media migration to ensure physical readability redundant copies to
ensure availability appropriate security and privacy measures to ensure reliability and appropriate metadata to allow discovery contextualisation and use including
information on provenance and rights the current practice commonly falls far short of this with preservation commonly linked with backup
or recovery usually limited to the physical preservation of the digital asset and lacking the steps of curation selection ingestion
preservation archiving including metadata and maintenance furthermore in the current state while datasets may be curated it is rare for
software or operational environments to be curated including these necessary to achieve reusability belhajjame collecting them automatically has been demonstrated
by santana perez where processes in virtual environment are monitored and their interactions with external resources recorded the collected information
is used to automatically create virtual image in which the job can be deployed and re run on the cloud
curation lifecycle the desirable lifecycle is represented by dcc digital curation centre diagram72 figure data management plan increasingly research funders
are demanding dmp data management plan different organisations have proposed different templates and tools for plans but that of dcc
is used widely73 as is the us equivalent74 dmp is defined wikipedia data management plan or dmp is formal document
that outlines how you will handle your data both during your research and after the project is completed oais reference
model oais open archival information systems reference model iso provides generic conceptual framework for building complete archival repository and identifies
the responsibilities and interactions of producers consumers and managers of both paper and digital records the standard defines the processes
required for effective long term preservation and access to information objects while establishing common language to describe these it does
not specify an implementation but provides the framework to make successful implementation possible through describing the basic functionality required for
preservation archive it identifies mandatory responsibilities and provides standardised methods to describe repository functionality by providing detailed models of archival
information and archival functions higgins set of metadata elements in structure has been proposed75 figure the curation lifecycle model problems
to overcome the following are some important problems that need to be addressed for curation motivation there is little motivation
for researchers to curate their digital assets at present curation activity obtains no reward such as career preferment based on
data citations in some organisations curation of digital assets is regarded as librarian function but without the detailed knowledge of
the researcher the associated metadata is likely to be substandard increasingly funding agencies are demanding curation of digital assets produced
by publicly funded research business model curation involves deciding what assets to curate and of those for how long they
should be kept determining an appropriate duration of retention for digital asset is problem economics and business models do not
manage well the concept of infinite time first business justification is needed in that the asset cannot be collected again
it is unique observation experiment the cost of collecting again by the same or another researcher is greater than the
cost of curation metadata metadata collection is expensive unless it is automated or at least partially automated during the data
lifecycle by re using information already collected commonly metadata is generated separately for discovery contextualisation curation and provenance when much
of the metadata content is shared across these functions comprehensive but incrementally completed metadata element set is required that covers
the required functions of the lifecycle it needs sufficient application domain data that other specialists in that domain will be
able to find and correctly interpret the associated data process the lifecycle of digital research entities is well understood and
it needs process support the incremental metadata collection aspect is critically important for success workflow models if adapted to such
an incremental metadata collection with appropriate validation are likely to be valuable here jeffery curation of data it may be
considered that curation of data is straightforward but it is not first the dataset may not be static by analogy
with type specimen in museum both streamed data and updateable databases are dynamic thus leaving management decisions to be made
on frequency of curation and management of versions with obvious links to provenance issues related to security and privacy change
with time and the various licences for data use each have different complexities the data may change ownership or stewardship
derivatives may be generated and require management including relationships with the original dataset and all its attendant metadata curation of
software software written years ago is unlikely to compile let alone compose with software libraries and execute today indeed many
items of software such as the workflows behind scientific method will either not run or give different results six months
later since many research propositions are based on the combination of the software algorithm and dataset then the preservation and
curation of the software becomes very important it is likely that in future it will be necessary to curate not
only the software but also specification of the software in canonical representation so that the same software process or algorithm
can be reconstructed and ideally generated from the specification this leaves the question of whether associated software libraries are considered
part of the software to be curated or part of the operating environment see below very often software contains many
years worth of intellectual investment by collaborating experts this makes it very valuable and hard to replace taking good care
of such assets will be requirement for most research communities curation of operational environments it is necessary to record the
operational environment of the software and dataset the hardware used whether instrumentation for collection or computation devices has characteristics relating
to accuracy precision operational speed capacity and many more the operating system has defined characteristics and includes device drivers software
library used by the application it is moot point whether software libraries belong to the application software or to the
operational environment for the purposes of curation finally the management ethos of the operational environment normally represented as policies requires
curation longer term horizon there is some cause for optimism media costs are decreasing so more can be preserved for
less awareness of the need for curation is increasing partly through policies of funding organisations and partly through increased responsibility
of some researchers research projects in ict are starting to produce autonomic systems that could be used to assist with
curation however the major problem is the cost of collecting metadata for curation firstly incremental collection along the workflow with
re use of existing information should assist workflow systems should be evolved to accomplish this secondly improving techniques of automated
metadata extraction from digital objects may reach production status in this timeframe76 relationships with requirements and use cases all the
requirements obtained from the interviews and the use cases indicated some awareness of the need for digital curation however few
ris had advanced towards providing systems to achieve curation and even those that had advanced had not full data management
plan including business case in place issues and implications commonality of metadata elements across curation provenance cataloguing and more so
common metadata scheme should be used metadata collection is expensive so incremental collection along the workflow is required workflow systems
should be evolved to accomplish this and scientific methods and data management working practices should be formalised using such workflows
to reduce chores and risks of error as well as to gather the metadata required for curation automated metadata extraction
from digital objects shows promise but production system readiness is some years away envriplus should adopt the dcc recommendations envriplus
should track the relevant rda groups and ideally participate further discussion of the curation technologies can be found in section
this takes longer term perspective and considers relations with strategic issues and other technology topics cataloguing technologies thomas loubrieu institut
francais de recherche pour exploitation de la mer ifremer introduction context and scope the technological review for cataloguing covers subset
of the different concepts to be managed in catalogues as seen in requirement section reference catalogues persons and organisations publications
research objects federated catalogues datasets resources physical samples procedures and software activity and event logs have not been considered in
the technology review because the subject is not mature enough in ri and ict to manage this information in catalogue
yet as matter of priority we focus on references and federated catalogues described above the review gives an overview of
the software applications or systems and interface standards used for cataloguing related information sources of state of the art technology
information used the standards considered are provided by the following bodies w3c www w3 org iso www iso org iso
home html ogc www opengeospatial org rda working groups on metadata domain specific standards cerif and geoscienceml we identify whether
tools are open source they may be desktop or server side with web interfaces software short term analysis of state
of the art and trends references catalogues persons and organisations the most popular system for person identification and cataloguing is
currently orcid they are involved in thor project which helps to connect together datasets papers and researcher information they are
also working on organisation cataloguing77 publications management systems cited by ri are web of knowledge78 and scopus79 for research objects
no technology has been cited further investigation would be required before developing catalogues for research objects federated catalogues dataset catalogues
are managed at the ri level with ckan in the open data world and rda or geonetwork in the iso
and ogc contexts ckan is open source application software developed by the open knowledge foundation80 this application is now very
popular in the open data projects and is used by eudat for the b2find function geonetwork is open source catalogue
application software for spatially referenced resources and especially datasets it is very popular in the gis community and will allow
ris to fulfil inspire requirements for data discovery81 both applications are web servers and can be used and managed on
line it appears to be pragmatic and feasible to harvest existing ckan and geonetwork in one ckan central server at
eudat figure figure ckan server provided for eudat b2find service resources especially observation equipment are managed in dedicated systems at
the ri level however two standards are popular to describe these items ssn ontology from w3c and sensorml from ogc
the semantic sensor network ontology together with connected ontologies prov is w3c standard dedicated to the description of sensors observations
and related concepts82 although no end user tools are available yet to implement this ontology the bodc for project senseocean83
is proposing developments in this perspective kokkinaki ogc is also standardising observation system description with the sensorml standard84 see figure
the sensorml standard is part of the sensor web enablement ogc initiative85 inspire is recommending this for sensor data sharing
and these technologies have been tested and assessed for air or water quality decision support systems with for example north
software solution bröring it is currently being developed as well in the marine community to ease the preservation and accessibility
of observation context information the idea developed in oceans of tomorrows projects fixo3 or nexos is to enable swe and
especially sensorml from on board the sensor or instrument to streamline data flow to the data centres plug and play
sensors the standardisation of instrument manufacturers specifications is also goal which is looked at with sensor registries such as the
yellow pages developed for emso ri86 which aims at being standardised in sensorml v287 figure sensorml compliant editor for marine
observation system emso ri physical samples are managed in dedicated systems no common standard has been identified yet for solid
earth specifically geosciml figure provides standard for boreholes and laboratory analysis specimens88 in biology specimens records are not standardised and
further analysis would be required to review off the shelf available software such as collection management systems89 or dedicated systems
specific to an ri figure borehole description in geosciml procedures and software although we are not aware of standards covering
the description of software applications and libraries there are de facto central infrastructures very popular on which to host software
code documentation and even project management tools bug tracking system in the past sourceforge was the most popular nowadays github90
is more popular github can be accessed via an api91 which could be useful to harvest in catalogue of information
related to software and algorithms no specific tools or standards are identified to document procedures generic documents or scientific papers
are used to describe the procedures overall solutions cerif proposed by epos provides an overall conceptual model for managing the
above information see figure figure cerif general data model catalogue federation will be done by aggregating ri catalogue distributed instances
content through common dedicated standards csw iso19139 for datasets sensorml for sensor or instruments etc into state of the art
catalogue applications for each type of catalogue ckan for datasets to prepare the future see below and enable cross catalogue
federation the availability of semantic capabilities rdf and sparql interfaces will be considered longer term horizon in envriplus catalogue federation
will rely on the most popular solutions in each field datasets observation systems samples software and procedures and we can
expect cross fertilisation between research infrastructures on this subject and rely on catalogue specific official processes csw iso19139 for datasets
or de facto ckan restful api standards to federate them this cross fertilisation of cataloguing strategies may be good focus
for think tank beyond the interoperability or federation of catalogues across ris per type of object datasets we can expect
interoperability between catalogues of different items datasets and observation systems actually some of the tools identified to implement catalogues already
provide generic sparql interfaces92 which can be foreseen as semantic interfaces between catalogues of different types of object in this
perspective the availability of such semantic interfaces on top of catalogue implementation will be selection criterion one expectation from the
provenance activity coupled with cataloguing is to provide guided user searches over catalogues by exploiting statistical results mined from previous
searches it is clear that many de jure and de facto metadata standards for cataloguing exist and are used even
stating that an ri uses ckan doe not indicate homogeneity since the semantics can be very different for different implementations
and many ris extend existing metadata standards therefore envriplus as it wishes to promote interoperability among ris will need to
manage metadata heterogeneity for datasets software components workflows persons resources publications etc one approach being used in epos is to
choose rich metadata model in this case cerif and provide matching and mapping software and conversion software for the commonly
used metadata schemes in the ris within epos these include dc dcat ckan egms iso19115 inspire and others but in
each metadata case with different dialects remembering that key performance measure is researcher productivity we need to be careful not
to drown researchers in sea of incomprehensible metadata one strategy is to include in the preferences available to users the
ability to select view by ticking the facets of metadata that are of interest in menu generated from that community
total metadata set the system then generates query that selects that subset and interposes it for this user so the
user sees database view that matches their interests relationships with requirements and use cases as seen earlier the requirement analysis
guides us toward identification and selection of core central catalogues for persons and documentation scientific papers software might be as
well managed in central repository federation of distributed catalogues and central harvesting for datasets observation systems and samples the catalogue
developments will be applicable in the following use cases ic_1 dynamic data citation dataset catalogue metadata can be re used
for registering datasets at datacite while avoiding multiple edits for the author ic_2 provenance information gathered in provenance management processes
should be hosted by catalogues for example the author of datasets tracked through the data lifecycle by the provenance process
will be stored in catalogue ic_8 cataloguing curation and provenance is the implementation case for catalogues fulfilling curation and provenance
requirements ic_9 provenance use of doi for tracing data re use datasets and scientific paper catalogues will be used as
background for this use case ic_11 semantic linking framework interoperability and semantic linking across catalogues datasets with observation systems and
persons will be provided by this use case tc_4 sensor registry will actually be the implementation of one catalogue of
sensor or instrument model and instances together with maintenance monitoring tools issues and implications the harmonisation of item descriptions in
catalogues across ris is the primary challenge of the catalogue topic the implication is that the catalogue development will not
cover every ri for every type of catalogue but demonstrate the interoperability of some ri systems and the actual value
added by envriplus to provide cross disciplinary catalogues in addition the adherence and actual involvement of key partners in ris
so that information required to populate the catalogue are actually available in the central or federated catalogues will also be
an issue the implication is that the development as foreseen in use case agile task force teams and subsequently ri
involvement in catalogue will be driven by the good will and availability of the key partners further discussion of the
cataloguing technologies can be found in section this takes longer term perspective and considers relations with strategic issues and other
technology topics processing technologies leonardo candela consiglio nazionale delle ricerche cnr and rosa filgueira university of edinburgh introduction context and
scope there are great many requirements for processing at every stage of the data lifecycle from validating error correcting and
monitoring during data acquisition to transformations for comprehensible presentations of final results every step in between has major processing requirements
all forms of data preparation filtering and transformation to achieve consistent input to subsequent stages in the data lifecycle or
the next step in scientific method analysis pattern matching and statistical reduction to extract relevant signals from complex and noisy
data large scale simulations to generate the implications of current models correlation of those results with well prepared derivatives from
observations and then refinement of the models lot of technologies and approaches have been developed to support these tasks including
high performance computing solutions aggregated computing resources thus to realise an high performance computer including processors memory disk and operating
system distributed computing infrastructures distributed systems characterised by heterogeneous networked computers called to offer data processing facilities this includes high
throughput computing and cloud computing scientific workflow management systems swms systems enacting the definition and execution of scientific workflows consisting
of liew list of tasks and operations the dependencies between the interconnected tasks control flow structures and the data resources
to be processed data analytics frameworks and platforms platforms and workbenches enabling scientists to execute analytic tasks such platforms tend
to provide their users with implementations of algorithms and statistical methods for the analytics tasks these classes of solutions and
approaches are not isolated rather they are expected to rely on each other to provide end users with easy to
use efficient and effective data processing facilities swms rely on distributed computing infrastructures to actually execute their constituent tasks in
europe prace definitely represents the major initiative for high performance computing similarly egi is point of reference for distributed computing
both these initiatives are discussed in detail in other parts of this deliverable see section and will not be further
analysed in this section in this section we will thus focus on scientific workflow management systems and data analytics frameworks
and platforms over the last two decades many large scale scientific experiments take advantage of scientific workflows to model data
operations such as loading input data data processing data analysis and aggregating output data the term workflow refers to the
automation of process during which data is processed by different logical data processing activities according to set of rules along
with the attendant tasks of for example moving data between workflow processing stages workflow management systems wms bux aid in
the automation of these processes freeing the scientist from the details of the process since wms manage the execution of
the application on computational infrastructure scientific workflows allow scientists to easily model and express the entire data processing steps and
their dependencies typically as directed acyclic graph dag whose nodes represent workflow steps that are linked via dataflow edges thus
prescribing serial or parallel execution of nodes scientific workflows have different levels of abstraction abstract and concrete an abstract workflow
models data flow as concatenation of conceptual processing steps assigning actual methods to abstract tasks results in concrete workflow there
are four key properties of scientific workflows which are handled differently in each scientific workflow management reusability workflow management systems
have to make it easier for workflow designer to reuse their previously developed workflows in their under development workflows many
workflows provide mechanisms for tracing provenance and methodologies that foster reproducible science santana perez performance workflow optimisation is not trivial
task there are different methods can be applied on workflow to reduce the execution time spinuso design almost all the
modern workflow management systems provide rich graphical user interface for creating workflows the aim of providing graphical composition mechanism is
to ease the step of describing workflows for the workflow developers collaboration due to the increase in the number of
workflows and collaborative nature of scientific research projects developing share and collaboration mechanisms through the network and internet for workflows
is must some projects such myexperiment de roure wf4ever belhajjame and neuroimaging workflow reuse garijo are specially focused on this
approach scientific workflows perform two basic functions they manage the execution of constituent codes and information exchanged between them therefore
an instantiation of workflow must represent both the operations and the data products associated with particular scientific domain it should
be assumed that individual operations and data products were developed independently in an uncoordinated fashion workflows must be usable by
the target audience computational scientists on target platforms computing environments while being represented by abstractions that can be reused across
sciences and computing environments and whose performance and correctness can be modelled and verified in parallel with scientific workflows series
of platforms and frameworks have been developed to simplify the execution of scientific distributed computations this need is not new
it is actually rooted in high throughput computing which is well consolidated approach to provide large amounts of computational resources
over long periods of time the advent of big data and google mapreduce in the first half of brings new
interests and solutions besides taking care of the smart execution of user defined and steered processes platforms and environments start
offering ready to use implementations of algorithms and processes that benefits from distributed computing infrastructure sources of state of the
art technology information used two major sources of information have been used literature available discovered by the web and technologies
web sites in particular the following websites have been source of information apache airavata website airavata apache org apache spark
website spark apache org dispel4py website dispel4y org galaxy website galaxyproject org gcube website www gcube system org kepler website
kepler project org knime website www knime org pegasus website pegasus isi edu taverna website www taverna org uk triana
website www trianacode org wf4ever website www wf4ever project org wings website www wings workflows org short term analysis of
state of the art and trends several technologies and trends characterise the data processing domain for scientific workflow management systems
swms liu several have developed user friendly way for designing and implementing computational scientific procedures under the workflow paradigm providing
guis and tools for easing the task of handling large and complex computational processes in science examples of them are
pegasus deelman supports execution of workflows in distributed environments such as campus clusters grids and clouds pegasus workflow management service
maps an application onto available resources pertaining to the cluster while keeping the internal and external dependencies of the workflow
in order pegasus workflow has been used to powers ligo gravitational wave detection analysis triana churches is an open source
graphical problem solving environment that allows you to assemble and run workflow through graphical user interface while minimizing the burden
of programming taverna wolstencroft provides an easy to use environment to build execute and share workflows of web services it
