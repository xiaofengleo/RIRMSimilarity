use pre packaged algorithm in service almost unknowingly as part of well formalised encapsulated established method they use to those
who are engaged in creating and evaluating new algorithms for innovative ways of combining and interpreting data clearly both continua
are valid and any point in each continuum needs the best achievable support for the context and viewpoint with such
diversity it is clear that one size fits all approach is infeasible this conclusion is further reinforced by the need
to exploit the appropriate computational platforms hardware architectures middleware frameworks and provision business models to match the properties of the
computation and the priorities of the users given their available resources if such matching is not considered it is unlikely
that all of the developing research practices will be sustainable in an affordable way for example too much energy may
be used or the call on expert help to map to new platforms may prove unaffordable such issues hardly rise
to the fore in the early stages of an ri or project so again we note forces that will cause
the understanding and nature of requirements to evolve with time this leads to the following follow up observations programme of
awareness raising and training events will be tuned to different viewpoints of participants and also link up with the relevant
target technologies and provider models it is more likely that this will benefit the systems developers who are setting up
processing services and the innovators who are creating new ways of using them bringing them together may trigger significantly beneficial
mutual understanding and alliances the packaging of computations and the progressive refinement of scientific methods are key to productivity and
to the quality of scientific conclusions consequently as far as possible processing should be defined and accessed by high level
mechanisms this allows focus on the scientific domain issues and it leaves freedom for optimised mappings to multiple computational platforms
this protects scientific intellectual investment as it then remains applicable as the computational platforms change this will happen as their
nature is driven by the much larger entertainment media leisure and business sectors the higher level models and notations for
describing and organising processing also facilitate optimisation and automation of chores that otherwise will distract researchers and their supporters providing
support for innovation in this context is critical without innovation the science will not advance and will not successfully address
today societal challenges it requires support for software development testing refinement validation and deployment conducted by multi site teams engaging
wide variety of viewpoints skills and knowledge for the complex data intensive federations the environmental and earth sciences are dealing
with this involves new intellectual and technological territory alliances involving multiple ris and external cognate groups such as eudat prace
and egi may be the best way of gathering sufficient resources and building the required momentum further consideration of these
issues may be found in the processing technology review section and as suggested further actions in section provenance requirements assessment
at present the need for and benefits of provenance provision are only recognised by some ris section in abstract we
are sure that most scientists appreciate the value of provenance but they tend to think of it as painful chore
they have to complete when they submit their final selectively chosen data to curation they often only do this when
their funders or publishers demand it that culture is inappropriate for many ris they are in the business of collecting
and curating primary data and commonly required derivatives clearly they want to accurately record the provenance of those data as
foundation for subsequent use and to achieve accountable credit for environmental and earth scientists use of provenance throughout research programme
can have significant benefits during method development it provides ready access to key diagnostic and performance data and greatly reduces
the effort required to organise exactly repeated re runs frequent chore during development as they move to method validation they
have the key evidence to hand for others to review when they declare success and move to production the provenance
data informs the systems engineers about what is required and can be exploited by the optimisation system once results are
produced using the new method these development time provenance records underpin the provenance information collected during the production campaign of
course all of this depends on users having control over which provenance data is generated and which is preserved the
provenance system being fully automated so that no niggling chores intrude and tools that exploit the provenance data and support
all the innovation and production steps the ris survey reported very different stages of adoption and when there was adoption
it did not use the same solutions or standards this was almost always related to data acquisition rather than the
use of data for research the change in culture among researchers may be brought about by envriplus through programme of
awareness raising and well integrated compendium of tools the latter may be more feasible if the development of the active
provenance framework is amortised over consortium of ris this leads to similar observations to those given above an awareness arising
and training programme should be considered to stimulate thinking about and use of provenance recording and exploitation automation is essential
for all aspects of provenance handling to avoid sapping productivity but this needs to be under the control of the
ris and practitioners concerned effective tools that show the value of provenance those suggested be spinuso et al spinuso are
key to changing the culture and encouraging early engagement with the quality of provenance myers these issues are further considered
when provenance technology is reviewed section and lead to suggestions in section optimisation requirements assessment at present the identified set
of optimisation requirements analysed in section is relatively sparse however there is anticipated to be demand which will become manifest
when production of research results ramps up as ris deliver continuous services and data feeds or as the numbers and
diversity of users grow experience shows that as data handling organisations transition from pioneering to operations many different reasons for
worrying about optimisation emerge these are addressed by wide variety of techniques so that investment in optimisation is usually best
left until the following kinds of question can be answered what precisely does the ri or its user community want
to be optimised what trade offs would they find acceptable to achieve that optimisation how can this be formalised as
measurable cost function encapsulating the answers to the first two questions very often there are significantly different answers from different
members of community the ri management may need to decide on compromises and priorities for such reasons awareness raising and
training campaigns may be appropriate though they may not be as urgent as they are for some of the other
topics optimisation needs to look beyond individuals and single organisations when looking at overall costs or energy consumption in group
of ris or the infrastructures they use tactics may consider the behaviour of data intensive federation for example when data
is used from remote sites or is prepared for particular class of uses the use of caching may save transport
and re preparation costs and accelerate the delivery of results however the original provider organisation needs to have accountable evidence
that their data is being used indirectly and the caching organisation needs its compute and storage costs amortised over the
wider community thinking about optimisation tends to focus on technology and operational costs however the most valuable asset of an
ri is almost certainly its community of practitioners those who exploit the facilities data and services to pursue their research
goals and those who create improve and operate the facilities the first subgroup uses enabling these critical parts of the
community to be as productive as possible can be viewed as an optimisation challenge in the context of envriplus it
is particularly important to consider empowering community members to collaborate effectively across boundaries how to provide automation that removes as
many chores as possible from their routine work while leaving them both the ability to understand and investigate what is
going on and to apply controls where they are necessary how to provide tools development environments and vres that easy
to use particularly during innovation without removing critical degrees of freedom from the innovation options these topics are revisited when
the technology options for optimisation are investigated see section community support requirements assessment the requirements for community support are summarised
and analysed in section from page onwards there you will find shopping list of virtually all of the facilities for
communication information sharing organisation and policy implementation that distributed community of collaborating researchers and their support teams might expect they
normally expect those facilities to be well integrated and easily accessed wherever they are from wide range of devices however
care should be taken to consider the full spectrum of end users few may be at the forefront of technological
innovations but the majority may be using very traditional methods because they work for them investment is only worthwhile if
it is adopted and benefits the greater majority of such communities taking into account their actual preferences there may be
two key elements missing in the context of envriplus which focuses on achieving the best handling and use of environmental
data workspaces that can be accessed from anywhere and are automatically managed in which individuals or groups can store and
organise the data concerned with their work in progress test data sets sample result sets intermediate data sets results pending
validation results pending publication since environmental researchers have to work in different places such as in field sites in different
laboratories and institutions they need to control these logical spaces which may be distributed for optimisation or reliability reasons these
are predominantly used to support routine work but can also be used for innovation this includes intelligent sensors requiring access
to variety of logical spaces for their operations development environments that can be accessed from most workstations and laptops and
that facilitates collaborative innovation and refinement of the scientific methods and the data handling sharing among distributed community testing management
of versions and releases and deployment aids would be expected in envriplus collaboration between various roles including citizen scientists across
intellectual organisational and academic cultural boundaries is widespread requirement we can illustrate this with the following roles there are occasional
heroes who span several of these roles but predominantly we have to pool different skills from different roles to make
breakthroughs or even to do the daily business domain specialists who may be more or less computer literate but who
will develop and use new patterns of data and ri facilities use at least characterised by parameter sets and written
procedures for themselves and others to follow such methods may be formalised as workflows scripts and programs using frameworks and
packages because scientific progress depends on sufficient repetition perhaps to of their work is routine repeating previous methods with refinement
but progress often depends on their insights as they recognise new potential in the available data or new questions their
domain should address data scientists who are adept at and develop new statistical and machine learning strategies and analytic procedures
for cleaning and preparing data and for extracting derivatives or information from data data intensive engineers who set up and
maintain great deal of plumbing on top of standard infrastructures to enable all of the data handling and storage required
and the use of the data either through access interfaces for external processing or through local resources with which to
process the data virtual research environment vre designers and builders who shape the science gateways and improve the apis and
interactive services they offer system administrators who oversee the operation of the platforms and software recognise impending resource shortages plan
and conduct procurement and installation of resource increments and keep the strength of security sufficiently high computational modellers and numerical
analysts who develop simulation systems and mechanisms for exploring their results to lesser or greater extent virtually every ri will
depend on such mix of roles and viewpoints community support needs to recognise and engage with these multiple viewpoints as
well as help them to work together this is particularly challenging in the distributed environments and federated organisations underpinning many
ris at least training and help desk organisation will need to take these factors into account productivity will come from
each category being well supported significant breakthroughs will depend on the pooling of ideas and effort across category boundaries new
requirements identified the plan for work in envriplus developed by the ris and incorporated in the dow has been validated
and endorsed by the requirements gathering as is shown in table there are few additional and not planned aspects of
data management and user support that appear in the conversations and underpin some of the general issues these have aspects
of improving usability to improve the experience and productivity of users and the teams who support them in part they
are better packaging of existing or planned facilities and in part they are intended to deliver immediate benefits to keep
communities engaged and thereby improve take up and adoption of envriplus products boundary crossing the participating communities experience boundaries between
the different roles identified above see section between disciplines sub disciplines and application domains and between organisations many of today
pressing research questions and many of the federations addressing them see section require teams to form and to think and
work effectively together across those boundaries envriplus can stimulate this by organising ad hoc think tanks so that it brings
together virtually participants from across the boundaries and stimulates them to think and work together on relevant topics by bringing
in suitable experts and setting up suitable practical challenges to be addressed during the course this requires elapsed time and
allocation of both training effort and trainee time so the target understanding that the course will deliver has to be
carefully chosen establishing suitable agile development processes where people work intensely together on common issue with carefully set goal then
assimilating the results and building on the networks provided delivering services and tools well suited to each role and organisational
context arranging workspaces that facilitate such collaborative behaviour while ideas are being developed and formulated this requires those involved to
have control over the release and sharing of the material they work on individuals may be involved in several groups
probably with different roles integrated communication facilities the individual elements of communication for distributed participants in an ri need to
be conveniently integrated there are several potential solutions in this area it may help if at least one well integrated
one were run to be available for ris project participants and envriplus this needs to present views that work well
for each category of practitioner some of the selected use cases in theme may serve to achieve this exemplars and
early benefits the development of exemplars of effective methods and software or services that support them is key to spreading
ideas testing them in new contexts and developing buy in this will be helpful in the training and outreach programme
it is also vital as part of the process of delivering as early as possible benefits to the active researchers
and other practitioners if we can deliver immediate benefits they will not have to struggle for so long investing unproductive
time in tedious workarounds an example follows data access interfaces researcher and others managing data driven processes spend great deal
of time identifying data they want arranging to be permitted access arranging transfers arranging local storage arranging onward shipment to
computation resources if necessary and returning storage resources when they have finished if this is packaged as convenient operation their
work is simplified and more productive the parts of such process are all being built but delivering an integrated solution
that just works would be large benefit it needs the provision of user or group workspace it needs means of
identifying the required data once deployed it can be grown in small increments taking the users along an improving path
they might prioritise some of the following identification using queries over associated metadata in the identity registries or in catalogues
see section extension of the operations that are easily applied to the accessed data we have found visualisation particularly relevant
handling batches of data consistently at the same time the tea tray metaphor handling intermediate transient results with various aids
for handling them in bulk and for clearing up afterwards promoting selected results to properly identified and citable arranging for
their data to be published or curated assessment of technology review there are number of pervasive issues that impact all
of the technology reviews these are nurturing boundary crossing collaboration to address new challenges harnessing both numerical models and data
driven statistical methods so that they work well in tandem data intensive federation as foundation software sustainability critical long term
issue lack of engagement from the ict industry each of these pervasive issues will be explained below as they would
otherwise reappear in many topics then progress with each technology review topic will be assessed cultural diversity the understanding of
the technological options has to take into account the diverse cultures of the ris and their communities these prior investments
and differing cultures have significant value to an individual an organisation an ri or scientific domain the cultures are reinforced
by educational and induction practices these cultures the ways in which disciplines work have been refined to work effectively disrupting
such cultures should not be undertaken lightly however these legacy and ingrained elements present serious barriers to more rapid adoption
of consistent or interworking approaches it is desirable to find path whereby envriplus and its cohort of ris is an
island of consistency and coherence for its own benefits and as beacon to others section presents comprehensive review of the
options and technologies it identifies the key players moving data driven research towards the nirvana of consistent data treatment it
is crucial to invest sufficiently in these causes by ensuring that there is very effective internal communication for awareness raising
education and decision support and ii by actively participating in two way channel between envriplus and the key external organisations
exemplary solutions and working practices well supported by software worthy of future adoption will be needed to evaluate options and
to rally rapid and widespread adoption within envriplus key use cases launch the work needed for those exemplars agile development
teams build them once exemplars exist they should be used within envriplus for the education campaign and for external outreach
to help the adoption of common practices and standards reach critical mass see section for an example however the solutions
from individual use cases will need broadening to become more generic patterns with wider applicability nurturing collaboration between different fields
every major discipline already has challenges developing collaboration and communication between its subfields the culture developed through higher education normally
addresses this by having common core that spans the fundamentals of the active approaches over time this core of mutual
understanding is whittled away as researchers progress specialise and develop their own skills and knowledge in particular niche as academic
tutors and research leaders we are often guilty of steering those we mentor into focusing on particular topic so they
may achieve promotion or be successful in gaining resources and leadership such attitudes and traditional mentoring behaviour may be outmoded
and we may need new behaviours and cultures to exploit today research opportunities and to address today pressing challenges that
much is well recognised in contexts such as envriplus but what are we doing about it in envriplus with its
context of ris the issue is broader in scope and more central many have reported that they wish to collaborate
learn from or harmonise with other groups the issue is two dimensional communication across domains subdomains and infrastructures supporting those
domains is one dimension another critical dimension is communication between roles collaboration across roles is critical atkinson 2013b where they
seek synergy across three viewpoints domain experts data analytics experts and data intensive engineers roles have been enumerated in section
we revisit and group the roles in table although this is inevitably simplification it serves to show that there many
more viewpoints than three they have complex inter relationships that need developing and nurturing if the infrastructures underpinning ris are
to serve their communities well and be economically sustainable it should be remembered that in every role most of the
effort is invested in routine work in some estimates that underpins all science but the remaining moments of invention and
introduction of new methods or technologies that leads to new advances is key for scientific progress that innovation is much
more dependent on cross boundary collaboration improving the experience and productivity of routine work has direct payoffs as well as
making innovation more likely managing the innovation so that it does not disrupt critical routine work is requirement and that
requirement propagates to the steps we take to improve cross boundary collaboration there are occasional heroes who combine mastery of
wide range of these viewpoints to lead campaigns and dramatic breakthroughs but sustainable and affordable science cannot be predicated on
sufficient supply of heroes and even they cannot develop sufficient depth in more than few roles or fields consequently we
have to become more expert at combining independently developed knowledge and skills from different minds and from different cultures this
is not an issue that can be tackled by envriplus alone but it is in key position to give an
important lead table illustrates the diversity of viewpoints and skills needed to deliver successful research and to make breakthroughs with
global and societal challenges the ris may review they have the right kind of experts to meet their goals while
doing this they may also take into account skills they currently access from elsewhere eudat egi and envriplus and consider
the sustainability of those relationships in comparison with their target ri lifetimes table some of the roles key to the
ris sustainable success role description domain specialists campaign leaders research leaders have broad view of their domain and commitment to
particular cause they organise resources and steer effort raise commitment and maintain focus on the goal and the quality of
every step on the path to achieving it as result they are usually adept at boundary crossing and may underestimate
the challenge it poses for others theoreticians theoreticians develop conceptually satisfying and in principle testable explanations of phenomena and observational
patterns these may address broad issues in domain or relate to some specific aspect of interest to one or more
domains experimentalists experimentalists devise and conduct programmes of work to test hypothesis this may be lab based or field based
these days it is often in silico it uses computation to run models analyse observational data or do both repetition
is often necessary to marshal sufficient evidence observers observers organise and conduct the collection of data that represents manifest properties
of chosen phenomena or systems they may commission instruments or establish coordination to obtain sufficient information with sufficient reliability for
sufficient sample of their target set of measurements or records instrument builders instrument builders may draw on many sciences engineering
specialisms and technologies to construct instruments that collect measurements of the relevant properties and that operate in the required context
citizen scientists these can contribute to any aspect of campaign data analysis and pattern recognition in galxyzoo164 or field observations
of bird populations with ebird165 kelling curators curators establish and run the collection publication and preservation of selected reference information
considered important by their community data scientists statisticians develop the mathematics and practical methods for inferring information latent in data
taking account of potential biases such as sampling measurement and recording errors machine learning experts machine learning experts deploy the
statistical methods such as strategies for handling missing data and statistical inference and develop and deploy algorithms over large bodies
of data to obtain derivatives that represent actionable information that is they are able to assess how reliable those derivatives
are relative to target decision making or calibration goals problem solving kit builders these kit builders develop libraries of packaged
methods that work well together for doing data preparation performing statistical steps and visualising results they provide ways of using
these so that users do not need to understand the details they are often provided as problem solving environments mobile
app encapsulations or browser accessible tools where the user can conduct and steer operations on their data without having to
explicitly manage data or resources they often have workflow or scripting notation to allow users to encapsulate repeated tasks as
one task visualisation experts visualisation experts develop ways of showing the significant aspects of data effectively so that scientists and
decision makers are best able to see and use the significant to them information these techniques adapt to range of
output devices from smart phones to immersive video caves they use dynamic viewpoint and presentation change controls to allow users
to explore data and recognise salient features otherwise hidden ict specialists systems architects the systems architects shape the overall structure
the choice of series of software subsystems layers and services choice of the hardware architectures that should support the software
the organisation and provision of data storage the provisions for user interaction security and operations management and the distribution and
interconnection of all of these across organisations and computational platforms as in conventional architecture key responsibility is to tease out
the actual requirements and planned modes of use to highlight potential conflicts and risks and to reconcile aspirations with available
budgets and resources as in conventional architecture considerable use is made of prior designs and pre assembled systems software engineers
software engineers are responsible for the good engineering of software ensuring that it is fit for purpose delivers the functions
and facilities required is capable of being run in all of the contexts where it will be deployed will prove
dependable not fail catastrophically and without diagnostic traces that it will be continuously available and that it is maintainable this
is complex engineering task where given prototype software that already runs in its originator context for their envisaged test cases
may take from ten to hundred times the original effort to achieve full scalability and deployability with acceptable dependability and
security as software may often have long lifetime investment in its quality and sustainability from inception to end of useful
life is worthwhile for carefully selected software see section or www software ac uk for open source projects of the
effort goes into user support for commercially supported software this is typically ensuring effective mechanisms for following up all user
issues is another software engineering responsibility data intensive engineers these engineers take the algorithms that are developed by data scientists
the workflows developed by research campaigns organise the data on appropriate storage media and map the algorithms onto appropriate hardware
so that overall goals can be reached economically or quickly there are variety of rapidly evolving strategies hci experts the
human computer interaction experts study and improve interaction at all levels from the atomic elements of hci communication such as
touch screen gestures to the deep relationships of shared knowledge and skills that affect interpretations of responsibility and preparedness to
take decisions vre experts virtual research environments are logical foci enabling communities to see collection of computational services data and
supported methods as holistic integrated resource that is easy to use the design and construction of these undertaken by vre
experts has the usual distributed systems architectural and engineering issues in most cases it also has the challenges of drawing
on diverse independently owned heterogeneous autonomous resources see section digital communications experts digital communications underpin every stage of the data
lifecycle from acquisition potentially in geographically isolated locations and with low power availability to the data centres hosting curation it
connects all roles of user from their office home or in the field to the full gamut of services it
builds on many digital transfer mechanisms with different modes of funding and management blazing and sustaining trails through this terrain
is the key responsibility of these experts storage db experts data volumes rates of delivery and rates of access all
have to be met by these experts by mapping data to appropriate technologies trading longevity of storage against speed of
access they meet these needs drawing on wide range of technologies from traditional bulk tape storage to the latest solid
state technologies 3d xpoint166 its applications for scientific computing are being investigated in the eu nextgenio project167 they draw on
range of communication technologies organise data placement and data movement they construct algorithms and access models to accelerate the common
requirements such as content based searches parallel writes and co location of computation and data storage by mapping onto appropriate
software and hardware platforms simulation experts numerical analysts and simulation experts take the mathematical models developed by theoreticians sometimes mathematically
described and sometimes as preliminary implementations and transform these into algorithms that run well on the available architectures and achieve
the required precision theoreticians computing science theoreticians formulate models of computational logic of distributed systems of algorithms of hardware architectures
of data representations and semantics of actor systems and so on many of these underpin the above work the original
description of map reduce and its types by milner and plotkin the theory is essential if transformations are to be
undertaken to handle the scale and diversity encountered between many metadata forms it is not expected that envriplus researchers will
engage directly with such theory but the ict experts that they work with should certainly be tracking the relevant theories
for their viewpoint systems administrators once systems are built they need to be provisioned the new versions of hardware and
software need to be deployed and connected in and the arrangements to allow access to resources while protecting systems from
misuse needs continuous vigilance they are often involved in aspects of innovation support such as configuring and deploying new subsystems
and software platforms hardware architects many advances by electrical engineering and production engineering lead to growing numbers of available logical
memory or data movement components for given power cost and volume harnessing these advances to yield more of the power
that science needs has to be by combining these elements in new ways as it is not possible to simply
make these components run faster the variety of combinations is potentially very large but the ready made units are largely
shaped by the dominant internet and entertainment businesses therefore the hardware architects develop ingenuity in delivering science platforms using general
purpose hardware to save costs only exceptional systems such as the hpc systems operated by prace have architectures tuned for
very large scale numerical simulation understanding which aspects of science need and perform best on particular hardware architectures requires engagement
with these architects short term and longer term strategies in facilitating boundary crossing are recommended in the short term within
the lifetime of envriplus the following steps should be taken recognise the value and contribution of each domain and each
role the envri week already brings together participants from multiple domains and multiple roles ensure that it invests and encourages
inter role as well as inter disciplinary communication the agile task force teams can span roles and domains where they
do this they form crystal of cross boundary understanding of developing depth asking them to do anything else while their
campaigns are intense would inhibit the agile behaviour and thinking required however in periods between campaigns sharing their experience with
others during envri week or in training programmes would help expand and generalise the impact of their work collaborative training
programmes should ideally engage participants spanning domains and spanning roles these might be succession of webinars scaled and timed to
not disrupt the routine work if motivational exemplar hands on practical can support this which requires collaboration across roles and
or domains this will be effective at building understanding dieter kranzlmueller director of the leibniz rechenzentrum lrz has identified the
importance of training that brings together researchers in environmental sciences with those in computing sciences168 deliver intellectual ramps to new
technologies and methods of working new facilities are invariably designed by experts who are no longer aware of how much
they have learned and how many skills they have acquired for others to adopt these new facilities it is essential
that there is an easy to get started mode of use that usually doesn reveal the full power and flexibility
and then there are incremental steps that form path to the full power this helps greatly in accelerating adoption but
it also helps cross boundary exploration and integration organise initial summer schools that deliberately cross both domain and role values
see below the longer term strategies should include the following in conjunction with others through partner institutions and international bodies
help support the development and recognition of collaborative careers for example some of the following steps which are today in
operation in few places but which need more widespread investment could be encouraged and supported collaboration exercises across disciplines as
part of undergraduate training collaboration exercises and projects that cross discipline and technical role boundaries to achieve goals that show
the value of interdisciplinary skills and perhaps virtual collaboration phd programmes that deliberately require supervisors from different disciplines and possibly
different institutions that develop deep understanding of inter disciplinary issue career selection and promotion procedures that recognise the value of
interdisciplinary communication collaboration and creativity in conjunction with others initiate summer schools that deliberately bring representatives from multiple disciplines and
from multiple roles to develop and share understanding of how best to address these boundary crossing challenges numerical models and
statistical methods in tandem the paradigm of using mathematical models to capture our understanding of the phenomena we observe has
certainly been with us since newton era it has had tremendous boost as computers have become progressively more powerful and
it certainly plays key role in the environmental and earth sciences for example in seismic inversion and modelling convection in
the mantle szalay has pioneered better use of simulations based on such models szalay jim gray thought of that as
the third paradigm after the observational and experimental paradigms he coined the term the fourth paradigm as new way of
observing and characterising data gray it is driven by the tremendous growth in digital data delivered from instruments from monitoring
digital activity from numerical simulations and from harnessing citizen science volunteers the scale of data and progress with statistical methods
such as machine learning also exploiting the growing computing power has led to new ways of recognising and describing patterns
in the natural systems of interest this introduces new opportunities for science and its applications as these two approaches numerical
modelling and statistical analysis of observations can be harnessed together to achieve breakthroughs and develop new understanding and applications however
achieving and successfully exploiting such combinations is very challenging to quote geoscientists in recent data science meeting at the alan
turing institute aston methodologically there is major gap between statistical modelling and machine learning on one side and numerical or
physical modelling on the other hence systematic approach to consistent data integration and model building is of highest value and
priority the challenge is widely recognised at several levels the conceptual frameworks ii the implementation and encoding as scientific methods
and iii the best ways of resourcing those implementations fox many environmental and earth scientists will be encountering these challenges
and seeking to reap the benefits of successfully harnessing the combination of statistical and numerical methods envriplus should seek ways
of pooling intellectual and practical effort to reap these benefits there are potentially theoretical issues there are certainly ict issues
in how to describe and support such activity there are organisational issues about how to support the working practices involved
in scalable and sustainable way at the very least envriplus should kick off strategy that includes these combined approaches even
if they do not become priority in its time data intensive federation foundations there is great need for data intensive
federations in the environmental and earth sciences particularly as they study multifaceted global phenomena there are many application domains where
practitioners are trying to exploit growing wealth of diverse and evolving data sources it is imperative to provide an affordable
and sustainable environment which improves their productivity as they develop and use data science methods we refer to the network
of data and resource sharing agreements as data intensive federation dif data intensive federations are virtual distributed environments that organise
the repeated use of dynamic data from multiple sources owned and managed by independent organisations into holistic conceptual framework that
makes it much easier for multiple groups of practitioners to perform their data driven work as such they are artefacts
that involve the construction and maintenance of social organisational and ict infrastructure they need to include crossing boundaries establishing and
honouring agreements supporting multiple work environments tool sets services and technologies they must enable practitioners to undertake decision or policy
support information services reference data and research using their framework as many of the participants are funded to do such
things we argue that building each data intensive federation incrementally as one off and in isolation is wasteful in effort
and produces solutions which are not only less effective and efficient than state of the art but also inhibit interoperation
we advocate investment in to develop foundational principles and reusable frameworks or data fabrics to use the rda terminology that
can provide the core of data intensive systems for all domains and can be tailored for those aspects of each
domain that are specific data intensive federations require the following features beyond the data warehousing and data lake strategies that
are used to support commercial applications where the data can be corralled into one regime under single data controller today
data will not be wholly owned or under the control of one organisation instead set of data providers may actively
participate in forming the federation while other data providers will remain external standard services from the external providers may be
used or special arrangements negotiated with them the federation will identify and develop framework in which the combined data can
be more easily used each user does not have to negotiate access or deal with idiosyncrasies each user does not
need to assemble the data they require into their own working space this can benefit data owners as they do
not have to deal directly with every usage request nor have to handle all data accesses provided that they can
obtain usage records to justify investment in their services this framework will deliver holistic view of the federated data that
facilitates current well recognised tasks it will also support experimental development of advances in the use and interpretation of data
selection of these will lead to advances in the holistic view new category of experts data diplomats an extension to
the roles in table need to be supported they will represent the different organisations and negotiate the rules for forming
using and evolving dif for example robust mechanisms to track and cite the use of contributing organisations data so that
they can demonstrate value added by their work to their funders these may be binding contracts for sensitive data the
data diplomats need to be able to negotiate formalise and record these sharing rules and then trust that the framework
will honour them even when multiple stages of derivation and caching have been used for sustainability any data provider may
require revisions to rules concerning the use of their resources tools should be provided to understand the interaction of rules
their impact on priority uses and their propagation an example of such framework being developed for sharing medical data under
contracts that meet local regulations in europe is given by elliot et al elliot that delivers data integration based on
logically described agents169 and their interactions robertson and papapanagiotou many forms of dynamic evolution must be accommodated examples include the
addition of new data sources improvements to the existing ones and changes in the trade offs in the underpinning digital
platforms in addition to supporting collaborative development of analytic techniques and scientific methods using these data in combination with simulations
the framework must support the development and evolution of data integration recipes these will be re playable to have equivalent
semantics in new contexts when dealing with burst of new real time data and social media inferences as the work
environment provided to practitioners now has such scale and dynamics research and innovation must be well supported that is experiment
and exploration must be possible with few restrictions as if in the production context without jeopardising production priorities when such
innovation is successful there should be smooth path to deployment translation for production use at scale in the controlled and
managed context procedures must be resilient to change in the external environment otherwise they will fail at critical time when
used in new context or after provider has updated their service when help is needed during natural disaster without such
resilience those responsible for geo information services have to be very conservative to avoid such failures if the resilience is
established it will keep the old procedures viable while new approaches are pioneered as far as possible most changes should
propagate automatically until all their consequences are dealt with otherwise sustaining dif will require unaffordable and unachievable effort from experts
to maintain services they also require features from other lines of research and development namely trust and reputation automated formation
and management for virtual organisations patel and network centric collaboration networks camarinha matos there are growing number of application areas
where such requirements are manifest sharing the for the underpinning architecture and novel functionality will be worthwhile the environmental and
earth sciences are an ideal starter community to work with as they have great deal of diverse data that is
already accessible and tradition of sharing their data to tackle both deep science and societal challenges the facilities will include
arrangements for practitioners to perform tasks such as finding understanding and obtaining data they require specifying data integration criteria and
data preparation pipelines requesting delivery of data for their use or to an analytic process collection or storage of results
with provenance and diagnostic records automatically attached submission of data including such results to the existing federation establishment of temporary
additions to the integrated data including the addition of new models for organising those data encouraging and facilitating the free
sharing of data and methods facilitating users proper attribution of data to the contributing federation members all of the above
will be conducted using high level and abstract notation that avoids distraction by or over tight binding to implementation and
target platform details many of them overlap substantially with the requirements articulated and the developments envriplus plans listed above we
may consider logical architecture for such dif with the elements shown in figure figure proposed architecture for data intensive federations
we see the world divided into three regions divided by vertical dotted lines in the diagram working from left to
right trusted external region organisations in this region provide services such as supplying credentials software tools and workflows and crucial
experts particularly the data diplomats organisations in this region produce software that is trusted and can be trusted to assign
authorisations to perform role appropriately this varies from members of the public with few authorisations to rule specifiers with significant
authority the majority of the users are in this region and their work generates most of the calls on the
services provided by the data intensive federation framework diff whilst this region is trusted to lesser or greater extent depending
on the credentials supplied it is entirely autonomous the individuals and organisations in this region are involved in many activities
some may depend heavily of particular dif and have regular patterns of interaction with it many will be engaged in
more than one dif or engage with the dif for fixed purpose or period controlled region the consortium taking direct
control of the data intensive federation kernel take full responsibility for this region it is one administrative regime and governance
system that is set up to steer the dif strategy to resolve conflicting requests and requirements to develop collaborative ethos
and to establish run and support the diff for its dif it will need business model to enable that role
to be sustained and sufficient resources to provide the computations storage and network connections needed for its internal operation and
for all interactions with the two external regions independent region here we find an extensive potentially global and heterogeneous collection
of data and ict resources that may grow incrementally when it is agreed that their resources are needed or they
want the dif to include their resources to publish them to this dif communities that growth will be subject to
data use agreements either standard terms they offer or specially negotiated arrangements the standard arrangements often have rules such as
usage rates special agreements may include an external organisation agreeing to notify the dif each time it has new release
of its resource and specify the data content and representational changes the dif may then adapt its recipes to suit
the new release and refresh the local cache avoid out of date version supply and to save repeated transport in
return the dif will accurately report all data usage the diff should consist of re usable subsystems that can be
composed and configured across distributed platform to meet the needs of the particular dif and to monitor and maintain the
various agreements that have been made to enable the use of external resources note that in some difs the quality
and enforcement of these agreements has to meet privacy and ethical standards or meet commercial in confidence agreements the governance
of the dif will determine its own policies and rules which will also be captured and implemented via the diff
many elements of the diff are just as envriplus would build them few identified by being coloured red in figure
we provide high level description of those elements in table that delivers holistic and integrated view of an heterogeneous federation
of data and computational resources while implementing and enforcing agreed inter organisational relationships table the elements of the data intensive
federation framework diff subsystem functional description diff api presentation via web services often as microservices vianden and notification services of
the data and facilities offered by the dif these will be organised as bundles so that user or tool developer
community can often focus on just one bundle the normal mode of use will deal with an abstraction of operations
and data use that avoids technical detail this allows development to take place outside the controlled region in the trusted
region diff gateway the diffg provides stable api that supports tools and programmatic use in consistent coherent and stable manner
it directly initiates many of the functions that are specific to dif such as recording agreements providing the holistic view
it verifies that all operations are compliant with agreements rules and contracts in particular it will provide interfaces for the
work of federation diplomats who negotiate agreements recognise the established aspects of the holistic model and devise mappings to deliver
them often drawing on recent innovations found via the diffkb it will provide tools for recording encoded rules for investigating
the interaction between rules and for analysing provenance records to assess rule compliance for rapid response to tasks entirely under
the diff kernel control it will directly call diff services or submit description as to what is to be done
to the diff work manager for larger tasks and all tasks that involve external services that may have changed since
the task expansion template was developed it will refer them to the diff mapping services to be adjusted to the
current context the diff mapping services will then pass one or more workflows to the diff work manager for large
and demanding requests the diffg will delegate their organisation to the diff planner this will analyse the requested workflow and
decided whether it should be partitioned it will then rearrange each partition taking into account the mapping by the dif
mapping service and information about target resources in external services and computational and storage services and the performance of previous
similar runs the mapped and optimised partitions will then be delegated to the dif work manager which will call on
specific local and external resources according to the annotations provided by the planner diff knowledge base the diffkb will contain
information about the holistic model its logical construction from the external and local sources and how it may be used
this will include explicit lists of allowable actions depending on session initiator authenticated identity authorisation current role and budget the
diffkb will have viewable form that may be navigated or queried to support novices learning about the holistic model and
federation and to support experts extending their understanding and planning their actions the diffkb will record how logical operations supported
by the diffapi are mapped to local and external services the diff gateway will use these mappings they will be
revised by federation diplomats by automated optimisation and in response to changes in available data and services the diffkb will
include descriptions of the catalogues the dynamic and static data and local and external resources these will eventually include the
relationships between these organisation elements constructed or discovered the available operations and methods for using those data and how they
should be used user annotation will be encouraged diff planner internally every significant action on the holistic view of data
will be represented in suitably abstract workflow notation yet to be chosen the planner will take parameterised version of such
workflow with its required data identified or embedding mechanism for obtaining the input data the diff planner will take into
account the sources of the identified data there may be multiple copies or they may be identified by queries over
catalogues or data sources and suitable target enactment services it will transform the workflow to make it cost less according
to an agreed or selected cost function it will then arrange for the diff mapper to prepare each partition for
execution possibly in coupled mode the diff planner will record its treatment in the diffkb and will reuse that treatment
when similar request occurs unless the digital context has changed diff mapping services the abstract workflows will need mapping for
two reasons the abstraction will omit many details such as marshalling and moving data implicit transformations management and clean up
of intermediate data and target specifics it will also handle changes that have occurred in the organisation or available facilities
in the external independent federation partners and accessed independent services these two forms of mapping are essential for sustainability they
deal with the inevitable and near continuous change in the digital environment they retain freedom to revise choices of targets
and computation arrangements from using local resources to using an external resource or switching between storm and spark as data
analysis framework diff work manager this takes requests for work to be done either hand crafted for simple local cases
of services offered via the diff api and submitted by the diffg or as result of the process described above
to handle more complex work the diff work manager makes final checks that the actions are authorised comply with the
rules and that the session owner has both the authority and allocated resources for the total request it then finds
the right mix of local and external resources to perform the task recording at least the minimum records in the
provenance store and sending results to the user note that such response to user may also have to comply with
rules some of these requests or some stages within the request may be interactive diff services these are internal services
to support all aspects of the diffg particularly the work of federation facilitators local data and catalogue management this includes
accommodating wide variety of catalogues and of multi faceted queries over them if necessary generating actions on the data they
reference services will also support rule definition revision testing and application local service will verify that requested task or stage
within task is compliant with current rules multifaceted query over the provenance records may select subset against which rule can
be tested either to verify that new rule now inhibits actions that were causing problems or that rule does not
find fault with valid actions the services should also support the development and testing of mappings and of requests that
may be installed as available once they meet acceptance criteria external data for solid earth dif this might include data
such as the fdsn coordinated seismic trace archives the lidar surveys gps streams and the nasa and esa satellite images
copernicus and sar that are available dif will choose target and negotiate these for example this one may obtain data
from fossil fuel and mineral extraction surveys even though much of those data are commercially confidential in some cases this
may require fairly strict rules about how those data may be used some data providers will also deliver identity services
for their data query services for selecting subsets and host computations on their resources for computing derivatives the may expect
the accounting systems of the diff kernel to properly report use of their data including reflection of consequent data derivatives
and publications local data these are data directly contributed or collected by the federation users or by federation partners who
choose to deposit directly these will need to meet sufficient metadata standards that they can be used by other parts
of the diff some automated tools for harvesting and validating such metadata will be developed as diff services the local
data will include caches for optimisation and user and group workspace the handling of such data will depend on other
services such as those provided by eudat the data files file collections databases and databases using variety of models and
representations and research objects will all have suitable pids so they may be referenced by methods and other parts of
the diff without implying location and storage media pid here means persistent identifier so that it is unique within the
required scope and persists for as long as it may be used not all of these need be permanent or
publicly accessible the framework may annotate local data to indicate such things as locate with computation replicate for scale make
durable archive transient others the other boxes in figure are the same as the corresponding functionality described in sections and
data science is fast growing field and research infrastructures have to be at the front line to best serve their
users without bothering them with technical details as such the ris should have the right kind of expertise employed or
otherwise hired following the initiative of the sister cluster project corbel to cooperate with the edison project170 it is recommended
that envriplus also enters into such cooperation to obtain dedicated data science training for their infrastructure staff software sustainability critical
issue scientists science and the applications of science are increasingly dependent on software consequently this dependency has to be thought
about as carefully as the dependency on instruments when an instrument is designed purchased deployed and run for long periods
relevant teams of experts are involved and if necessary trained at every stage extreme care is invested in engineering and
production quality is of great concern and is assessed repeatedly upgrades during the operational lifetime take substantial planning and investment
software requires comparable care and similar engagement of appropriate expertise as software is largely invisible and often acquired incrementally today
this attention is lacking as more and more of the data driven working practices depend on multi layered stacks of
software their continuity ability to keep functioning and quality depends on the underlying software being adequately sustained those meeting the
challenge of deploying infrastructure quickly or getting new scientific method supported can be excused taking short cuts and lashing together
software components they find however those concerned with planning research infrastructures their strategy and finance should recognise that this is
building potential software crisis if software is required to meet new functionality new forms of analysis or new capabilities new
sustained data rates then adequate time must be invested in its design development and testing for example the campaign to
develop data handling for the large hadron collider lhc began in almost years before the first particle collisions took place
when live data acquisition was delayed for over year by helium explosion leading to significant magnet damage the team driving
the worldwide lhc grid to production quality were relieved to gain extra time before full data rates as well as
simulations had to be handled chalmers much research and development into smart data movement and optimal data distributions as well
as workload scheduling was need to reach the necessary operational quality much investment in developing skills and organisation was needed
to achieve sustained running similarly for the square kilometre array ska the software campaign to fully exploit the capacity of
the synthesised aperture data acquisition via arrays of antennae forming the radio telescope has run in tandem with the physical
telescope design prior at lofar173 forms crucial input to this activity where major advances in data handling or data analysis
or modelling are required adequate investment and time must be allocated to the software the cost of software is roughly
to for its initial construction and to for its lifetime maintenance maintenance involves three significant aspects bug fixing dealing with
errors that were not exposed by the initial testing of maintenance adapting to context changes the underlying layers of software
the inter process communication and coordination frameworks and the external services on which the software may depend all change the
dominant drivers in the digital ecosystem are commercial science has little influence and must accept many changes as described above
section most partners in data intensive federation are autonomous organizations each driven by many pressures they will change their services
data formats choices of standards and so on without reference to others the sources of standards community chooses to follow
will when those sources refine their standards cascade into software upgrade requirements of maintenance enhancing functional and non functional capabilities
the moment new working practice or method is introduced it stimulates ideas and change there are already ambitions from the
user communities and many external stimuli these changes need to be resourced through the full software engineering lifecycle from design
to deployment if the science is to remain competitive of maintenance in addition commercial software vendors spend of their costs
on customer support whereas open source software projects spend of staff time on customer support swedlow without that customer support
which includes courses and on line help many users will fail to use the software successfully this will either lead
to them failing and not achieving their scientific goals or it will result in stream of bug reports exacerbating the
maintenance costs revisiting the comparison with an instrument prototype to prove that detection method works can be knocked up in
lab and tested without considering all of the engineering issues and lifetime calibration and maintenance tasks similarly software can be
built quickly to test an idea all too often it is then deployed into production context without considering the lifetime
costs were scientists to depend on it for their work of course much software never becomes widely deployed it is
used for short time hence the context doesn change it is used by an individual or small group and hence
latent errors are not exposed and then is forgotten hence never needs upgrades we should carefully provide an environment where
scientists with few co workers can easily build deploy and then pension off such software the focus of software sustainability
however is the subset of software that does persist does become key to the culture and working practices of community
and which therefore needs to be engineered with care as it has become mission critical dependency this subset should be
carefully identified there will be continuous stream of candidates management must choose the subset very carefully to not miss software
on which their community depends visible services operating systems and compilers etc get bundled into the provided platforms by default174
major simulation suites codes have much longer lifetimes than hardware platforms and tend to have community that is supported to
invest in their engineering and maintenance the challenge is to spot all of the subsystems and the glueware that assembles
them to provide the research environment often embedded in science gateways and in specific services such as cataloguing and curation
not expand the subset beyond the capacity of the software engineering resources they can muster significant role of management in
the software industry is to keep killing off software projects such projects often spring up from ideas and stimuli their
teams have some of those ideas are vital for the next project or the future of the company many are
not the same will be true for ideas emerging in an ri community with respect to their future success pruning
this subset so that the remaining software have enough engineering resources is continuous and demanding battle the ris and envriplus
should have in place the management effort and decision procedures to identify and maintain an explicit list of the software
elements that are in the subset that needs to be well engineered and carefully maintained175 after the end of the
envriplus project this responsibility has to transfer to the envri ris community at large there will be significant list which
will be beyond the resources of envriplus and the ris alone strategy is needed to handle this mismatch for more
background material and campaigns to raise this agenda with funders readers are referred to the work of the software sustainability
institute ssi three strategies are available for investing sufficient engineering effort in mission critical software buy or co develop software
from commercial vendor that delivers the quality maintenance and support as vendors often have millions of customers they can invest
in quality engineering and develop teams of experts in user support often this will not precisely match the research community
wants two ways around this are engage the vendor in your cause this may not be difficult as there are
many commercial opportunities that emerge when the vendor has new capability in their product fine example is the interaction between
the astrophysics community undertaking high data volume sky surveys with microsoft see section and szalay the handling of the data
intensive federation challenge is already pressing problem in many industries for example an accountant in one bank assessing risk has
to deal with seven different loan systems each time as there is no holistic view and no prospect of reducing
the number of loan systems confidential communication meet most of your needs with the vendor software often large part of
the logic or most of the functions may be obtained from the vendor software the extra functionality is written using
these the original requirement in the essential list is reduced to and replaced by this remaining wrapper the engagement of
the vendor considered above may still leave the envriplus community with some such residual engage in an open source software
campaign these are run under established regimes such as the apache software foundation177 the value of engaging or leading such
an open source campaign is that it should gather critical mass of contributors with sufficient software engineering skills to deliver
and maintain the required product this campaign must be well governed and recruit key experts for the various target platforms
for the optimisations required and for systematic testing it also needs the necessary user support team although the software licence
may be free participation implicit if you are serious user has to include contributions to the overall effort at the
very least user support for those with similar requirements as for commercial software the available open source products are unlikely
to match the envriplus requirement exactly again two options must be considered engage in the open source campaign working with
the open source community for this product extend it to meet your needs there is likely to be good support
for all the stages of software engineering needed as the active community will have established this there may also be
others in this open source product community who have similar requirement it will then be possible to pool your design
thinking and software engineering staff resources to deliver the new capability use the open source product with locally supported extension
this is the same as using commercial software to do as much as possible again the residual replaces the previous
member of the critically important software set take full responsibility for software requirement this is the last resort and the
full range of maintenance tasks and user support tasks must be resourced with suitably trained and skilled staff generally speaking
this is too demanding and expensive for single organisation or small user community to take on it is usually necessary
to form alliances to obtain or extend the necessary resources envriplus is just such an alliance this will then lead
to bespoke software tuned to meet the identified purpose without unnecessary overheads or unnecessary interfaces with other software it will
also result in long term responsibility for that software maintenance after care in selecting the mission critical subset and reduction
of costs using the above two strategic options there will be residual set of software items with reduced functionality where
possible that needs to be built and maintained locally resourced team may open up new territory and yield product that
can move into the commercial sector or initiate an open source campaign in most cases it will not and all
of the resources will need to be found and sustained locally for each member of the critical software list the
envriplus and ris will need to decide how best to treat it according to the above three strategic options taking
into account the ri lifetime costs they will then need to find or pay for as outsourcing contracts the necessary
skills and software engineering capacity and sustain that resource for as long as their user communities depend on their software
there are traps that catch the unwary and these will need to be managed throughout the lifetimes of the ris
for example depending on vendor product that is discontinued or changes its licence model depending on an open source project
that loses critical mass new middleware platform or toolset overtaking one that was previously best missing critical software because it
was not high profile no one knew it was there or what it does inheriting or choosing to build on
unmaintainable software failing to adopt the standard that becomes widely adopted as well as taking responsibility for their own bespoke
software there are three forms of shared software maintenance that every ri community with an software dependency needs to be
responsible for their fair contribution to the multi community software elements the mappings to and integration between the common software
elements to meet their specific needs and on the hopefully rare occasions when major element needs to be replaced by
thriving alternative the integration of that alternative today this maintenance investment is only available for novelty items and recognised simulation
codes many other software elements need maintenance for the investments in infrastructure to survive and for the improved research environment
to be sustained funders research strategists organisations providing platforms infrastructure builders and vre developers need to form alliances to achieve
this for the research infrastructures that are strategically important the communities of researchers and others using these facilities may need
to campaign for this to be included in the long term agenda to summarise mattmann identified sustaining four research tracks
as critically important for future data science based on his many years of experience at nasa and at the apache
software foundation mattmann rapid scientific algorithm integration intelligent data movement use of cloud computing harnessing the power of open source
in software development for science the first three of these depend on software and in many cases it is built
with substantial component of open source input identified in the fourth bullet all of the routine user interaction is through
vres and portals that require many elements of software in and behind them all of the data handling throughout the
data lifecycle depends on software tools workflows and services all of the innovation depends on shared development environments ides and
apis again totally dependent on software the dependency on software is pervasive the commitment to sustaining all such critical software
has to be equally pervasive it must be sustained for the lifetimes of the ris assessing the data identification and
citation technology review section gives very clear explanations of the value of good quality working practices for identifying and referencing
all items of data that are or may become significant in research it highlighted two pervasive challenges faced by all
those who are engaged in stages of the data lifecycle or are using or producing data in their research or
for decision support one there are diverse suggestions but not agreed and widely adopted standards underpinning the necessary actions whether
those actions are carried out by humans or software two today there aren good tools and technologies that make it
easy for humans or software to perform these tasks efficiently there is great deal of work underway and we can
be optimistic about viable deployable support for data identification and citation becoming available within the next few years this poses
another two challenges one how to identify and align with the software and methods that will be most widely supported
and adopted and two how best to use the emerging software metadata standards and proposed methods in the envriplus context
that requires developing standard practices metadata and protocols that allow interworking within and between the ris and other organisations that
is an issue prevalent in nearly all ris for all technology topics indeed cataloguing curation and provenance all need to
make effective use of the functionality and facilities data identification and citation will provide conversely the work on catalogues may
provide facilities for pid registries with associated metadata scientists in each field will need to associate their identified items with
concepts in their view of the natural world terms for widely adopted agreed concepts may be identified by standardised vocabularies
underpinned by formal ontologies see section for such agreed concepts these external references provide identification and citation however scientists may
take different views of the phenomena they observe or they may be developing conceptual framework for new phenomena new species
that they have identified in this case they need framework for defining and citing the new concepts that they manage
and develop presumably this would use the data identification and citation machinery when their contributions reach acceptance or are published
these localised identities should easily migrate into the standard reference space of managed identities conversely if they fail to establish
evidence to back up their idea their localised developments will not affect researchers other than those they are currently collaborating
with optimisation will interact with data identification for two reasons both caching and the co location of data processing and
derived data depend on precise data referencing integration into workflows of the functions required for data identification and citation is
