was initially developed for the enactment of bioinformatics workflows it is now widely used it emphasises usability providing graphical user
interface for workflow modelling and monitoring as well as comprehensive collection of predefined services galaxy blankenberg is web based system
that aims to bring computational data analysis capabilities to non expert users in the biological sciences domain the main goals
of the galaxy framework are accessibility to biological computational capabilities and reproducibility of the analysis result by tracking the information
related to every step on the process the galaxy workflow model does not follow the dag paradigm as it allows
to define loops being directed cyclic graphs dcgs approach knime beiskenr shares many characteristics with taverna with stronger focus on
user interaction and visualisation of results yet with smaller emphasis on web service invocation furthermore knime focuses on workflows from
the fields of data mining machine learning and chemistry while taverna is more concerned with integration of distributed and possibly
heterogeneous data graphical user interface facilitates design and execution monitoring of workflows kepler is frequently used graphical swms similar to
taverna and knime it provides an assortment of built in components with major focus on statistical analysis kepler workflows are
written in moml an xml format or kar files which are an aggregation of files into single jar file kepler
is built on top of the ptolemy ii java library from which it inherits the concepts of directors and actors
the former ones control the execution of the workflow while the actors execute actions when specified by directors apache airavata
marru is an open source open community swms to compose manage execute and monitor distributed applications and workflows on computational
resources ranging from local resources to computational grids and clouds airavata builds on general concepts of service oriented computing distributed
messaging and workflow composition and orchestration these examples are task oriented that is their predominant model has stages that correspond
to tasks and they organise their enactment on wide range of distributed computing infrastructures dci normally arranging data transfer between
stages using files vahi these systems have achieved substantial progress in handling data intensive scientific computations in astrophysics in climate
physics and meteorology in biochemistry in geosciences and geo engineering and in environmental sciences in this category we could also
include other works like swift wilde trident simmhan ws pgrade guse kozlovszky shiwa er flow alternative approaches to task oriented
workflows are the stream based workflows this mirrors the shared nothing composition of operators in database queries and in distributed
query processing that has been developed and refined in the database context data streaming was latent in the auto iteration
of taverna it has been developed as an option for kepler and it is the model used by meandre acs
and by swift which supports the data object based operation using its own data structure data streaming pervaded the design
of dispel atkinson dispel was proposed as means of enabling the specification of scientific methods assuming stream based conceptual model
that allows users to define abstract machine agnostic fine grained data intensive workflows dispel4py filgueira implements many of the original
dispel concepts but presents them as python constructs it describes abstract workflows for data intensive applications which are later translated
and enacted in distributed platforms apache storm mpi clusters etc bobolang falt relative new workflow system based on data streaming
has linguistic forms based on and focuses on automatic parallelisation it also supports multiple inputs and outputs meaning that single
node can have as many inputs or outputs as user requires currently it does not support automatic mapping to different
distributed computing infrastructures dcis for data analytics frameworks and platforms lot of variety exists including apache mahout is platform offering
set of machine learning algorithms including collaborative filtering classification clustering designed to be scalable and robust some of these algorithms
rely on apache hadoop others are relying on apache spark apache hadoop is basic platform for distributed processing of large
datasets across clusters of computers by using mapreduce strategy in the reality this is probably the most famous open source
implementation of mapreduce simplified data processing approach to execute data computing on computer cluster li worth to highlight that one
of the major issues with mapreduce resulting from the flexibility key feature users are called to implement the code of
map and reduce functions is the amount of programming effort in fact other frameworks and platforms are building on it
to provide users with data analytics facilities apache mahout apache spark is an open source general purpose cluster computing engine
which is very fast and reliable it provides high level apis in java scala python and and an optimised engine
that supports general execution graphs it also supports rich set of higher level tools including spark sql for sql and
structured data processing mllib for machine learning graphx for graph processing and spark streaming gcube data analytics candela coro is
an open source solution conceived to offer an open set of algorithms with the as service paradigm the platform relies
on set of dcis for executing the computing tasks including d4science and egi this platform is equipped with more than
ready to use algorithm implementations which include real valued features clustering functions and climate scenarios simulations niche modelling model performance
evaluation time series analysis and analysis of marine species and geo referenced data new algorithms can be easily integrated in
fact the platform comes with development framework dedicated to this java algorithms as well as scripts are well supported once
integrated each algorithm is automatically exposed via rest based protocol ogc wps as well as via web based gui that
is complete dashboard for executing computations by guaranteeing open science practices every computation leads to research object recording and making
available every piece of the task ipython jupyter perez is notebook oriented interactive computing platform which enacts to create and
share notebooks documents combining code rich text equations and graphs notebooks support large array of programming languages including and communicate
with computational kernels by using json based computing protocol similar solutions include knitr which works with the coding language and
dexy is notebook like program that focuses on helping users to generate papers and presentations that incorporate prose code figures
and other media the heterogeneity characterising these systems make evident that when discussing data processing technologies there are different angles
perspectives and goals to be taken into account when analysing technologies from the scientist perspective the following envisaged trends should
be taken into account technology should be ease of re use it should not distract effort from the pure processing
task scientists should be exposed to technologies that are flexible enough to enable them to quickly specify their processing algorithm
pipeline it should not require them to invest effort in learning new programming languages or in deploying configuring or running
complex systems for their analytics tasks methods and algorithms are expected to be reused as much as possible thus data
processing should enable them to be published and shared as service rather than do it yourself scientists should be provided
with an easy to use working environment where they can simply inject and execute their processing pipelines without spending effort
in operating the enabling technology this make it possible to rely on economies of scale and keep the costs low
solutions should be hybrid it is neither suitable nor possible to implement one single solution that can take care of
any scientific data processing need certain tasks must be executed on specific infrastructures certain tasks are conceived to crunch data
that cannot be moved on other machines from where they are stored these trends actually suggest that scientists are looking
for workbenches virtual research environments virtual laboratories candela 2013b providing them with easy to use tools for accessing and combining
datasets processing workflows that behind the scene transparently exploit wealth of resources residing on multiple infrastructures and data providers according
to their policies such environments should not be pre cooked rigid rather they should be flexible thus to enable scientists
to enact their specific workflows they should provide their users with appropriate and detailed information enacting to monitor the execution
of such workflow and be informed of any detail occurring during the execution finally they should promote open science practices
they should record the entire execution chain leading to given result they should enact others to repeat repurpose an existing
process longer term horizon data processing is strongly characterised by the one size does not fit all philosophy it does
not exist and will never exist single solution that is powerful and flexible enough to satisfy the needs arising in
diverse contexts and scenarios the tremendous velocity characterising technology evolution calls for implementing data sustainable processing solutions that are not
going to require radical revision by specialists whenever the supporting technologies evolve whenever new platform capable of achieving better performance
than existing ones becomes available users are enticed to move to the new platform however such move does not come
without pain and costs data analytics tasks tend to be complex pipelines that might require combining multiple processing platforms and
solutions exposing users to the interoperability challenges resulting from the need to integrate and combine such heterogeneous systems strongly reduce
their productivity there is need to develop data processing technologies that tend to solve the problem by abstracting from and
virtualising the platform that take care of executing the processing pipeline such technologies should go in tandem with optimisation technologies
see section and should provide the data processing designer with fine grained processing directives and facilities enabling to specify in
detail the processing algorithm relationships with requirements and use cases most of the ris that participate in envriplus have computer
based scientific experiments which need to handle massive amounts of data being some of them generated every day by different
sensors instruments or observatories in most cases they have to handle primary data streams as well as data from institutional
and global archives their live data flows from global and local networks of digital sensors and streams from many other
digital instruments often they employ the two stage handling of data established initial collection with quality monitoring then an open
ended exploration of data and simulation models where researchers are responsible for the design of methods and the interpretation of
results these researchers may want to re cook relevant primary data according to their own needs their research context has
the added complexity of delivering services such as hazard assessments and event earthquake detection and categorisation which may trigger support
actions for emergency responders they therefore have the aspiration to move innovative methods into service contexts easily data streaming is
essential to enable users such scientists from atmosphere biosphere marine and solid earth domains to move developed methods between live
and archived data applications and to address long term performance goals the growing volumes of scientific data the increased focus
on data driven science and the areal storage density doubling annually kryder law several stress the available disk or more
generally the bandwidth between ram and external devices this is driving increased adoption of data streaming interconnections between workflow stages
as these avoid write out to disk followed by reading in or double that load if files have to be
moved therefore data streaming workflows are gaining more and more attention in the scientific communities another aspect to be considered
is that scientific communities tend to use wide range infrastructures for running their data intensive applications hpc clusters supercomputers and
cloud resources therefore workflow systems that are able to run them at scale on different dcis without users making changes
to their codes are currently in trend it is also necessary to provide facilities to run data intensive applications across
platforms on heterogeneous systems because data can be streamed to and from several dcis for performing various analyses for these
dcis it is not feasible to store all data since new data constantly arrive and consumes local store space therefore
after data are processed and become obsolete they need to be removed for newly arrival data so data stream workflow
systems should be combined with traditional swms systems which effectively coordinate multiple dcis and provide functions like data transfers data
clean up data location and transfer scheduling all in all the requirements for data processing are very heterogeneous evolving and
varied simply because diverse are the needs when moving across communities and practitioners moreover even within the same community there
are diverse actors having different perceptions ranging from data managers that are requested to perform basic data processing tasks to
data scientists willing to explore and analyse available data in innovative ways when analysed from the perspective of data scientists
the problem tends to become even more challenging because data are heterogeneous and spread across number of diverse data sources
thus before being analysed for the sake of the scientific investigation the data need to be acquired and prepared for
the specific need steps will be needed to refine the understanding of these requirements to identify consistent and significant groups
where the supplied toolkit for infrastructures may offer common sharable solutions developing that clarity may be another focus for think
tank issues and implications scientific workflows have emerged as flexible representation to declaratively express complex applications with data and control
dependences wide range of scientific communities are already developing and using scientific workflows to conduct their science campaigns however managing
science workflows for synergistic distributed and extreme scale use cases is extremely challenging on several fronts workflow management system design
interaction of workflow management with os and provisioning scheduling systems data movement and management for workflows programming and usability advanced
models provenance capture and validation to name few major challenge for envriplus ris applications is the integration of instruments into
the scientist workflow many scientists retrieve the data from web and or archive facility provided by their ris and then
realise some post analyses not many ris offer the possibility to work with life data streamed directly from their instruments
sensors therefore how the ict workflows community can enable seamless integration of live experimentation with analysis in way that increases
the overall turnaround time and improves scientific productivity can be identified as one of the major challenges which involve provisioning
models algorithms and mechanisms for resource provisioning compute data storage and network this includes open questions like how to efficiently
determine the resources necessary for workflow execution over time what information needs to be exchanged between the wms and resource
provisioning systems how does the wms adapt to the changes in resource availability execution examining the interplay between the wms
and system side services data movers schedulers etc wms and the operating system or hardware present on the hpc platform
issues of not only performance but also energy efficiency need to be taken into account support streaming data models and
manage trade offs of performance persistence and resilience of data movements adaptation novel approaches to workflow resilience and adaptation this
includes how does the wms discover hard and soft failures there are several open questions that need to be addressed
can provenance information help in detecting some of these anomalies and the corresponding root causes how does the wms adapt
to changes in the environment to failures to performance degradations how is the resource provisioning workflow scheduling etc impacted how
do we steer and reschedule workflows when there are failures provenance what information needs to be collected during execution to
support provisioning execution and adaptation what metrics and metadata need to be in the provenance store and what should be
the level of detail frequency of provenance information collection identification and interaction with all the system layers to collect the
provenance data best strategy to store the provenance data development of provenance analysis models to analyse large and complex provenance
information analytical modelling exploration of more complex hardware and workflow designs including novel memory architectures with in situ analysis and
co processing collaboration another important aspect of the problem is the ability to support workflows within scientific collaborator and related
to that how to support the execution of set of workflows workflow ensemble on behalf of the user of collaboration
and how to describe and map collaboration workflows besides complex scientific workflows lot of scientists are willing to specify their
data processing algorithms by realising what falls under the research software umbrella this represents valuable research asset that is gaining
momentum thanks to the open science movement lot of such software is actually implemented by people having limited programming skills
and computing resources in these scenarios environments conceived to use the software as is and with minor directives annotations enact
its execution by relying on distributed computing infrastructure are of great help coro this might enable the scientist to easily
execute the code on number of machines greater than the one he she usually use this might enable to expose
the algorithm as service and thus to include it in scientific workflows further discussion of the processing technologies can be
found in section this takes longer term perspective and considers relations with strategic issues and other technology topics provenance technologies
barbara magagna umweltbundesamt gmbh eaa introduction context and scope provenance deriving from the french term provenir with the meaning to
come from was originally used to keep track of the chain of ownership of cultural artefacts such as paintings and
sculptures as it determines the value of the artwork but this concept becomes more and more important also in the
data driven scientific research community here it is used synonymously with the word lineage meaning origin or source the knowledge
about provenance of data produced by computer systems could help users to interpret and judge the quality of data lot
better in the w3c prov93 documents provenance is defined as information about entities activities and people involved in producing piece
of data or thing which can be used to form assessments about its quality reliability or trustworthiness sources of state
of the art technology information used as this topic is intensively studied both from the research viewpoint and from the
viewpoint of those deploying and using provenance in production contexts there are large number of relevant papers and reports cited
from the text and further identified in the reference section from page onwards the following urls identify other useful sources
https www w3 org tr prov overview https rd alliance org groups research data provenance html https rd alliance org
sites default files krohnposter pdf http de slideshare net drshorthair om alignment with ssn prov oboe bfo http twiki ipaw
info bin view challenge webhome https github com nceas open science codefest wiki provenancer https www dataone org webinars provenance
and dataone facilitating reproducible science http eprints soton ac uk opm pdf http d2i indiana edu provenance https kepler project
org users add_on_modules provenance http www taverna org uk documentation taverna provenance http www mygrid org uk projects semantic provenance
project https www eudat eu semantics http sead data net short term analysis of state of the art and trends
already by early provenance of the scientific results was regarded as important as the result itself moreau considers that in
order to support reproducibility workflow management systems are required to track and integrate provenance information as an integral product of
the workflow consequently tan distinguishes between workflow provenance or coarse grained which refers to the record of the entire history
of the derivation of the final output of the workflow and data or fine grained provenance which gives detailed account
of the derivation of piece of data that is in the result of transformation step specified in database query krohn
calls the latter the database provenance with its sub concepts why where and how provenance these describe relationships between data
in the source and in the output for example by explaining where output data came from in the input bunemann
showing inputs that explain why an output record was produced bunemann or describing in detail how an output recording was
produced cheney krohn adds to this characterisation third type provenance of web resources with its sub concept access provenance including
both actions of publication and consumption of data hartig provides base for research on the provenance of linked data from
the web park describes republishing as the process of transforming sensor data across the internet lebo introduces prov pingback which
enables parties to discover what happened to objects they created after they have left their domain of influence following the
linked data principles researchers still face the challenging issue that the provenance of the data products they create is often
irretrievable in many cases the tools for composing lineage metadata are not provided with the software used for scientific data
processing bose sees also the problem that no definitive method standard or mandate exists for preserving lineage of computational results
while this was true in the early the provenance community reached significant milestone in when the world wide web consortium
w3c published its provenance documents although combining prov with linked data offers great potential for discovery access and use of
provenance data the research community needs practical answers about how to do it solutions are necessary to bridge the gap
between existing systems built on technologies not well suited to adopting linked data design and an interconnected web of provenance
with other systems lebo stehouwer comes to the same conclusion there seems to be consensus that it would be very
good to move away from manually executed or ad hoc script driven computations to automated workflows but there is still
reluctance to take this step traditional approaches of provenance management have focused on only partial sections of data lifecycle and
they do not incorporate domain semantics which is essential to support domain specific querying and analysis by scientists sahoo often
analysis has to be performed on scientific information obtained from several sources and generated by computations on distributed resources this
unleashes the need for automated data driven applications that also can keep track of the provenance of the data and
processes with little user interaction and overhead altintas comprehensive provenance frameworks as proposed by sahoo garijo 2014a myers or filgueira
seem to be the adequate answer to overcome these challenges these approaches differ from each other and are described below
in more detail the following section specifies some basic issues related to provenance see simmhan uses subject representation storage dissemination
tools collection supported by scientific workflows and by semantic based provenance systems different uses of provenance can be envisaged while
currently specific provenance systems typically only support couple of them simmhan data quality lineage can help to estimate data quality
and data reliability based on the source data and transformations it is also used for proof statements on data derivations
audit trail provenance can trace the audit trail of data determine resource usage and detect errors in data generation the
process that creates an audit trail runs typically in privileged mode so it can access and supervise all actions from
all users this makes not only the data lineage transparent but also the use of data after its publication which
could expose sensitive and personal information it is questionable if usage tracking should be by product of provenance which normally
should just focus on the origins and transformations of the data product rather than on its users bier replication recipes
detailed provenance information can allow repetition of data derivation attribution pedigree of data can give credit and legal attribution to
the data producers enable its citation and determine liability in case of erroneous data summaries of such records are useful
when funders review the value of continuing support for data services informational generic use of provenance is to query based
on lineage metadata for data discovery by browsing it context to interpret data is provided the subject of provenance information
can be of different types as already mentioned above depending on its transparency data oriented provenance is gathered about the
data product and is explicitly available process oriented deduced indirectly provenance focuses on the deriving processes inspecting the input and
output data products the granularity at which provenance is detected determines the cost of collecting and storing the related information
the range spans from provenance on attributes and tuples in database to provenance of collections of files representation of provenance
different techniques can be used depending on the underlying data processing system annotation metadata including derivation history of data product
is collected as annotations and descriptions this information is pre computed and thus readily usable as metadata inversion derivations can
be inverted automatically to find the source data supplied to them to derive the output data queries user defined functions
in databases this method is more compact provenance related metadata is either directly attached to data item or its host
document or it is available as additional data on the web hartig both types may be represented in rdf using
vocabularies or it may be data of another form the most common representation languages used are xml rdf owl using
domain ontologies cerif dispel4py various vocabularies and ontologies exist that allow users to describe provenance information with rdf data provenance
models during session on provenance standardization at the international provenance and annotation workshop ipaw the first provenance challenge on simple
example workflow was set up in order to provide forum for the community to understand the capabilities of different provenance
systems and the expressiveness of their representations moreau after the third provenance challenge the open provenance model opm consolidated itself
as the de facto standard for representing provenance and was adopted by many workflow systems the interest of having standard
led to the w3c provenance incubator group which was followed by the provenance working group this effort produced the family
of prov specifications94 which are set of w3c recommendations on how to model and interchange provenance in the web opm95
in opm open provenance model provenance is represented by graphs it is used to describe workflow executions the nodes in
this graph represent three different types of provenance information resources created as artefacts immutable pieces of state steps used as
processes actions or series of actions performed on artefacts and the entities that control those processes as agents the edges
are directed and have predefined semantics depending on the type of their adjacent nodes used process used some artefact wascontrolledby
an agent controlled some process wasgeneratedby process generated an artefact wasderivedfrom an artefact was derived from another artefact and wastriggeredby
process was triggered by another process roles are used to assign the type of activity that artefacts processes and agents
played in their interaction and accounts are particular views on the provenance of an artefact opm is available as two
different ontologies which are built on top of each other the lightweight opm vocabulary opmv and the opm ontology opmo
with the full functionality of the opm model the prov model is very much influenced by opm here resources are
modelled as entities which can be mutable or immutable the steps used as activities and the individuals responsible for those
activities as agents seven types of relationships are modelled used an activity used some artefact wasassociatedwith an agent participated in
some activity wasgeneratedby an activity generated an entity wasderivedfrom an entity was derived from another entity wasattributedto an entity was
attributed to an agent actedonbehalfof an agent acted on behalf of another agent and wasinformedby an activity used an entity
produced by another activity roles are kept to describe the type of relationship and the means to qualify each of
the relationships using an ary pattern are provided opm introduces the concepts plan associated with certain activity and prov statements
grouped in bundles defined as entities figure the communalities between prov left and opm right garijo 2014a the prov family
of documents provides among others an ontology prov the data model prov dm and an xml schema prov xml provenir
sahoo is domain upper ontology provenance ontology used in translational research it is consistent with other upper ontologies like sumo
suggested upper merged ontology bfo basic formal ontology and dolce descriptive ontology for linguistic and cognitive engineering provenir extends primitive
philosophical ontology concepts of continuant and occurent along with ten fundamental relationships the three top level classes are data process
and agent where data is specialised in the classes data_collection and parameter spatial temporal and thematic provenir is used in
the semantic provenance framework spf as explained below plan garijo 2014a in order to be able to represent workflow templates
and workflow instances garijo 2014a extended prov the plan concept is derived from prov plan the step concept represents the
planned execution activities and the inputs of step are modelled as variable with the properties type restrictions and metadata opmw
garijo 2014a is designed to represent scientific workflows at fine granularity opmw extends plan prov and opm it is able
to model the links between workflow template workflow instance created from it and workflow execution that resulted from an instance
additionally it supports representation of attribution metadata about workflow opmw is used as provenance representation model in the west workflow
ecosystem alignments with prov96 to be compliant with the ogc standard iso geographic information observation and measurement simon cox made
efforts to align with prov in an observation is an action whose result is an estimate of the value of
some property of the feature of interest obtained using specified procedure provenance storage in case the data is fine grained
provenance information can become larger than the data it describes this determines its scalability this is particularly true when annotations
are added manually instead of automatically collecting them provenance dissemination in order to use provenance system should allow rich and
diverse means to access it these can include provenance mining visualisation and browsing if provenance is stored in rdf owl
it is possible to query using sparql many tools have been developed for prov for this purpose visualisation tool like
prov viz produces derivation graphs that users can browse and inspect garijo 2014a provenance collection might be performed by stand
alone tools such as provenancer97 which enables provenance capture in but these are of more useful when embedded in workflow
system provenance collection supported by scientific workflow systems data analysis can be facilitated by scientific workflow systems that have the
ability to make provenance collection part of the workflow here the provenance should include information about the context in which
the workflow was used execution that processed the data and the evolution of the workflow design among the most popular
of these are taverna kepler and pegasus here only few described in some detail see also descriptions in section kepler
is cross project collaboration to develop scientific workflow system for multiple disciplines that provides workflow environment in which scientists can
design and execute workflows kepler uses ptolemy ii software java based system and set of apis the focus is to
build models based on the composition of existing components called actors and observe the behaviour of these simulation models when
executed using different computational semantics called directors formerly provenance recorder had been implemented to be configured as director with standard
configuration menu and becoming part of the workflow definition altintas today the kepler provenance enriches the capabilities of the workflow
as add on module suite provenance is toggled on and off in the kepler toolbar when on and when running
workflow with supported director sdf ddf or pn execution details are recorded into database in the keplerdata modules provenance directory
this powerful feature is leveraged by modules such as reporting and the workflow run manager which provides gui to manage
and share your past workflow runs and results98 the dispel4py data streaming system filgueira spinuso is versatile data intensive kit
presented as standard python library it describes abstract workflows for stream based applications which are later translated and enacted in
distributed platforms it allows users to define abstract machine agnostic fine grained data intensive workflows scientists can easily express their
requirements in abstractions closer to their needs without demanding knowledge of the hardware or middleware context in which they will
be executed processing element pe is computational activity it encapsulates an algorithm or service and is instantiated as node in
workflow graph users only have to use available pes from the dispel4py libraries and registry and connect them as they
need in graphs which leads to extensive re usability the provenance management system of dispel4py consists of comprehensive system which
includes extensible mechanisms for provenance production web api and visualisation tool the api is capable of exporting the trace of
run in the w3c prov json representation to facilitate interoperability with third party tools provenance collection supported by semantic based
provenance systems taverna is an open source and domain independent workflow management system comprising suite of tools to design and
execute scientific workflows it has been created by the mygrid team and is funded by fp7 projects biovel scape and
wf4ever it is written in java and includes the taverna engine used for enacting workflows that powers both taverna workbench
the client application and taverna server executing remote workflows taverna automates experimental methods through the use of number of different
services from diverse set of domains it enables scientist who has limited background in computing limited technical resources and support
to construct highly complex analyses over data and computational resources workflow sharing is arranged via myexperiment taverna can capture provenance
of workflow runs including individual processor iterations and their inputs and outputs this provenance is kept in an internal database
which is then used to populate the history results in the results perspective in the taverna workbench the provenance trace
can be used by the taverna prov plugin to export the workflow run including the output and intermediate values and
the provenance trace as prov rdf graph which can be queried using sparql and processed with other prov tools such
as the prov toolbox within taverna workflow can be annotated to give attribution to the authors of workflow or nested
workflow although taverna is not semantic based it supports the semantic description of workflows the semantic provenance framework spf sahoo
provides unified framework to effectively manage provenance of translational research data during pre and post publication phases it is underpinned
by an upper level provenance ontology provenir that is extended to create domain specific provenance ontologies to facilitate provenance interoperability
seamless dissemination of provenance automated querying with sparql and analysis to collect provenance information at first stage existing data stored
in rdb was converted to rdf with help of d2rq using the domains specific parasite experiment ontology peo on second
stage an ontology driven web form generation tool called ontology based annotation tool ontoant was developed to dynamically generate web
forms for use in research projects to capture provenance information consistent with peo in rdf the spf stores both the
dataset and provenance information together in single rdf graph this allows for application driven distinction between provenance metadata and data
and additionally facilitates that updates of data are seamlessly applied to the associated provenance the west workflow ecosystem garijo 2014a
integrates different workflow tools with diverse functions workflow design validation execution visualisation browsing and mining created by variety of research
groups workflow representation standards and semantic technologies are used to enable each tool to import workflow templates and executions in
the format they need west uses and extends the open provenance model and the w3c prov standard by plan which
is able to represent plans the extension is considered necessary because the opm and prov models are not able to
represent workflow templates and workflow instances the opmw vocabulary is designed to represent scientific workflows at fine granularity built upon
plan opm and prov and allowing the linking between workflow template workflow instance created from it and workflow execution that
resulted from an instance garijo 2014a demonstrate the efficiency of such an approach by the usage of different tools such
as wings for generating workflows workflow execution engines such as pegasus the fragflow system for workflow mining prov viz for
visualising provenance structures wexp for exploring different workflow templates the organic data science wiki an extension of semantic wikis for
workflow documentation and virtuoso as workflow storage and sharing repository life science grid lsg cao is cyber infrastructure framework supporting
interactive data exploration and automated data analysis tools it uses the karma provenance framework100 developed at indiana university to capture
raw provenance events and to format them according to the open provenance model specification additionally it integrates automated semantic enrichment
of the collected provenance metadata using the semantic open grid service architecture ogsa semantic annotation framework developed at university of
manchester the sustainable environmental actionable data sead provides data curation and preservation services to deploy those services for beneficial use
to active research groups it intends to support the long tail of smaller projects in sustainability science assuming that metadata
could be used to help organise and filter data during research the sead approach allows data and metadata to be
added incrementally and the generation of citable persistent identifiers for data it comprises three primary interacting components project spaces virtual
archive and researcher network the project space is secure self managed storage with tools that allow research groups to assemble
semantically annotate and work with data resources the web application leverages the tupelo semantic content middleware developed at ncsa which
provides blob plus rdf metadata abstraction over an underlying file system and rdf store the web application itself is an
extension to the java based medici semantic content management web application sead has also added set of restful web services
that can be used within the analysis application to read and write data with desired provenance and metadata sparql query
service is also implemented the virtual archive is service that manages publication of data collections from project spaces to range
of long term repositories it is federated layer over multiple repositories that manages an overall packaging and publication workflow and
provides global search capability across data published via sead it leverages the komadu provenance service102 which is stand alone provenance
collection tool that can be added to an existing cyberinfrastructure for the purpose of collecting and visualising provenance data it
supports the w3c prov specification komadu is the successor of the karma provenance tool which is based on opm another
semantic tool which can be adopted for provenance information collection is b2note103 the eudat project developed first prototype version using
python and common semantic python libraries like rdflib and sparqlwrapper this webservice allows annotation of imported text documents with terms
coming from bioportal envthes and gemet from eionet this prototype is currently being tested and extended using the django restful
framework to be further integrated with the lter lifewatch portal longer term horizon in order for data driven research to
be reproducible it is an essential requirement to define unambiguously all data inputs analysis steps and data products as well
as software and algorithms used with persistent identifiers this will allow for connections to cataloguing and maintenance of provenance records
supporting automated metadata extraction and production for machine actionable workflows future provenance management developments will have to implement interoperability functions
of workflows the need for global inter disciplinary collaborations will continue to grow with demands for scientific data to be
shared processed and managed on different distributed computational infrastructures provenance management should embrace the whole lif cycle of data and
incorporate domain semantics by encouraging and building on controlled vocabularies formalised as ontologies see section which is essential to support
domain specific querying and analysis by scientists the approach used for provenance representation has significant impact on the storage dissemination
and querying phases of the provenance lifecycle sahoo provenance analytics and visualisation techniques will receive more attention in future applied
research spinuso so far it has been largely unexplored by analysing and creating insightful visualisations of provenance data scientists can
debug their tasks and obtain better understanding of their results davidson cao relationships with requirements and use cases requirements there
is big interest among the ris to get clear recommendations from envriplus about the information range provenance should provide this
includes drawing an explicit line between metadata describing the dataset and provenance information also it should be defined clearly whether
usage tracking should be part of provenance it is very important to provide support for automated tracking solutions and provenance
management apis to be applied in the specific science environments although there are some thesauri already in use there is
demand for getting good overview of the existing vocabularies and ontologies that are ready to use or that need to
be slightly adapted for specific purposes work packages there is strong relationship between wp and the wp task provenance as
there must be direct link between the data and its lineage that can be followed by the interested user the
recommendations provided for data identification and citation should be used in provenance service solutions provenance tracking is also an important
feature for the tasks processing and optimisation the connections with the tasks curation and cataloguing are evident as well as
all of these recommendations must be built upon the same data model semantically and technically speaking as defined in the
task semantic linking framework and integrated in the task interoperation based architecture design relationships with use cases as foreseen in
wp9 ic_1 dynamic data citation connections to cataloguing and maintenance of provenance records supporting automated metadata extraction and production for
machine actionable workflows ic_2 provenance aims amongst others at defining minimum information set that has to be tracked finding conceptual
model for provenance which conforms to the needed information maps existing models to the common model and finds repository to
store the provenance information ic_06 identification citation in conjunction with provenance is aimed at identifying good practices for using pids
for recording provenance throughout the data object lifecycle including workflows and processing ic_8 cataloguing curation and provenance is the implementation
case for catalogues fulfilling curation and provenance requirements ic_9 provenance use of doi for tracing data re use provenance capture
techniques will be used as background for this use case ic_11 semantic linking framework interoperability and semantic linking across catalogues
datasets with observation systems and persons upon common data and metadata model will be provided by this use case issues
and implications commonality of metadata elements across curation provenance cataloguing and more thus common metadata and provenance scheme based on
widely adopted international standards should be used link to existing vocabularies and ontologies to enable domain semantic provenance representation thus
strong collaboration with the semantic working group having better visualisation tools at hand for provenance dependencies will increasingly help to
reduce the ris reluctance to adopt workflow solutions with provenance functionalities thus it is important to follow related developments and
to try to implement the most relevant one in the provenance service envriplus should consider collaborating with eudat on the
development of provenance tools as foreseen in wp and influence the general execution framework gef so that it supports the
provenance collection functionality envriplus should follow the rda provenance working groups and participate provenance in envriplus is task which is
due in later stage of the project thus it is must to follow in the meantime tools and services now
under development that will allow seamless linking of data articles people supporting streamlining of the entire data management cycle virtually
instantaneous extraction of metadata and provenance information and facilitating data mining and other machine actionable workflows further discussion of the
provenance technologies can be found in section this takes longer term perspective and considers relations with strategic issues and other
technology topics optimisation technologies paul martin universiteit van amsterdam uva the optimisation work is scheduled for later in envriplus hence
this section is preliminary as the challenges on which optimisation must focus are not yet decided however virtually every ri
agrees that the priorities should be chosen to improve productivity of researchers introduction context and scope system level environmental science
involves large quantities of data often diverse and dispersed insofar as there are many different kinds of environmental data commonly
held in small datasets in addition the velocity of data gathered from detectors and other instruments can be very high
data driven experiments require not only access to distributed data sources but also parallelisation of computing tasks for the processing
of data the performance of these applications determines the productivity of scientific research and some degree of optimisation of system
level performance is urgently needed by the ri projects in envriplus as they enter production this topic focuses on how
to improve many of the common services needed to perform data analysis and experiments on research infrastructure with an emphasis
on how data is delivered and processed by the underlying infrastructure there needs to be consideration of the service levels
offered by infrastructures and of the available mechanisms for controlling the system level quality of service qos offered to researchers
this topic should therefore focus on the mechanisms available for making decisions on resources services data sources and potential execution
platforms and on scheduling the execution of tasks the semantic linking framework developed in task on linking data infrastructure and
the underlying network can be used to embed the necessary intelligence to guide these decision procedures semi autonomously ultimately based
on the relevant task of the envriplus project we will need to provide an effective mapping between research level quality
attributes ease of use responsiveness workflow support to infrastructure level quality attributes on computing storage and network services provided by
underlying infrastructures define test bed requirements for software and services and identify conditions for operating final software and services inside
each domain and between multiple domains extend and customise existing optimisation mechanisms for computing and storage resources and provide an
effective control model between processes of data analysis and the underlying infrastructure resources making the application performance as easy as
possible to control at runtime thus the focus of the technology review in envriplus from the optimisation perspective is to
determine two things what the ri projects already have at their disposal for effective data access delivery and processing what
mechanisms can be used to meet ri projects processing and optimisation requirements104 the optimisation section of the envriplus technology review
focuses on the second point above the first point should be addressed in other parts of section particularly section short
term analysis of state of the art and trends in principle optimisation can be conducted at every level of interaction
at the social level between investigators at the human computer interface level between researchers and their tools at the service
level at the functional level at the infrastructure level and so forth any number of optimisations can be applied at
each of these levels based on an understanding of the technologies and engineering currently being used at that particular level
thousand different bespoke manipulations in order to ensure perfect operation in reality while there will always be scope for hand
crafted solutions to every problem where the payoff is sufficient to offset the effort required to understand produce and maintain
those solutions what is increasingly necessary is the ability to produce generically optimisable systems as described in the optimisation requirements
analysis section there exist different ways for human experts to embed their insight into the operation of system the investigator
engaging in an interaction can directly configure the system based on their own experience and knowledge of the infrastructure this
is the bespoke optimisation already alluded to the creator of service or process can embed their own understanding in how
the infrastructure operates this is key to producing high quality software middleware or infrastructure but it is not always applicable
in broader contexts experts encode their expertise as knowledge stored within the system which can then be accessed and applied
by autonomous systems embedded within the infrastructure this is the approach that is being adopted in envriplus in its formal
modelling semantic linking interoperable architecture design and provenance support to embed knowledge into the system it is necessary to do
so at multiple levels and it is necessary to link those different levels from the abstract requirements of researchers to
the fundamental characteristics of the infrastructure this has been the focus of the technology review for optimisation in this instance
optimisation is conducted according to certain metrics measured at various levels from different perspectives from the high level user perspective
these metrics concern quality of service qos most experimental or analytical tasks especially when distributed are subject to degraded performance
when limited by the underlying infrastructure especially when that infrastructure is shared with other applications thus most qos research is
focused on telephony and the internet the international telecommunication union defined standard for telephony qos in that was revised in
iso the itu later defined standard for information technology qos in iso regardless of context qos requirements are generally the
same the application requires certain levels of performance in terms of speed stability smoothness response etc advances in distributed computing
drive research into service based infrastructures that provide assets on demand reacting to changes in the system in real time
menychtas thus the notion of qos wherein an application requires certain level of performance speed stability smoothness etc from components
has been subjected to greater scrutiny of late as the demand to move more and more quality critical applications onto
the internet raises reliability issues that may not be resolvable by blanket over provisioning of computational and network resources li
et al li proposes taxonomy for cloud performance which can be generalised to grid and other virtual infrastructure contexts constructed
across dimensions of performance features and experiments aceto et al aceto stress the importance of monitoring of virtualised environments if
system provides the ability to prioritise different applications processes users or data flows as opposed to simply making best effort
attempt to do everything then technical factors that influence the ability to fulfil qos requirements include the reliability scalability effectiveness
sustainability etc of the underlying infrastructure and technology stack other factors however include the information models used to describe applications
and infrastructure that then can be used to infer how to manage qos requirements for example kyriazis demonstrates how qos
might be specified and verified when mapping workflows onto grid environments on the platform level the qos of the application
and qoe of users are ensured by dynamically allocating resources with the fluctuations of workload there are only limited resources
and the computing and networking infrastructures also have maximum capacity therefore all the resources have to be shared in virtualised
manner so the challenge is to determine the resource requirements of each application and allocate resources most efficiently the state
of the art of this problem can be classified into resource provisioning resource allocation resource adaptation and resource mapping manvi
workflows provide means for researchers and engineers to configure multi stage computational tasks whether as part of the generic operation
of research infrastructure or as part of specific experiment workflows are typically expressed as directed cyclic graphs key property is
that workflows provide means to manage dataflow there are number of different workflow management systems that could be enlisted by
research infrastructure for framing workflows deelman taverna pegasus and kepler the specification of workflows for complex experiments provides structural information
to the operating environment about how different processes interrelate and thus provides guidance as to how data and processes need
to be staged in order to better support research activities given information about all the different workflows concurrent in system
it is also then possible to regulate the scheduling of resources to best optimise overall system performance conscripting elastic virtualised
infrastructure services permits more ambitious data analysis and processing workflows especially with regard to campaigns where resources are enlisted only
for specific time period resources can be acquired components installed and processes executed with relatively little configuration time provided that
the necessary tools and specifications are in place these resources can then be released upon the completion of the immediate
task however in the research context it is necessary to minimise the oversight and hands on requirement for researchers and
to automate as much as possible this requires specialised software and intelligent support systems such software either does not currently
exist or operates still at too low level to significantly reduce the technical burden imposed on researchers who would rather
concentrate on research than programming finally the adoption and collection of precise provenance information permits deep analysis of historical data
and resource use which can be used to refine decision procedures and so enhance the overall performance of the system
longer term horizon in the longer term the increasing complexity and use of virtualised infrastructure will widen the gulf between
researchers and the hands on engineering necessary to manually configure the acquisition curation processing and publication of datasets models and
methods thus context aware services will be required at all levels of computational infrastructure to manage and control the staging
of data and the provisioning of resources for researchers autonomously and these services will have to be aware of the
state of the entire systems catering not to the whims of individual researchers but taking into account the wider use
of the system by entire communities the establishment of such topics will be wholly dependent on integrative thinking taking heed
not just of developments in individual areas of for example workflow management provenance and cataloguing but also the development of
techniques to promote interoperation between all parts of research infrastructure relationships with requirements and use cases the optimisation topic is
strongly related to the compute storage and networking topic the processing topic and the provenance topic in particular the focus
of optimisation is on more efficient use of underlying infrastructure especially of the kind provided by initiatives such as egi
the target of optimisation is on better data retrieval and processing autonomous optimisation relies on knowledge embedded in the datasets
services and resources involved in data retrieval and processing tasks significant portion of which is generated as part of provenance
services there are number of envriplus use cases for which the optimisation task is potential contributor55 the data subscription service
for the transport and staging of data onto cloud resources implementing prototype cross ri provenance model using workflow management systems
and eudat services requires intelligent data movement and resource management re processing of data by users using their own algorithms
requires smart resource control issues and implications it is possible to automate large portions of research activity however this is
contingent on the existence of good formal descriptions of data and processes and on there being good tool support for
initiating and informing the automated procedures with regard specific experiments and applications the optimisation of resources is dependent on the
requirements of researchers the quality of service offered is based on certain taxonomies used to frame constraints that are then
translated into requirements for the configuration of networks and infrastructure three branches can be distinguished in classical performance taxonomy barbacci
et al concerns list quality of service attributes that may be of concern to researchers factors lists properties of the
environment that may impact concerns methods lists the mechanisms at the disposal of the system that can be used to
monitor concerns it is necessary to identify the concerns of researchers in specific use cases investigated within envriplus and to
analyse the factors dictating performance in current research infrastructures the role of task in envriplus is to provide methods for
monitoring and responding to selected concerns the broader implications of generic optimisation of infrastructure and resources extends to the increasing
prevalence of and reliance upon virtualised infrastructure and networks being able to generate deeper understanding of how different kinds of
task impose different requirements on different underlying infrastructure by being able to reason from the level of user level quality
constraints down to physical resource specifications is invaluable if we wish to be able to handle ever more extensive computational
research this is particularly true if we want to keep the accessibility of research assets as open to the broader
research community as possible rather than within the hands of few well resourced experts in this light we need to
consider infrastructure as utility one that is intelligent and self organising further discussion of the optimisation technologies can be found
in section this takes longer term perspective and considers relations with strategic issues and other technology topics architectural technologies keith
jeffery british geological survey bgs malcolm atkinson university of edinburgh and alex hardisty cardiff university introduction context and scope as
defined in wikipedia105 information technology architecture is the process of development of methodical information technology106 specifications models and guidelines using
variety of information technology notations for example uml107 within coherent information technology architecture framework108 following formal and informal information technology
solution enterprise and infrastructure architecture processes these processes have been developed in the past few decades in response to the
requirement for coherent consistent approach to delivery of information technology capabilities they have been developed by information technology product vendors
and independent consultancies based on real experiences in the information technology marketplace and collaboration amongst industry stakeholders for example the
open group109 best practice information technology architecture encourages the use of open technology standards and global technology interoperability information technology
architecture can also be called high level map or plan of the information assets in an organisation including the physical
design of the building that holds the hardware it is fair to say that architecture framework reference model scheme design
and fabric are all used with various meanings in the literature it is generally agreed that the architecture describes as
design or model the data structures and semantics the software components the compositions and workflows and the interactions between components
and users as functional aspects it also describes non functional aspects usually treated as constraints for security privacy rights costs
performance in the case of envriplus the different ris are in very different stages of maturity some plan to offer
and some indeed already offer user portal to access datasets and in few cases processing capabilities some provide apis to
processing and urls or other addressing mechanisms to datasets it is assumed that envriplus will offer reference architecture and standard
component software toolkit for constructing an access mechanism probably through each one portal to each other ri given the use
of standard software components and standard architecture this should allow peer to peer interoperable access among envriplus ris and superset
system outside of envriplus to provide virtual research environment capability providing user driven interoperation across the various ris as envisaged
for the eu vre4eic project110 which has envriplus and epos as project partners through uva and ingv figure figure the
wider landscape this has implications in terms of what each ri at envriplus level or within envriplus ris need to
provide to allow portal and api access to the ri within envriplus portal and api access to envriplus acting as
portal across its ris access from vre to ris such as envriplus figure figure interface requirements the ris may themselves
have user and api access to the ris within them through portal such as the ics integrated core services being
constructed within the epos ip project figure figure epos ip ics first step would be to agree set of reference
configuration expanding on these concepts and relationships between them to permit relevant interfaces to be properly specified this nicely illustrates
conflicting pressures that an infrastructure architecture needs to resolve the primary goal is to improve researcher productivity as far as
possible for most researchers this requires consistency automation tuning in their virtual research environment and the portals and tools they
use to do their work they may further tune this environment using group and personal preference setting to achieve inter
ri or cross discipline access and work they would each like to remain in this productive context and access data
and tools from outside their ri through the same interfaces and with the same tools however those architecting the infrastructure
for their ri and for the other ris need to consider feasibility and sustainability if direction taken leads to engineering
that is too difficult to build that demands excessive resources or is so complex that it cannot be maintained for
the longer term see section then the architects must steer the infrastructure away from this it is their responsibility to
take the long term view and balance concerns succession of pair wise arrangements between ris for specific data can lead
to such problems initially there are small number of maintained data integration relationships but potentially there are such pairings for
wide range of categories of data and non linear growth in workload is not sustainable the architect therefore has to
recognise such potential problems explain them and persuade ri construction teams not to start down such paths devise compromise strategy
that balances the conflicting issues in way that is acceptable to the clients in envriplus case the ris once the
clients decision makers have agreed explain that solution to the construction teams and monitor the construction sites to ensure compliance
possible compromise solution in this case may be achieved by integrating the ideas of the data intensive federation framework diff
see section with the approach adopted by eu vre4eic project described above the diff provides set of apis that are
consistent and sustained that the portals and tools that each ri uses can interface with it also hosts and enforces
the rules agreed between data providers on the use of their data and growing repertoire of recipes for data translation
that it applies on behalf of its clients building maintaining and supporting those recipes by hand is also likely to
prove infeasible but vre4eic uses description driven logic to generate the recipes hence compromise is feasible following such path however
architects also have to deliver against the clients scheduling expectations the feasible path still involves and so may not be
available in time in which case temporary solution has to be fabricated that leaves an opportunity to fit the better
compromise in later this exposes the critical importance of decision making the above decisions have long term consequences affect many
individuals and organisations and may affect costs substantially and delay or improve time to production such decisions need to be
taken by properly constituted bodies the decisions need investment in investigations and evidence collecting and then judgement drawing on all
aspects of the relevant expertise see section item for further discussion of decision making issues and the end of section
for an example of some critical decisions and how they may be partitioned sources of state of the art technology
information used uml although graphical language standardised by omg object management group in fact causes the architect designer to consider
carefully the architecture of an ict system111 uml has static structure diagrams dynamic behaviour diagrams and interaction diagrams it is
based on object oriented approaches and as such suffers from the close integration of data and processing extended entity relationship
modelling provides formalism for structure diagrams which is process independent odp open distributed processing has reference model rm odp based
on the ansa project of andrew herbert it has the concept of viewpoints enterprise information computational engineering and technology in
support of flexibility and independence the languages used to express the concepts and relations in each viewpoint are abstract they
are not directly translatable to formal system specifications or to software or data structures in physical system which is the
aim of case computer aided software engineering systems something additional is needed unified modelling language for example can be used
to practically represent odp oriented systems designs ump profile plugins based on uml4odp iso iec make this possible in commercially
available ides tool chains such as those from ibm sparx systems113 no magic114 etc model driven approaches below facilitate transformation
of computation independent models odp models being one example to platform independent models and platform specific models again supported by
wide range of tools including those from sparx systems and no magic mdd in recent years the concept of model
driven systems engineering or model driven development has emerged115 the idea is that the system architecture can be described by
model from which the actual system software data storage structures and semantics constraints can be generated semi automatically with its
roots in case computer aided software engineering tools from the 1980s the aim is to translate from conceptual specifications to
physical systems thus improving the efficiency of the systems development process reduced cost and time and the effectiveness improved quality
while retaining the clear linkage to user requirements at conceptual level enterprise validation short term analysis of state of the
art and trends it is fair to say that the current state of systems development is somewhat chaotic fashions come
and go each with group of enthusiastic consultants claiming to have the magic wand to make systems development rapid inexpensive
high quality and matching user requirements however some immediate trends are clear virtualisation the user neither knows nor cares where
and how the information processing is done as long as their requirements functional and non functional are respected in service
level agreements quality of service agreements etc interoperation to satisfy the desire for end users to be able to access
not only resources in their domain of interest but across domains such as the ris of envriplus re use of
standard components of software as building blocks joined together like lego this has implications for standardisation of the structure of
apis and messaging interfaces the definition of data structures and semantics separately from software move away from object orientation in
order to be able to use generic software components the need for systems to be distributed partitioned parallel and mobile
client device independent the need for systems to handle data streams from instruments detectors and for users to be able
to control the parameters of data taking composition of software components linked to datasets as workflows with parallel sequential distributed
centralised control and exception management longer term horizon the current trends that adopt virtualisation more widely are likely to continue
as will the demand for increased access processing and ease of use increasingly through visualisation and mobile devices this implies
the need for an integrated catalogue to provide view over the envriplus ris and within which the envriplus ris can
update to ensure they are appropriately represented the catalogue would then be used for ri to ri interoperation query to
the portal of one ri could be extended to the portal of another ri or an envriplus super portal could
be created utilising the catalogue to form workflows dispatched to appropriate ris for data access and processing toolkit that supports
each ri installing way of federating its response to queries and requests with other ris may be the best way
forward see section whatever solution path is adopted it will require maintenance see section relationships with requirements and use cases
the existing use cases and derived requirements all point to the need for integration mechanisms to overcome data heterogeneity both
syntactic and semantic improved re use of common software components at any one ri developed by another ri re use
of workflows perhaps provided as services at each ri and improved best practice in curation and provenance recording issues and
implications the envriplus architecture for interoperability has to accommodate provide superset view over the heterogeneity of the components ris in
the aspects of data software components users resources computers equipment the heterogeneity may be encapsulated at each ri within services
ideally common in functionality and non functional aspects across all ris but implemented specifically at each ri however much research
use is likely to be working with other services so the canonical common catalogue will be required with appropriate software
to provide access to the assets recorded there in order to construct workflows to meet end user requirements simple reference
configuration embodying the concepts and relations expressed in figure and figure and explained in the text can assist to reach
common standing of the points at which standard interfaces apis need to be specified further discussion of the architecture technologies
can be found in section this takes longer term perspective and considers relations with strategic issues and other technology topics
technologies for semantic linking paul martin universiteit van amsterdam uva introduction context and scope the role of the semantic linking
model is to provide framework for translating between the different standards used for data and process specification in the environmental
sciences in the context of the envri reference model this model should provide formal basis on which to improve the
interoperability of ri services and products by focusing on the vocabularies used by the envri ris feeding into the design
of the abstract architecture for interoperable ris in general the model also serves to provide the machine readable formalisation of
the envri reference model or at least its concept model ultimately based on the relevant task of the envriplus project
we will need to capture the conceptual vocabulary of the envri reference model and the correspondences between different concepts described
by different viewpoints define framework by which existing standards taxonomies and ontologies can be mapped to the reference model and
via the reference model to each other provide tool support for defining new mappings between standards and for searching the
semantic space defined by the resulting interlinking thus the purpose of the technology review in envriplus from the linking model
perspective is to determine what technologies are available for ontology specification and formal verification and what technologies exist that could
help us to develop new or adapt existing tools short term analysis of state of the art and trends combining
all environmental domains into one single ri is neither feasible in development nor manageable in operation during the past several
years interoperability between infrastructures has been extensively studied with different interoperability solutions proposed for different levels of interoperation between computing
infrastructures charalabidis ngan between middleware blair and between computational workflows zhao these solutions iteratively build adapters or connectors between two
infrastructures and then derive new service standards via focusing community efforts such iteration promotes the evolution of services in infrastructures
but cannot fully realise infrastructure interoperability while these solutions only focus on specific layers of the global problem without considering
the overall science context riedel meanwhile white et al white argued the importance of an ontological reference model in the
development of interoperable services in infrastructure the linking framework for envriplus martin is being founded on semantic web technologies berners
lee though the core principles are technology agnostic key among these technologies is the resource description framework rdf that has
come to be used as generic means to describe information implanted in web resources building upon rdf the web ontology
language owl is knowledge representation language used to describe ontologies and is significant factor in many semantic infrastructure modelling projects
zhao baldine within envriplus the core of the linking framework would be the oil ontologies which are described in owl
owl is well used in the semantic description domain but limitations of owl include the inability to describe integrity constraints
or perform closed world querying motik which might otherwise be useful in for example certain well prescribed areas of the
envri reference model there are also various problems with dealing with diverse schemas incomplete metadata and the limitations of query
interfaces gölitz the notion of mapping out the topology of standards in environmental science research practice and infrastructure reflects very
much the linked open data approach the linked data approach offers certain advantages such as ensuring openness shareability and reusability
ferris there is however lack of good tool support for linked data solutions enoksson which is one of the areas
that task is intended to address semantic linking is often investigated in the context of ontology matching mapping or alignment
