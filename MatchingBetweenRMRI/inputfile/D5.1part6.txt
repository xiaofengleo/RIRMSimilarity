crucial labour and error saving step processing will then need to execute the data intensive workflows and call on data
identification services the basic consistency for data identification and citation should be achievable within the envriplus project lifetime see sections
and but as in so many scientific contexts this leads to further challenges in this case dealing with the more
complex composite and time varying data generated by the work of ris and their research communities see section finding ways
of succinctly efficiently and precisely identifying the growing volumes and many subtleties of the data used by and produced by
future data driven research will always be challenge as one aspect is covered increases in volumes increases in rates increases
in diversity and researcher ingenuity will pose new ones or break existing solutions it is vital to be on the
ladder addressing such issues as that is key to international research leadership and to addressing societal challenges there is clearly
good reason to believe from the understanding and insights shared in section that data identification and citation will be progressing
well up that ladder during envriplus there are further considerations that may be addressed in the future these are enumerated
here in no particular order many of them apply to subsequent technology review topics as well identification and cataloguing the
relationship between data identification and cataloguing is very close the identity record could also be catalogue entry for the referenced
data the metadata required for the identity purposes could be subset of the total catalogue metadata an operation possibly standard
query pattern could yield the information required for the purposes of data identification and citation it seems unlikely that independent
development and support of identification and cataloguing will make long term sense however there may be distinguished subset of catalogues
that register data identity and the identifiers these use would be used in other catalogues provenance processing and optimisation the
principles and procedures for minting adequate references would still need to be independently designed and agreed but their implementation would
employ appropriate catalogue functionality at which point non functional issues such as performance and availability would come to the fore
harmonised solutions would simplify interworking but they confront established cultures and investments roles for data identifiers it is obvious that
cataloguing and provenance will need reliable data identifiers to refer to data from their records it is desirable that almost
all data processing refers to its inputs in terms of the data identifiers this permits implementations to store copies of
data replicated for availability and preservation and then to choose the one with lowest contribution to the costs identified by
optimisation as the storage technologies resource provisions and data intensive middleware evolve if the scientific methods are couched in terms
of data identifiers rather than naming systems based on particular storage schemes the mappings invoked during method enactment can adapt
to those changes hard wired naming means everyone who is involved in formulating the method has to be involved in
adapting to the changed digital context clearly an unsustainable policy indeed the role for data identities in optimisation is much
greater if data are identified by trustworthy mechanism the optimiser can recognise when the same data are requested on two
occasions in the same workflow by the same user running different process or by different users and save work accessing
transferring and transforming it again raising the level of discourse the majority of discussions in requirements gathering and in technology
review were couched in terms of practical and concrete implementation terms such delivery mechanisms are critical178 but developing precise abstract
models then clarifying them through discussion and revision is of greater and long term benefit179 the reference model see section
provides vocabulary and context where such discussion takes place analysis and decisions couched in these higher level terms are much
less subject to the uncertainties of digital technology evolution those decisions tend to have rationale that is not based on
the demands and issues of the current projects or current equipment and its software rather it is based on the
scientific and community goals these need to be shaped and brought into as much harmonisation as possible that is easier
at more abstract level once the goals are agreed the mapping to implementations can develop and yield the best approximation
to those goals given current circumstances for example should the minting of data identity be an atomic process that is
one that happens all at once leading to complete and final record the requestor would provide the data and all
of the information required for metadata in one go the identity system then allocates pid and builds all of the
associated metadata and makes permanent record of the minted association or should non atomic incremental sequence of operations be supported
for example either the workflow requests data pid to allocate or sends the data and gets pid later the workflow
can supply the data and required metadata later the workflow can say that it wants the pid promoted to preserved
status where it has been quality assured or say that that binding between pid data and metadata should be discarded
transient identities such incremental approaches might allow internal identities for workflow intermediates and potential result sets to be allocated quickly
clearly fixity is not important at this time and costs of supporting it and other metadata would slow workflow many
workflows fail or are under development so their information should not be captured and curated but their intermediate data may
be highly relevant for diagnosis and for testing sub tasks during development what kinds of operation are allowed on reference
can workflow or user retrieve the data associated with it can they retrieve aspects of the associated metadata can they
formulate queries on the metadata that retrieve set of identities can that set be used to perform bundle of the
above operations and so on accommodating diversity although the campaign for harmonisation is vital it will never completely achieve conformity
for two reasons there are many external forces such as collaborative and global agreements that lock ris and their communities
into different standards and working practices and ii researchers and their support teams are ingenious inventors of new methods crucial
innovation on which advances are built we would not wish to inhibit this the former can be accommodated by offering
functions that map sometimes with loss of precision to common interchange form rdf triples conforming to the semantic web model
berners lee is such capability required for data identities and the operations on them the user driven variations can be
accommodated by having some fields in the data identity registry capable of holding any user defined material json format records
rdf records xml records matrices or plain text the ability to use this information in queries would then be required
spinuso as most underlying database systems are capable of accommodating such flexibility see below this is potentially feasible it is
most likely to be required as the designated community becomes adept at using the existing facilities hence we would not
expect it to have much prominence in the current round of requirements registry platforms it is necessary to build registries
and other catalogues on top of high quality database systems there may be more of catalogue framework above the database
provided by the cataloguing campaign see sections and that delivers functionality that packages the underlying database semantics so that for
example the database delivery platform may be replaced database engineering has huge investment and that helps address sustainability issues raised
in section it also delivers scalability by use of multiple nodes delivers accommodation of multiple data models handles nosql and
sql distributed queries and delivers mechanisms for high availability and mitigation of systems failure that might otherwise cause loss of
data or inconsistent states one approach to sustainability is to use widely supported open source projects such as some of
those under the apache foundation www apache org another is to depend on commercially supported software there are several good
database vendors temporal patterns are all of the registries or catalogues built incrementally for example many ivoa catalogues are rebuilt
with specified periodicity this has two advantages they act as stable referenceable data source for that period typically six months
and ii redesign and improvement of the catalogue building methods can progress during that interval and then be released are
the visible registries of data identities always built incrementally are they ever rebuilt what is the model for rebuilding them
distribution patterns the registry of data identities for particular community is almost certainly logically presented to them as single entity
however is it in fact distributed entity to handle transient and initial interactions locally and fast but to promote persistent
complete records to authority sites replicated for durability and availability is this recursive federation structure see section where sub communities
within an ri pool their data identities and group of ris then pool these identity integrations this may be more
consistent with requirements for autonomy it may also be easier from the viewpoint of incremental adoption however it almost certainly
increases the operational and implementation complexity it may be later adaptation but that may be easier if it is anticipated
jam tomorrow is not enough180 having visions of harmonised support for multiple ris data and communities and well planned path
to deliver that goal is important however it will be futile and wasted effort if the results are not adopted
by the majority of relevant practitioners the researchers the technologists engineers and managers who support them and the users of
the produced derivatives experience has shown that promising exciting advances but not taking the practitioners along with you leaves them
finding alternatives to get today work done once those alternatives have been developed it takes very long time and lot
of effort to re recruit the community consequently all of the important categories of practitioner have to see benefits as
envriplus progresses they have to see jam today180 this means finding immediate benefits for them tools that help with their
common tasks data that they want such as the usage of their data summarised and automation of parts of working
practices and scientific methods as soon as they become available made accessible to them this links back to raising the
level of discourse above as if we identify critical subtasks request data identity issue data identity present data identity to
have action applied to it etc then the discourse contains concepts that can be incrementally developed and can prove useful
almost immediately their usability would be much helped by linking with communities of tool builders the visualisation services in spinuso
myers et al myers identified the value of making the stages of curation incremental so that those stages yield benefits
to practitioners as they work they benefit from having some metadata associated with data in catalogue so they can find
subsets of interest to them and apply operations to all members of such subsets this induces them to introduce metadata
terms such as dates and significant properties because they are relevant for their current research campaign those same metadata will
have been tested and improved by the time they are used for curation assessing the data curation technology review data
curation technology is reviewed in section data curation is always important to allow independent review of scientific methods and of
decision support service output it also offers reliable repository for an open ended set of researchers experts or members of
the public to access the data for any future research there may be restrictions such that they require to show
authority or that they are limited in their uses of the data obtained or in the resources that they may
consume extracting or processing the data in the ris driving envriplus they may collect observations of phenomena that will not
be repeated there is also growing pressure for curation from funding authorities these all combine to make data curation essential
for ris in the longer term curation is more than archiving it oversees the processes of deposit and access to
maintain the quality of the collection and support its appropriate use some ris such as elixir are already involved in
long established agreements for sharing the responsibility for curated life sciences data multiple organisations federating to curate collections distributes the
cost of access and support pools effort for quality oversight and improves the protection against information loss through multiple copies
and multiple funders in such contexts the arrangements may be long standing for pdb berman similarly many ris are engaged
in global commitments for curation for example the data collected by euro argo needs to be curated and made accessible
according to the global programme of ocean observation such long term or collaborative arrangements set the scene for specific data
curation campaigns however in the envriplus community there are many who will gather or produce significant data without previously established
models and practices for these it would be beneficial to identify common practices widely adopted and relevant standards and supporting
software so that they could have better prospects of their curated services interworking and benefit from shared implementation and support
effort section identifies some key standards and the coordinating standards development organisations particularly rda producing the patterns for curation that
may be widely adopted the current state of ris needs to be further understood and common solutions stimulated by programme
of awareness raising and training by bringing together the ris that are at similar stage possibly with experts from the
digital curation centre181 and with potential providers such as eudat b2safe group182 there will be better chance of alliances forming
leading to common solutions some of the considerations enumerated for data identification and citation see section also reapply here unsurprisingly
as curation almost certainly requires all of the steps of identification to have already been taken furthermore the citable properties
of data will probably be used for extending the set of metadata referencing the data for referring to re used
type format interpretation and so on descriptions and for forming related groups of data all of which may be the
subjects of curation identification and curation the relationship between data identification and cataloguing includes the uses of identity in curation
listed above it is also possible that the mechanisms for longevity will draw on the same archival platforms and operational
arrangements particularly as the curated data would lose its value if the identities of the preserved data and of data
referenced in relationships were lost it is possible that when an archival process designates material no longer curated eventually resource
and relevance management may require this procedure the curation workflow will place an rip tombstone indicating the data demise in
the identification system so that subsequent access requests can receive an informative error message there is similar relationship issue with
cataloguing do the curation services use the same catalogue services and protocols as those that support other functions in the
ri do the catalogue services depend on the curation services catalogues roles for curation it is clear that there is
an immediate demand for straightforward data curation to meet science and decision making validation goals and to curate reliable record
of the state of observed and modelled systems as time progresses this may also accommodate reflection of the current state
of understanding of the observed phenomena and the mechanisms behind them this may be achieved by capturing the numerical models
and their representations as simulation suites by capturing the ways in which those systems are run by capturing the ways
observational and simulation data are then used the formalised workflows and the related documentation and publication these may be equally
important in validating approaches and in studying how the view of environmental and solid earth systems progresses see section for
one reason why that may be an interesting focus curating the working practices perhaps as summaries of provenance records might
complete the picture this will allow the study of the changing populations of users of each facility the changes in
hot topics and the response to changes in the modelling and analytics arsenals raising the level of discourse the discussion
of what should be curated what properties it should have of the responsibilities in that curation process and the responsibilities
and functions of the system platforms and organisations needs to operate at the rm level until there is clarity and
precision this should also include the ways in which the curation system may be accessed and used can it also
be treated as an active data repository from which data can be retrieved and used and into which results can
be submitted clarifying such matters before getting into the engineering and technical detail is certainly necessary transient curation there is
case for time limited publication of data to allow wide access and use of data that is expensive to obtain
but that can be recomputed the well known examples are the results of large simulation runs of fluid mechanics mantle
circulation of seismic wave propagation of cosmic and astrophysics events such as the big bang merging black holes and astro
seismicity and so on szalay it need not be simulation it could be very extensive machine learning run over all
available ecosystem data to characterise relationship patterns using the curation system for publishing and accessing such results would align the
access and resource mechanisms with those used for other data in consequence users would find their authorisations resources and working
practices will also work unchanged on these data the temporary nature has three motivations the results are often very high
volume time series of multi dimensional data the models and their mappings to simulation code are progressively improved and the
boundary condition data are also improved the result is that sooner or later re run supersedes the previously curated run
data but the curation of the fact that the previous results with their provenance existed must be curated and retained
so that the provenance of derivatives of the model make sense iris183 is acting as repository for earth models that
each involve between and finite elements but larger models that correspond to time series of states are far less tractable
supporting work in progress see below accommodating diversity the campaign for harmonisation is particularly significant in the context of curation
inevitably it will never achieve complete coverage of facets of the metadata and representations nor achieve universal adoption the curation
system normally has to have capabilities to cope with virtually any data that becomes important in the communities that it
supports this is very difficult to predict so the curation model has to be open ended though possibly optimised for
the frequent and time critical activities curation platforms there are already very large scale and long running curation systems those
that support life sciences reference data pdb berman and those that support sky surveys szalay the life sciences variety tend
to have number of professional curators overseeing quality and operations even though they are highly mechanised this may be mandated
by their age the virtual observatories accommodating sky surveys are invariably built on databases and involve less human labour but
very sophisticated workflows that rebuild the catalogues from the observations periodically organisations that provide national data curation and derived information
services such as bgs and ingv typically build on commercial database platforms but develop sophisticated models and operational regimes on
top of these it is moot point the extent to which these could all be built on common platform today
and amortised across more uses the digital curation system irods184 originally focused on digital documents which being human generated are
limited in size has the aspiration to be such general purpose underpinning technology it has micro services triggered by curation
events submitted data of particular type or from particular source that can implement any programmable rule in principle therefore it
is very flexible and it is supported in europe by eudat as b2safe it may be limited in its capacity
and ability to cope with diversity for database solutions to such limits see this item in the list in section
temporal patterns are the contents of curated collection continuously evolving as each request for items to be curated comes in
or is there regime whereby updates are grouped to mitigate potential inconsistency problems and to provide periods of stability there
will also be periodic refreshes of the underlying machinery and software presumably carrying forward the curated data faithfully distribution patterns
the curation service and its implementation serving an ri or group of cognate ris will normally be presented as single
logical organisation at the very least it will have behind the scenes copies replicating information at multiple sites delivering the
lockss principle185 there is potential for other more significant partitions to keep data close to where it will be processed
or where it was produced in order to save movement costs or ownership concerns one motivation for this is presented
in the next item jam tomorrow is not enough180 delivering jam today to the curation user community is challenge myers
et al myers report some success with this they made the provision of workspaces and storage and automation in its
handling payoff for practitioners as they started using the system for their work in progress this required them to incrementally
provide metadata and quality assurance that would eventually be needed for curation but they benefitted from help with managing the
campaigns in their data driven research and in finding and using relevant subsets of data spinuso et al spinuso have
also shown that such tools in this case running over provenance metadata can visualise and mechanise organisational tasks that become
serious chore as the number of data items and research steps rise such integration of shared workspaces with the curation
machinery would address one of the missed requirements see section but would require distributed curation system so that workspaces could
be close to sources sinks and compute resources that the particular user group is actively using assessing the cataloguing technology
review cataloguing see sections and plays fundamental role in providing efficient indexes to accelerate the access to any items that
the ris and their communities choose to collect collate describe and organise catalogue associates an agreed description of each item
metadata that summarises the item and specifies how it may be found used and interpreted the creators and users of
the catalogue decide what the items should be what the descriptions should contain and enable and what can be left
implicit or open ended the engineers organising the implementation of the catalogue need to decide how the parts of each
entry should be created and maintained with sufficient quality and how the operations on catalogue can best be implemented the
allowable operations have to include access by searching but the query system specifying the search and implementing it is design
choice the other operations often include obtaining reference token as result of query or no matching items result obtaining all
or part of the information associated with an entry based on query or token obtaining all or part of the
information for subset of entries specified by query applying specified from supported set or user defined operation to all the
entries obtained by query adding information about an item adding items using the catalogue as primary data source to analyse
properties of the population of items it contains such catalogues provide crucial resource around which discipline may organise the collection
and use of data indeed they were the initial focus of all discussions in ivoa see section they are similarly
important for the organisation of the storage and use of the data hence they underpin identification and citation curation processing
provenance and optimisation and may be crucial in many aspects of operations and management their importance and central role cannot
be overstated in envriplus and in many of the ris there are already many existing uses of catalogues section both
within individual ris and in some cases spanning group of ris these draw on widely supported technology such as ckan
in many cases and often use core of standards for metadata and its representation that is built on international campaigns
for developing consistency catalogues to hold and manage access to frequently used and critical data have been central to computing
since the days when alan turing shaped the campaign to crack the enigma code at bletchley park three critical properties
are expected today they should be understandable easily used and shaped by the user communities that commission them great deal
of domain led debate about exactly what should be in their catalogue and what operations should be well supported is
necessary investment by every discipline sub discipline observational programme and experimental campaign with serious intellectual investment by those pursuing the
research goals the catalogue will become key resource for communication within the discipline they can become very large or entries
are not uncommon this makes their computational support require careful engineering to achieve availability performance and reliability at an acceptable
cost building on good platforms such as high quality database systems see the two previous sections and is therefore essential
whilst they are no longer shopping list that can be browsed and self managed users still expect to interact with
them the predominant form of interaction is query access often embedded in workflows that then perform operations on all of
the selected items the skysurvey campaign was one of the triggers for the international virtual observatory alliance ivoa formation and
certainly the stimulus that made jim gray propose the fourth paradigm gray with ten years experience it has valuable insights
to offer on the design and use of catalogues raddick and 2014a and budavari as mentioned in the introduction environmental
and earth sciences are more complex than astronomy but the accumulated and published analysis of their workloads and user behaviours
will surely offer some benefits and maybe some implementation strategies that are worth pursuing the envriplus catalogue campaign will deliver
mechanisms for holding the system oriented and software oriented aspects of an ri because the ict experts know that they
need this the extent to which catalogues are built that handle domain oriented items is less certain but they are
critical to the success of the environmental and earth sciences in some cases such as lifewatch186 and elixir the maintenance
and curation of information organised via catalogues is primary role today the underpinning platforms are probably independently chosen engineered maintained
and operated in each context for long established catalogues and for very large communities that have the necessary resources this
is likely to continue but for the many others envriplus working with other infrastructure engineers should develop and deliver common
solutions that are adopted this would not only have economic and sustainability benefits it will also facilitate cross domain collaboration
it is clear from section that many are building on shared solutions such as ckan and drawing on core metadata
standards the extent to which such sharing and common standards choices is pervasive needs further investigation and consideration there are
other considerations that may be addressed in the future these are enumerated here in no particular order many of them
apply to other technology review topics as well cataloguing and other topics as discussed above and in the previous two
sections and cataloguing has key role to play providing the required technology for identification and citation and for curation it
has several valuable roles to play in processing for example finding the data one or many items on which the
processes should be applied accumulating sets of results in derived data catalogues supporting efficient bulk operations on selected subsets of
the catalogue providing users with prompts and lists of what operations services workflows and tools are available that they can
use to perform their processes cataloguing the formalisations of new and revised scientific methods that users may follow enumerating the
structure representation and interpretation of the ways in which data are or may be stored enumerating the co workers and
others who may have similar research interests or have skills they need in order to achieve new process subject of
course to proper ethical and privacy rules listing the catalogues what they contain and how they may be used similarly
catalogues provide mechanisms that underpin the collection search and use of provenance records the optimisation mechanisms can mine information from
catalogues such as the numbers of items of particular kind and accumulate information in catalogues such as data about previous
runs and previous mappings in order to learn from these for future similar runs in short catalogues form critical scaffolding
both for the science and the technology of ris roles for cataloguing catalogues are used consciously by most of the
supported communities they provide conceptual framework for discussing what items are important what properties of items are important how these
should be determined and what are acceptable quality controls they then provide logistic support for assembling and using that information
as these are central roles the community will refine their view of what should be catalogued and how it should
be described they or their global standards may also choose how the descriptive information is represented similarly as described in
the item above the technological platform will draw heavily on catalogues this will need to be shaped by efficiency and
engineering concerns lossless metadata compression arias as well as standards and consistency concerns advances will include the progressive addition of
information to support automation thereby reducing the time consuming and error prone tasks that researchers and their support teams have
to undertake raising the level of discourse the discussions about what should be catalogued how those items should be described
the information content of metadata and the operations to be supported should be defined precisely without recourse to properties of
the underlying storage and software platforms this will deliver designs that stand the test of time and that can deliver
stability to practitioner communities re engineering onto different platforms as the trade offs change will then be feasible transient catalogues
in most cases the catalogues will be updated incrementally but there may be reference catalogues that are rebuilt in the
manner of the sky surveys in ivoa regimes see section there may be cases where scientists require the construction of
catalogue during long running set of workflows or research campaign they then summarise or analyse its contents and discard it
the potential for and value of such transient catalogues may be investigated providing scientists with tools as convenient as those
which they have today on their personal machines to allow them to organise personal our group catalogues has been shown
to be very helpful in medical imaging schuler we suspect similar tools for environmental scientists would prove very beneficial for
their research and its applications accommodating diversity there are two major subtopics here diversity within catalogue and diversity between catalogues
some of the metadata within each catalogue will need to have consistent and well defined pattern so that workflows and
catalogue management tasks can be fully automated and so that the fundamental modes of query and lookup work correctly section
identified four key groups of metadata standards that are already critical for the envriplus catalogues however the metadata associated with
each item may also accommodate user provided additions relevant to work they are undertaking or testing the value of some
new item attribute this may be motivated by support for new subdomain or observational system this permits the innovation on
which progress in science and its applications depends as already explained in the previous sections if an evaluated user added
attribute proves valuable it can be promoted to standard field in later release of catalogue the range of types and
representations of items for which catalogues may be produced is almost unbounded it depends only on the ingenuity of researchers
to recognise things to collect list and process initially catalogue may only be used by those focused on the items
it holds but sooner or later it will be used with other catalogues holding different items described in different terms
this combined use will occur because the researchers posing such multi catalogue investigation have an understanding about how the two
catalogues are related if the relationship is based on some recognised properties geo location and time then it is reasonable
to expect that built in transformations in the composite query system will select an appropriate set of candidate pairs that
may be tested for the relationship if the relationship depends on previously unrecognised features blah in one catalogue and pling
in the other then ideally it should be possible for the user to supply relationship test related blah pling and
embed that in the composite query so that the query machinery can more optimally return the subset several popular relationship
tests will emerge and provoke catalogue redesign supporting such complex data access patterns via catalogues is one way in which
they can help with boundary crossing between research communities or investigation viewpoints the capability would need to be incrementally introduced
cataloguing platforms it is evident as described in this list item for identification and citation and for curation arguments and
references are presented in sections and that the catalogue technology should be supported by well engineered and supported database system
that delivers scalability accessibility recovery from failure and supports multi facetted range queries to select subsets and handle measurement error
ideally it should support variety of models beyond relational nosql such as mongodb187 or scientific database see section so that
the freedom for users to explore extensions and the encodings communities use can at least also use xml and rdf
scaling on nodes in one cluster is essential but it is probably desirable for reliability to scale across clusters that
are geographically distributed this also enables queries and data accesses to be handled with less data transport costs and delays
another expected attribute of the platform is that it handles time stamping of changes and can therefore run an old
query at an old time and retrieve the previous result helpful for curation diagnostic investigations and partial re runs temporal
patterns there is choice between building catalogues incrementally which will probably happen in most cases and in building them periodically
the latter strategy is particularly useful when the catalogue denotes all of the features or elements found in set of
scanned resources the periodicity can be based on the volatility of the sources the costs of scanning them and the
domain requirements for up to date data compromise is possible once composite queries are introduced the main body can be
refreshed periodically to integrate new material exploit improved information extraction algorithms and respond to request for changes in the catalogue
design smaller and easily handled catalogue of recent observations can be maintained incrementally and reset to empty at the next
periodic build composite query over the two catalogues then yields up to date information distribution patterns modern environmental and earth
systems research is concerned with the whole globe and its interior individual research programmes and campaigns can combine information about
almost any aspect of this complex system consequently the domain scientists expect to be able to draw on data catalogued
in many places and administered under different regimes see section it will be very helpful when catalogue query systems have
adopted sufficient standards and canonical representations that single query can be automatically mapped to distributed query set to the relevant
set of catalogues at each destination catalogue the canonical form of parameter names and value ranges are transformed into the
local coordinate system and representations the results from the multiple catalogues are then streamed back and assembled into result of
the form the users or software calling the query api expects almost certainly such developments will draw on more general
purpose distributed query standards and protocols the query system will typically be called from science gateway or from workflows acting
on behalf of users or data administrators it is moot point how much of the data integration framework goes into
the catalogue and query system and how much of it is encoded as data handling recipes couched in workflow language
that can be re used in multiple scientific methods and management workflows jam tomorrow is not enough this is not
so much of an issue for cataloguing as catalogues are already widely used and proving their worth catalogue users can
afford jam two aspects of this issue may be considered by developing common catalogue platform on reliable and affordable database
platforms wide range of catalogues needed for other data management parts and wide range of ris catalogues could be supported
by common software this will only happen if initial systems are available early enough that the other potential users don
feel they have to build their own if they do many opportunities for amortising costs sharing understanding and making data
accessible via consistent apis will be lost another opportunity to help occurs when small groups or individual users want their
own catalogues for research programme project or an experiment offering an easily configured roll your own catalogue service and an
easily downloadable catalogue platform alternative routes via which they get help and the associated training and support would deliver what
is required packaging this as convenient service for individual researchers or group to easily organise the data for their research
has proved very popular for medical research data schuler the key to cataloguing and hence just about every other ict
aspect such as provenance curation processing identification and citation is rich metadata with canonical core and user defined extensions the
metadata should come with matching and mapping specifications to other metadata standards and set of convertors to permit the sort
of homogeneous query over heterogeneous sources indicated above within the vre4eic project such inter conversion work is on going between
oil from envriplus and cerif from epos there are already existing converters to from cerif with dc dcat egns iso19115
inspire and others assessing the processing technology review processing transforming analysing and generating data is pervasive activity throughout the data
lifecycle that is often required at many stages and many iterations of scientific methods and their applications it is already
deeply embedded in the cultures and working practices of ris where it exhibits great diversity from time critical and low
powered quality monitoring and pattern detection close to data sources through massive analyses to infer time dependent behaviour over large
regions with acceptable accuracy or simulation runs generating synthetic versions of phenomenon observable properties to preparation of visualisations of significant
results consequently the activities referenced by processing are extensive complex and often crucial parts of innovation and new achievements the
technologies concerned see section are themselves diverse complex and critical to the missions of the ris and their researchers the
multi layer set of resources from computational hardware and storage system platforms through layers of software platforms that become progressively
more specialised to the means by which practitioners create initiate steer and manage computations in most cases the lowest layers
are generic and standard equipment and software systems can be used such standard systems are greatly influenced by the commercial
pressures from entertainment media and business that dominate the ict industry there are few cases such as low power sustained
operation hpc see section for running large simulations for non localised interactions and cross correlations derivations based on all meets
all data comparisons188 where specialised hardware and provision is warranted in most cases common shared provision using cloud or local
cluster to amortise operations and management costs is the appropriate platform above these widespread and common layers the layers of
software systems incrementally shape the facility to match particular working practises and particular requirements these include the programming languages and
extensive widely used frameworks and method libraries that meet general or data intensive requirements these are often augmented by specialised
libraries of functions required by each community or by subgroups within those communities continuously running services for providing selective and
transforming access to data and to perform frequently required packaged functions also contribute processing power analytic tools such as matlab
and scripting languages and workflows are used for composing these functions and services to formalise and package repeatedly required processing
combinations virtually all scientific methods fall into this category as repeated runs are required during development and validation and then
repeated use is required to process each batch of data data acquired during an observation period or data acquired at
each site or data acquired for each region such formalisation ultimately removes chores and opportunities for error it enables experts
from different sub disciplines to refine parts of the method for which they have expertise and it provides framework for
optimisation these formalisations can soon become complex sometimes involving millions of sub stages hence they become difficult to work on
even for experts tooling and diagnostic aids often drawing on provenance records are great help but tooling also needs to
support the initial experiments the first test of an idea about how to process some data consequently tools or interfaces
that enable the users to try out ideas using their own resources with minimum distracting technicalities are of paramount importance
such development systems should keep careful provenance records to attribute credit as many methods build on earlier methods and as
the provenance system see needs to be able to identify exactly which method was used fluent movement of the method
formalisations between development and production contexts will reduce domain scientists dependency on ict experts such as workflow and optimisation specialists
and thereby accelerate innovation and production this will depend on fully automated selection of appropriate platforms and automated optimised mappings
from formalised methods to those platforms whereas the technologies for basic support of encoded methods in number of scripting programming
and workflow languages is robust and ready for very demanding production use workflows supported the recent discovery of gravity waves
abbott the technologies to make the method development uncluttered by technical detail and to automate mapping exist for only few
notations and few target platforms there is strong mutual exclusivity between two modes of organising processing in one the user
interacts on their hand held device or via portal to directly submit control and monitor processing on their own resource
or on platform to which they have gained access this may be through problem solving tool through interactive programming as
in the ipython example given in section or through portal providing some particular forms of analysis on some particular forms
of data these will often behind the scenes draw on the same repertoire for defining methods as we described above
this mode is appropriate for learning about systems for testing and developing ideas and when only modest repetition is required
in the second the user an event detector or scheduled time initiates the request for processing which is then submitted
to queuing and resource allocation system and it then runs on the target platform the time between initiation and response
can vary from under second to days or even weeks for very demanding jobs helping users monitor and organise such
processing particularly when they have many related requests in research or derivative data generation campaign is an essential element in
addressing the scale of modern data this links closely with the provenance system see section driving the tools from provenance
records and delivering to the provenance all the information needed for its records this also meets another issue in handling
massive data volumes after partial failure it automatically enables parts of work completed to be retained and after clean up
restart to complete complex method such issues will become important in ris as they scale up and as their methods
become more demanding there are further considerations that may need investigation they are fewer in some ways as processing is
perhaps the best supported and most understood part of infrastructure however it is good in parts for example the frameworks
and tools for building new data intensive methods still demand deep understanding of technological issues that should be automated for
reliability and to reduce chores some potential topics for further consideration are enumerated here in no particular order many of
them apply to other technology review topics as well processing and other topics almost every other technology topic depends on
processing few examples follow identity minting190 section will need processing during minting to verify quality of metadata communicate with reference
sites and compute fixity data curation section similarly processes metadata to verify consistency it also runs processes to make copies
for preservation and to transform between media and contexts for longevity cataloguing section requires processing to populate catalogues from other
data to create automated metadata to perform matches during queries to assemble responses translating to target representations if necessary and
to preserve contents by mapping them to new forms as new versions and representations are adopted optimisation section requires processing
to mine performance parameters from previous runs to detect seen before requests to analyse new requests and generate plans to
compute the cost of candidate plans and to map the optimal plan to target platforms provenance section needs processing to
transform incoming information from many platforms and subsystems into its standard form and to generate responses to requests for subsets
of its records potentially in summary form it may also transform those results into other standard representations roles for processing
it is called in regular patterns every seconds or every day to support all of the routine operations of data
management such as qa data shipment data compression etc often as part of an automated and autonomic system it is
called to execute all of the steps in established scientific methods these may be submitted directly or as consequence of
users interacting with portal or analytic tools it is called on an ad hoc basis as user tests an idea
performs some quick transformation generates presentational form or does management task on their workspace data raising the level of discourse
the ways in which users and communities discuss their processing are often mature and deeply embedded in their culture through
training and practice for example one community may invariably use python another matlab another and another graphical representations of workflows
most users will choose data and supply parameters to packaged methods to do their routine work when they need to
craft specific process most users will mainly compose and parameterise pre existing functions and services specialists and innovators need access
to all of the details however the framework that supports them and delivers processing can be considered without recourse to
these details in more abstract terms such as create encoded method reuse encoded method parameterise encoded method run parameterised encoded
method etc articulating provisions at this level may make commonalities recognisable and highlight particular cases that need special treatment transient
processing opportunities and facilities that aid creative thinking are very valuable in science often at the start of an idea
or invention of new process user will want to experiment to test and explore their idea processing must support such
computational experiments well it should avoid distractions and impediments from heavy weight machinery and administrative procedures fluent path from experiment
to operational practice is desirable for innovation to flourish that path should not depend on assembling team of gurus for
sustainable science expert guided optimisation as necessary step on the path to production should become rare as automated mappings and
data intensive engineering improve accommodating diversity as described above there are great many programming languages and other method encoding technologies
in use these all need to be supported to avoid disruptive impacts this is usually well done by systems for
the actual execution of the methods however today there are significant issues to be overcome modern data driven science depends
on re using and refining methods already developed in many cases boundary crossing needs input into these methods from different
cultures using different notations where those can be viewed as programs today workflow systems compose them where they are scripts
in or matlab or workflows in different languages very little help of production quality is available scibus is good example
kacsuk developing such methods often requires good ides to organise all of the software engineering and then it requires deployment
systems that establish and run the new software in the target contexts neither of these stages crucial to the development
of ris infrastructure and essential for scientific innovation are well supported across boundaries processing platforms the lowest levels of the
processing platform are dealt with in section these provide the computing cycles and storage the networks the operating systems and
in most cases the containment for processes they also handle resource scheduling computer computer and process process interactions and provide
security for most data intensive or numerically intensive processing there are other layers that he ri infrastructure should exploit for
example there are data intensive frameworks good for high and sustained throughput such as apache storm and apache spark191 similarly
mpi is standard underpinning exploited by many but not all large scale simulations choosing shared repertoire of these middleware frameworks
that facilitate demanding computation that handle reliability and availability and that are maintained and often run by mature global communities
of engineers is crucial decision for the ri infrastructure builders whether suitable database technology that combines computation with data access
in the query context and with data delivery in the projection of results context should be included in this platform
is an open question sooner or later significant numbers of ris will need such platform see section temporal patterns what
proportion of the workload can be decoupled from human activity and run when systems are lightly loaded what proportion is
time critical for hazardous event recognition earle what proportion of the processing needs human interaction for steering and development and
what proportion can be run in batch mode some of which may still yield rapid responses distribution patterns how much
of the envisaged processing needs to be done at specific sites for ownership or management reasons how much of it
will benefit from being co located with the data it operates on to avoid bulk data movement which consumes both
time and energy how much of the work can be run almost anywhere and how much needs specialised platforms how
many scientific methods will there be that need to combine processing on two or more different kinds of specialist platforms
to what extent will the processing benefit from parallelisation across nodes within cluster and between clusters to what extent can
the distribution of processing be automated and optimised jam tomorrow is not enough virtually all practitioners in this context are
adept at using computers they all already enjoy reasonable access to platforms where they can run small scale programs on
local data consequently this is not as much of pressing issue as it is for other technologies however the moment
users move out of this comfort zone preparing and encoding complex data driven methods running against remotely held or large
scale data requiring substantial computational resources they get very little help we should be building intellectual ramps and tools that
support them for each of the directions in which researcher can develop their processing needs192 in addition providing effective interfaces
for external tools on hand held devices and range of interactive platforms would be helpful for example sencha193 used in
the verce project atkinson or the django194 framework to provide an api to the data with angularjs195 and semantic ui196
to build the interactive framework although ember197 may be an improvement on angularjs taylor assessing the provenance technology review for
modern data driven science there is pressing need to capture and exploit good provenance data as explained eloquently in section
provenance the records about how data was collected derived or generated is crucial for validating and improving scientific methods it
enables convenient and accurate replay or re investigation it provides the necessary underpinning when results are presented for judging the
extent to which they should influence decisions such as mitigating natural hazard198 to publishing paper it provides foundation for many
activities such as attributing credit to individuals and organisations providing input to diagnostic investigations providing records to assist with management
and optimisation and preparing for curation the ris will need to perform these functions and consequently the infrastructures they depend
on will need to support provenance collection and use well the interaction with identification and citation and with cataloguing is
made explicit today it is challenging to plan and deliver an implementation which is sustainable sufficiently shared or dependant on
common widely supported platform see section and which copes with the multiplicity of services and platforms that typically do not
adopt common standard for provenance when they support provenance at all the section provides very clear list of the groups
standards bodies and research campaigns that are addressing provenance as it has been recognised by international bodies such as w3c
as crucial as society depends more and more on information derived from data via long paths of inference interpretation filtering
transformation and integration an introduction and evaluation of the current candidates for shared approaches though the analysis was deep and
the coverage broad in section there are still opportunities to consider some issues further these are enumerated here in no
particular order many of them apply to other technology review topics as well provenance and other topics taking the topics
in the order they were presented in section we can illustrate significant interactions in the following way identification and citation
are prerequisites of provenance identifying the artefacts individuals organisations and computational contexts may all be necessary for full provenance records
curation will depend on provenance systems to provide automatically relevant metadata otherwise unrealistic amounts of human effort would be required
cataloguing will almost certainly underpin the provenance system as provenance requires searchable catalogues of its records processing has to support
provenance collection supply from all its services platforms and workflows provenance information in form that can be readily integrated processing
may also use provenance records to resume after partial failures with clean up of partially complete intermediate results and re
use rather than re computation of those that have been completed necessary provision as data scales and activity rates increase
the provenance records can be great help to practitioners as they prepare and refine the encodings of their methods providing
records to help with diagnosis and to easily organise exact re runs after putative repair providing records that can be
analysed to review progress with research campaign and to assess the coverage of required field of evidence providing records that
can be analysed to discover usage trends and to facilitate resource planning or to spot critical targets for optimisation roles
for provenance these have been covered under the previous heading to great extent however traditional usage tends to focus on
retrospective analysis and preparation for curation as spinuso has demonstrated spinuso and as other work reported in section shows it
is possible to use the provenance system actively as work progresses this encourages uptake as it improves the productivity of
the practitioners raising the level of discourse the facilities provided by the provenance system and its requirements on other systems
such as processing cataloguing identification and curation can be articulated at high level mainly from the information viewpoint by using
the vocabulary and structures of the reference model see section this will help clarify the requirements on other systems to
supply information and what are the temporal constraints must system support live provenance record transfers so that user steering can
be delivered what functions that the provenance system can be expected to support what categories of multi faceted query should
the provenance system support or what operations are provided to switch provenance recording on and off during processing what mechanisms
if any allow users to add their metadata to provenance record what kinds of provenance summaries should the provenance system
support what are the privacy versus sharing rules for provenance data and how are they specified transient provenance records at
least an agreed synopsis of provenance records will be stored with provenance records but what about the provenance records of
intermediate data and of products generated during testing refinement and innovation before production runs not having provenance on at such
times is unwise as it can support many forms of investigation helpful to the development team it may be part
of the evidence that system is safe and of sufficient quality to deploy in production however indefinite storage and storage
validating an encoding of method that is now superseded by another validated method may be wasteful how are provenance record
lifetimes determined what mechanisms should there be for users to affect the lifetimes of stored provenance records relating to them
accommodating diversity as section identifies the current variations in standards for representing and working with provenance make it very difficult
to establish single provenance system the diversity of behaviours of subsystems platforms and processing systems such as workflows and simulation
models makes the provision of uniform provenance system difficult the users and ris may also have different policies and practices
that the common system needs to accommodate to what extent can this be done with automated translation intermediaries provenance platforms
it is difficult to identify single provenance system that covers the whole data lifecycle and supports all of the provenance
uses section reviews five candidate systems that are developing but none is yet ready for production use it may be
the case that standard representations and established workflows will need to be gathered around supported subsystem such as cataloguing that
meets part of the task spinuso has used mongodb to store prov compliant records for range of provenance gathering and
usage modes spinuso so building on database platform is another approach for sustainability section the approach eventually selected should either
be widely adopted supported system none of which exist at present or one supported by suitable alliance as there many
others needing such systems it is possible that through rda or similar bodies or through multi community coordination alliances for
environmental science consortium can emerge not only to agree the metadata and ontologies that are needed in environmental and earth
sciences but also collaborate on tailoring good platform to support those provenance data temporal patterns the consideration of whether provenance
records are available during the run has been raised above that can support monitoring and diagnostic tools and be used
to trigger data management operations such as collecting results some processing platforms prohibit such communication if provenance data is fine
grained its transport and storage may be significant overhead in which case summary records need to be derived sufficient for
later purposes before discard of the details user and community policies over the trade off between complete details and overheads
will need to be taken into account distribution patterns in most contexts the data intensive federation section will be using
distributed data resources which will hold their own provenance records the provenance records for data derivatives will probably held at
the site where the derivation was performed or at the site which initiated the derivation that site will need to
reference the provenance of the input data used as input from that derivation record tree of references to remote records
may be the result this may be even more complex if the derivation was performed by workflow distributed across platforms
as network traffic is expensive it may be best to postpone provenance record shipment to assemble the tree or chain
of provenance records until they are needed this may be achieved by distributed query or provenance workflow that runs in
the background marshalling provenance data into collated sets the users and resource providers will resist adoption if provenance gathering imposes
significant overhead this may be most sensitive in distributed and real time contexts yet it is precisely these contexts where
proper attribution of responsibility and clarification of the evidential chain may become important without it improving the quality of services
and the information they offer decision makers may depend on guesswork jam tomorrow is not enough section already draws attention
to the work presented at our kick off it4ris workshop by myers et al encouraging the adoption of good metadata
practices during the research processes myers this clearly applies to the provenance element of that work and early provision for
envriplus ris should attempt to emulate their success some of the tools developed to use provenance records have excellent visible
summaries of the information those in spinuso if possible early work in envriplus could develop exemplars that are easily understood
and visually powerful to use in outreach and training this will help lower barriers to uptake and gain engagement fear
of unacceptable overheads or demands on researchers time must be addressed by showing the scale of the actual overheads and
providing controls that allow researcher communities to switch them off when necessary we hope of course they will not do
that but they do need this safety net assessing the optimisation technology review optimisation reviewed in section is important for
every aspect of the infrastructures and working practices envriplus sets out to support making best use of people time and
minimising energy consumption are probably the most important goals certainly long term the shorter term goals need sharpening with explicit
cost functions for example clarifying the productivity of which roles need highest priority at given period the results that are
time critical and the cases where throughput is the highest priority in each case within the constraints of an energy
or funding budget these cost functions cannot be narrowly defined we have seen that identification and citation curation cataloguing and
provenance are deeply interconnected and they all depend on processing consequently improvements in identification and citation that simply pass the
costs on to cataloguing and curation would not have the intended value there is therefore an argument for making optimisation
cross cutting concern two aspects of optimisation in large and sustained systems need well managed structure temporal partitions of decisions
the awareness that an optimisation is needed may wait until community or method is encountering problems while this trigger may
be useful indicator of where to focus and what the priorities should in the short term it will be difficult
and expensive to address the issue if the required mechanisms are not already embedded in the technologies in use consequently
at the early stages of building or commissioning infrastructures or productising new scientific methods the capability to subsequently optimise must
be evaluated simply throwing kit at the problem strategy often suggested by resource cloud or technology computational cluster vendors may
prove unaffordable in the longer term in the early years of envriplus the optimisation wp should be drawing attention to
the selection of technologies which support optimisation well many of which are introduced in section it would be unwise to
postpone this internal awareness raising though most of the ris and all of the topic leaders are aware of it
organisational partitioning as explained in section sustaining long term support for research for working practices and for he data lifecycle
is best achieved by delegating to others substantial parts of the technology particularly software design and maintenance alliances be forged
to share the residual responsibility for minimised list of software elements and here their supporting hardware that is particular to
and essential for an ri or group of ris research by carefully choosing suitable software platforms apache spark and software
systems mongo db three gains are made an appropriate community of experts is driving the design construction and maintenance of
those projects they can gather and harness experts who understand very particular engineering and optimisation issues and hence optimise their
subsystem far better than self build approach could ever hope to achieve they also get pre release access to new
technologies 3d xpoint199 and hence can deliver its very significant advantages more quickly than local teams wide and active community
of demanding users in apache spark case large companies explores the functions and capabilities the result is that latent errors
that have evaded the release quality controls are likely to be encountered first by someone else similarly capability issues such
as rates of data handling are also likely to affect someone else first consequently many potential problems are dealt with
without any impact on your community as result your responsibilities are reduced and the technology supplied already has the required
support for the specific optimisations that you need to do at present most of the optimisation considerations remain to be
addressed in the future these are enumerated here in no particular order giving some more technical details about potential structure
as we go optimisation and other technology topics as explained above optimisation should really be cross cutting issue as all
aspects of an infrastructure and all uses of it can be subject to optimisation for example energy saving is normally
pervasive issue local reduction that increases energy consumption elsewhere would be valueless the exception to this is remote field or
marine deployment where there are no local energy sources roles for optimisation optimisation can deliver improved services to researchers deliver
data derivatives and analyses more promptly or deliver more data analyses with their budget or available resources note that these
two optimisations are usually in conflict and the trade off between them needs to be captured in cost function that
can be calculated can be measured and therefore can be minimised as in many branches of engineering measurement of progress
towards goal is key as the complexity of the hardware and software systems defy modelling but optimisation can also deliver
productivity improvements for any of the roles concerned with setting up deploying running and managing the infrastructure for example it
can deliver significant labour savings to those forming and deploying the virtualised computing contexts that the operational system requires see
item below raising the level of discourse the discussion about what potentially needs optimisation can be addressed in terms of
higher level concepts articulated in terms of the reference model see section similarly an abstract characterisation of potential target cost
functions to be minimised will facilitate discussion of their relative importance aspects such as the relative importance of energy consumption
response time and time to first operational deployment can then be discussed by those steering the ri and communicated clearly
to those building and operating it clutter of technical details would not only inhibit discussion and obscure communication it would
also be fragile as the digital context and scientific methods changed it would need to be revisited transient optimisation generally
speaking optimisation is long term investment with an even longer term payoff however there are occasions when there is need
to optimise cost function that downplays other factors for example during hands on training session or project review demonstration response
time may override balancing issues that normally apply such as energy consumption local resource costs and other community member throughput
similarly with pressing submission deadline the throughput to gain statistically significant evidence may have the highest premium accommodating diversity as
indicated above almost everyone in all of the roles in table may make case that if some aspect of their
working practice is not improved the entire ri infrastructure or community will suffer or they may collapse under the strains
of their current tools services and workarounds as the ri progresses and its users become more expert as data collection
accelerates or gains resolution as data analyses and models grow in complexity as derived data and result production increases as
demand for data access from external data intensive federation partners rises or as curation volumes hit maximum anyone can identify
and describe pressing need the software engineers will be overwhelmed by concurrent requests and unable to fulfil them all it
is vital that the governance system accepts these request for optimisation and makes decisions about how they should be prioritised
they should consult the software engineers to assess the amounts of effort involved and the scope for improvement with the
current platform and digital context after an optimisation target has been set the software engineers need to propagate appropriate shares
of responsibility to those who are supplying and supporting subsystems this is why they have the customer support mentioned in
section optimisation platforms as explained above the underpinning software layers platforms and subsystems will normally have significant capabilities for optimisation
this capability should be significant criterion in their selection but care has to be taken to understand the interactions between
layers for example if apache spark is selected then it does not distribute work over multiple clusters as many workflow
systems do pegasus deelman therefore it is limited to the capacity of the cluster on which it is running when
work is submitted there are further limitations that can limit its capacity on particular platforms for example it grows its
ram occupancies indefinitely if the analytic algorithm requires it that leads to pathological page swapping however combining frameworks for building
data intensive applications with facilities to run them across heterogeneous platforms may offer solution many platforms employ virtualisation to allow
predetermined computational context to be deployed and to deliver protection there are two common forms that which depends on hypervisor
and that which depends on linux containers lxc the former intercepts all machine operations and maps them from the controlled
image space into the external digital context if they are legitimate the lxc intercepts only all of the calls to
the operating system which is the way programs communicate with their external environment the hypervisor mechanism to allow safe cohabitation
underpins cloud systems the lxc mechanism underpins complex large scale operations and was pioneered by google popular version is docker200
in our recent measurements the overhead for data intensive workloads was much lower under lxc than in cloud aws context
filgueira the choice of data storage and management systems file systems parallel file systems hdf5201 and databases see discussion in
sections and on database options including scientific databases and nosql such as mongo db is also critical issue for some
classes of optimisation some of the platforms have pre packaged choices see section temporal patterns many optimisations are sensitively coupled
to details of the underlying deployed platforms and the encoding of scientific methods both of which change frequently as indicated
above there is usually concurrent demands for urgent rescue packages traditionally they were handcrafted by performance engineer as section points
out this is not sustainable if the experts also have to revisit previous optimisations when they have been invalidated by
changes in context or encoded task automation is required that at the very least handles almost all of the changes
and automatically revises the optimised mappings from required task to digital implementation this self same automation should perform much of
the initial optimisation but that requires formalisation of the target cost function sufficient metadata about the digital environment and about
the task obtained from mixture of human input and automated analysis of task encodings and prior run provenance records thus
the framework supporting optimisation and the information it draws on should improve with time as remarked above improvement will only
be maintained if there are sufficiently precise measurements available distribution patterns after many years of experience at nasa mattmann identified
intelligent data movement as one of the four key factors in making data driven science using big data success mattmann
this applies at all scales from the activities within node and cluster to the location and movement of all data
between sites optimal strategies for avoiding double handling putting data where you will need it tactics to avoid going to
backing store between subtasks in an encoded scientific or data handling method unless the intermediate is needed for diagnostic purposes
filgueira tactics to use the right kind of storage when reading or writing to disc stores koltsidas the new storage
