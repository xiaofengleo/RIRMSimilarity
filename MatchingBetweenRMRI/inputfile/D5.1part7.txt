technology 3d xpoint with significantly different properties is rapidly being brought into the storage framework there is deep tradition of
developing variety of hierarchical storage systems from magnetic tape to the solid state disks in some cases the storage is
separate network attached unit coupled to one or more clusters on the same site in other cases the storage is
distributed and associated with every computational node the former may provide good environment for curation but the latter may be
much faster for high throughput data analytics thus the optimal strategy for data movement data distribution replication and preservation is
complex when everything is located at one site modelling and analysing the options here is worthwhile within one platform or
subsystem layer this will be done by the contributing organisation responsible for the provided platform however two issues still need
attention the provided platform will need configuring informing it which data should be persistent and which should be active which
will be written and accessed by bulk serial transfers and which will require random reads and writes the interaction between
the provided layers may be handled by the layer providers on many occasions it will be the responsibility of the
ri team and envriplus the management of distribution and data traffic becomes much more complex when we consider geographically dispersed
sites particularly if this is in the context of data intensive federation see section here the police and rules should
ideally take into account the feasibility of optimising data placement and data transport different sites may have very different provisions
and operational regimes and so be well suited to particular aspects of the workloads different communication routes may have very
different properties here there is an acute need for good descriptions of sites and networks so that as much as
possible of the decisions can be automated the rules and policy should be assessed against models of the anticipated inter
site and inter organisation data traffic measurement is essential to monitor costs and to measure progress the provided infrastructures that
underpin this and bundle provision to many consumers geant see section may appear to be free at the point of
use however any attempt to organise data placement data traffic and inter organisation contracts in order to minimise energy costs
will need to properly account for the energy involved in data movement all the way along the path jam tomorrow
is not enough optimisation might not sound like an opportunity for jam today however if we analyse what are the
pressing activities in the ris at present we may find they split into two groups the established group who may
have immediate needs for optimisation in this case choosing standards for the descriptions of some aspects of their infrastructure and
then providing help with collecting and setting up those descriptions might be first step by choosing critical niche data placement
across sites it may be possible to work with relatively small amount of descriptive data then installing system that helped
place data optimally against some agreed cost function might yield rapid results that are noticeably helpful the setting up group
who may at present be getting focussed on building and deploying the necessary platform layers and subsystems and providing the
computational contexts their community wants by introducing tools for setting up library of images and of sharing them and deploying
them easily we would achieve at least three benefits the library would encourage sharing the scripted image build management would
accelerate refinement and adaption the hard pressed teams building and deploying the proto production systems would have enhanced productivity examples
are given in fox and the pegasus team uses docker scripts to construct and deploy images the images are automatically
built and stored in their docker hub repository202 so every time they upload docker file the image is automatically built
and stored in the docker hub repository these are then deployed ready for use using docker compose files building up
libraries relevant to commonly required deployments would be an advantage haydel et al present examples where docker has accelerated their
use of specialised hardware architectures haydel assessing the architectural approaches review architecture the first of our cross cutting themes is
reviewed in just as with the architecture of buildings architecture for infrastructures underpinning ris concerns balancing the complex and often
competing pressures within feasible budget of software and systems engineering effort in an acceptable time meeting the needs of the
research communities and encouraging their expansion making continuous operation and maintenance of the data lifecycle feasible sustainable and affordable using
tried and tested designs wherever possible fitting into these innovations where necessary for function not for fashion using familiar and
well understood presentations of the expected and standard facilities ensuring that each aspect of engineering is properly analysed and taken
into account using previously successful design patterns and implementation methods where possible exploiting as much pre fabrication as can be
effectively composed into coherent whole again as in the architecture of buildings we do not have the luxury of greenfield
site the new facilities need to fit with maybe extend and partly replace existing investments so that they can be
adopted with enthusiasm by the existing inhabitants complex community see section they also need to fit with the surrounding context
which in most cases requires complex network of agreements and operational interconnections see section and finally the construction needs planning
permission and to comply with building regulations for the former we must include the relevant standards that already apply in
the field significant bundle of which are incorporated in the inspire directive eu parliament and many others apply for building
regulations many are potentially in play for example the rules concerning the use of nationally and regionally funded facilities prace
centres rules the rules governing shared infrastructure platforms those imposed by geant and cloud providers see section and the eu
h2020 backed eosc see section once building or infrastructure has been constructed it needs maintenance to continue to serve its
user community including adapting to their new requirements but in the case of infrastructure this is more difficult as the
digital context is changing rapidly due to uncontrollable commercial and economic forces see section because of these complexities carefully considering
architectural issues is critical astute design of the architecture can much improve matters for the researchers and other data users
as they encounter more comprehensible and coherent system that they find much easier to use for the data providers and
those managing and running the data handling chains in the data lifecycle who find that all aspects of their digital
tools and services fit together meet accepted standards and can be trusted to have sustainability for those setting up the
infrastructure they have well identified pre designed subsystems to import and guidance on how to fit them together with minimum
extra construction and maintenance effort for those providing funds and resources for the infrastructure they know that the maximum overlap
with systems they already resource and between systems that being introduced has been carefully considered the complexities of system architectures
need good media for their discussion recording and analysis for building it used to be drawings and models today it
is predominantly computer aided architectural designs caad with accompanying methods for generating drawings and models for analysing engineering and regulatory
requirements and for feeding into construction planning and management the first steps of an equivalent approach for large scale distributed
and multi organisational computing systems have been built around the open distributed processing odp standard which is used in envriplus
to represent and develop the reference model rm see section this helps system designers and builders use vocabulary and representation
that can be interpreted unequivocally it also helps address the complexity by establishing five viewpoints from which issues can be
examined unfortunately although well developed for human experts it is not yet so well connected with simulation and evaluation tools
or automated coupling to construction planning and execution in summary and effective architecture needs to meet the following principles in
way that can be communicated to all relevant parties acceptability it must be such that deployed version will meet regulatory
ethical and political concerns necessary for approval but also sufficient to satisfy broader public scrutiny for example if policy decisions
draw on results from environmental and earth science ris it must be possible for those who wish to question those
policies to review and understand how the data driven evidence was produced at quite different level those working in the
field need privacy protection otherwise miscreants know they are away from home or where to find them to take them
hostage or steal their equipment usability it must be feasible where necessary for researchers to develop experiments using familiar methods
and workflows subject to governance constraints rules agreed by their community or federations in which their community has engaged such
as maximum periods before which results are made shareable if their methodological innovations prove valuable there should be well supported
path to bring the new methods into the wider production repertoire sustainability as far as possible the software used must
have either known support from vendors or known active open source community where it is appropriate as part of the
architecture to contribute to open source software then it is necessary to budget for contributing to that community and be
sure there is sufficient global community behind the open source that it can be anticipated to continue where it is
appropriate to lead the development of architecture specific software this should be developed with the software sustainability institute model of
sustainable software the underpinning infrastructure storage digital communication and computational resources must also have sustainability that will meet the community
needs for decades flexibility the functionality of the apis offered by the core or initial infrastructure to other software should
allow wide range of future uses that are as yet unpredictable and should as far as practicable allow the architecture
to accommodate much larger data volumes or more sophisticated models than initially encountered diversity the architecture should be capable of
evolving to support the many varieties of data formats and analytic methods currently in use and frequently changing practitioners should
be able to introduce subject to an ri policy new forms of data new structures interrelating data and new tool
sets for performing operations over data scalability although early instances of the architecture will rely primarily on human trust and
associated manual operation of experiment workflow as the volume of data and experimental demand increases it should be capable of
adapting to include automation of workflow where this is consistent with other architectural and governance principles for most ris such
automation is already in place but it may not yet cover sufficient aspects of the data lifecycle and sufficient range
of research campaigns particularly those that are interdisciplinary validity the management and governance of the implemented architecture should ensure that
potential modes of failure and misuse will be progressively identified and enumerated as each is identified and prevented by combinations
of rules and software tests for the subsequent releases of the architecture should be introduced and overseen in clearly defined
governance structure that includes independent oversight the short term requirements well enumerated in need to be addressed while taking these
longer term issues and principles into consideration candidate architectural strategies considered there include the use of the uml formalisations of
the odp reference model to separate concerns the use of central core that supports portals and other uses the use
of model driven development these are not mutually exclusive for example the central core will almost certainly emerge or be
requirement whichever way the system is modelled and however the construction is coordinated with each approach the challenge is to
make best use of existing investments in ris and to consider the many details needed in each context the current
investment in the odp reference model taken further to develop the vocabulary and dialogue for dealing with the implementation details
via the engineering and technology viewpoints is probably the best path see section this will almost certainly develop an onion
like structure with common core delivering the requirements that every ri requires and then layers that tailor it to meet
the more specific requirements ultimately the common core will probably meet most aspects of data intensive federations see section however
initially it will almost certainly be simpler and less specific for sustainability inter disciplinary harmonisation and for amortising costs see
section it will therefore draw heavily on the frameworks and models supported by resource providers see section and eosc thus
further issues that should be considered by the envriplus community include the extent to which the architecture should be shaped
by the provided services of resource providers versus the extent to which it needs to be crafted to meet class
of ri requirements or class of environmental requirements the notation used for formalising and agreeing architectural construction and maintenance contracts
augmented and developed versions of the odp reference model are one candidate critical factors will be the choices made by
international alliances such as the rda the extent to which affordable tools and training become available and the extent to
which federated efforts commit to developing and refining the detail the common functionality which can be amortised across many ris
because they agree that they all want kernel implementation of functionality scalable multi site catalogues adaptation to data intensive aspects
of data driven science the widely deployed platforms are well tuned to the ratios of data handling and computation cycles
that underpin the dominant commercial fields and the sciences when simulation and small scale data analyses was the only game
in town today our encoded scientific methods include substantial portions which are data intensive they perform data input and output
and data transfers to such an extent that data handling becomes severely limiting factor various architectures and subsystems are emerging
that can address this but there is no one size fits all solution consequently how the infrastructure best incorporates these
contemporaneous advances is an open question managing software automatically is an essential for sustainability and for large scale distribution when
it is new it will always need fitting into the digital context the rest of the infrastructure provides either as
it is imported or as it is developed in situ after that introduction it would be ideal if it were
managed entirely automatically deployed to appropriate platforms scheduled parallelised and was fed appropriate input and the uses of its output
were understood by the system today such automation is lacking except in limited cases and contexts as there is no
established and widely applicable model for describing software sufficiently precisely as result there is insufficient payoff for investing the effort
the semantic linking research see section plans to investigate this in the context of the envri reference model see section
assessing the semantic linking review the semantic linking technology section will significantly contribute to issues such cataloguing see section curation
see section provenance see section architecture see section and reference model see section as well as being the only sustainable
path to harmonisation and inter ri integrated coherent views of data and services the reason is that it seeks scalable
strategies for coping with diversity by handling different ways of describing and representing all of the concepts data and software
of interest this has pervasive and substantial impact because as we have seen there are many forms of data and
metadata in almost every ri context and certainly between them where there are standards there are often more than one
standard that could have been applied and scope for variations within standards this is not just matter of representational variation
for the same entities or properties of entities it is deeper variance where the conceptual space is named and partitioned
differently and organised via different structures researchers have long invested effort in accommodating these variations they can always hand craft
transformations for the data from each source they use into form the next stage of their work requires it has
been estimated that such data wrangling takes of researchers time this shows that transformations preserving relevant information are possible but
we have moved into an era where one off solutions are not acceptable the data for data driven science has
to be organised and presented for multiple uses commitment every ri and the envriplus project wholeheartedly endorses to improve productivity
to accelerate discovery to reduce errors and to improve the cost benefit ratios from investing in environmental ris however such
an oft repeated goal of harmonisation is not easily achieved the combinatorial space of forms of data and metadata is
too large for hand crafted solutions to handle indeed it probably grows faster than the capacity of the experts in
data integration can write algorithms to handle differences consequently the semantic linking strategy is to assemble higher level descriptions that
can then be used to automatically generate and revise the transformation algorithms section offers very thorough survey of much current
addressing this topic including that ongoing in contemporary eu projects such as the vre4eic project110 and shows that there is
great potential at least for data and metadata if the approach is structured using the envri reference model we can
therefore illustrate how this will pay off in various parts of envriplus and thence in the infrastructures and working research
environments of the ris the contribution to cataloguing comes through two effects when data and metadata are loaded into the
catalogue if they are in different from the catalogue standard translation shims203 should be available to convert all of the
components into the standard stored form when user or workflow seeks to access information from the catalogue terms in incoming
queries can be transformed into the catalogue notation and results can be transformed to meet the requestor requirements the contribution
from cataloguing will be the organisation of the evolving set of transformations and maybe the higher level descriptions that are
used to generate those transformations for example the transformations themselves would be described in terms of the semantic category of
their input the semantic category of their output and information about how these are represented in standard established by envriplus
for example when query mentioned geological era say the pleistocene and the data were stored as years before present time
range could be generated to find that transformation the cataloguing system requiring it would need to query the catalogue of
transformations for transformers that handle named eras as input and yield ybp ranges as output there may be several of
these to handle various binary encodings or to handle different views on the equivalences if no transformer is found then
request to have one made could be sent the automated transformation generator might enquire of the catalogue of transformations whether
there are partial solutions handling the input or delivering the output that would contribute to the new transformer one aspect
of the semantic linking research will give the terms that catalogue transformers and deal with their variations the contribution to
curation is very similar curation systems tend to require most of their metadata complying with their chosen standards transformations to
this can be handled as described for catalogues the same ingest treatment for data can also be applied subsequent requests
will potentially offer transformations to required forms the curation system often requires binding to software and computational contexts that generated
the data the issue of software description discussed below then comes into play the relationship with provenance is very similar
provenance systems are far more usable and useful if they achieve uniform representation of provenance but the subsystems called during
the enactment of an encoded scientific method often support different representations or only offer logs again if catalogue of transformations
is available it can draw on these it may also deliver transformations to the tools and systems that exploit provenance
one of the primary purposes of the architecture is to aid communication about the systems as they are planned constructed
maintained and used in particular the architecture should identify all relevant software probably the dominant topic of discussion platforms middleware
layers data intensive frameworks storage systems workflow systems other subsystems programs workflows libraries of functions and main services etc it
is necessary to describe precisely how they fit together at an abstract level this role is well supported by the
contributions of the envri reference model however as construction and operation are considered considerably more detail is needed for reasons
given in section the majority of the subassemblies and frameworks will be the product of other independent organisations which will
describe their product in terms they choose most of the conceptual space describing such components lacks standards or lacks the
adoption of standards consequently the concepts entities nomenclature and units used vary greatly they are also encoded differently and are
grounded on different ontologies most of which will not be explicit consequently there is substantial role for semantic linking helping
to consistently describe the parts in use and the operational relationships between them this may prove demanding as section explains
because ontological and semantic standards for this space are partial or under development see below for separate discussion regarding software
there is close relationship between semantic linking and the reference model the developed viewpoints and existing ontologies will be vital
starting point and will shape the direction of travel development of the existing viewpoints will provoke revision of some of
these ontological foundations and will require extensions the depth and precision of the engineering and technical viewpoints needed to guide
the architecture and infrastructure construction will certainly demand extensions it expected that the semantic linking and the reference model will
advance in tandem for these topics the description and handling of software is major challenge section points to work in
progress the recent work of eu wf4ever project belhajjame describes extensive work describing workflows the work in the admire to
describe data processing elements included properties of input and output handling rates to inform optimisers and validators martin the work
by the taverna group describing the web services available to bio scientists bhagat opened up this issue for substantial range
of web services in that case it required separate sustained funding to build critical mass of descriptions it is an
open question whether investment in relevant descriptions is already budgeted for in envriplus and the ri communities the deeper challenge
remains to establish comprehensive way of describing software that covers the range of software and aspects relevant for envriplus the
metadata descriptions in catalogues raised in section the software used to be recorded in provenance and curation with the data
it produced see sections and or the clarification of issues in architecture section it may be useful to first identify
the categories of software needed in the reference model and architectural conversations and to pick off these one by one
as way of dividing that challenge into feasible chunks assessing the envri reference model review the reference model described in
section is as much about organisation of the design and construction of research infrastructures as it is about technologies and
the review properly considers such matters the previous envri project saw substantial development of three viewpoints of the reference model
using odp changing the underpinning technology for representing the large distributed and multi organisational systems and development efforts is therefore
not an option odp has proved good approach for tackling the complexities of distributed systems with the scale and diversity
needed for environmental ris there are three issues clarified in section regarding the reference model assessing whether the new and
larger community of ris require revisions to the three already developed views in order to meet their needs some revision
will be necessary but the requirements gathering in section has not revealed any reasons for radical change other refinements to
the existing views such as the clarification of the stages of the data lifecycle see figure will need to be
recorded in odp and the ontologies developing the reference model further so that it can be used to formalise critical
aspects of the architecture see sections and and to plan and organise collaborative construction and maintenance involving the multiple organisations
within an ri for bespoke software the organisations of group of ris for shared subsystems and the organisations of resource
providers and technology producers as sustainability is taken into account see section this is precisely what odp was designed for
but it requires the development of engineering and technology viewpoints as planned in the envri plus project gaining sufficient traction
that the reference model is used not only at the strategic and planning level but also to shape and coordinate
the work of the engineers actually building the infrastructure this use is required to avoid the large project disasters that
are inevitable when the interfaces and inter dependencies are not precisely agreed and recorded word of mouth and exchanges of
documents email instant messages are all vital at arriving at mutual understanding about what should be done at such boundary
but they are completely inadequate when each software engineer is pressing on with their part of the implementation anchoring the
agreements in framework provided by the reference model and odp is good way of mitigating such disasters finding resources for
each of these lines of development simultaneously will be challenge the first requires dialogue with broad range of practitioners in
each ri good start was made during the requirements gathering see section and there are good working relationships inherited from
envri for some ris engaging the people in each ri who have the relevant information and then analysing and recording
it in digested form will take effort on both sides and elapsed time for digestion yet the next two issues
probably require immediate connections can these be based just on the legacy from envri the second extending the reach to
selection and assembly of subsystems and their integration by configuration software interfaces and bespoke front ends requires substantial input from
those who are expert in the details of each candidate component and those who fully understand the engineering trade offs
whilst odp and reference model experts can guide this process record findings and coordinate that buy in from experts in
various parts of the system is essential but these experts are usually hard pressed developing solutions in their own context
often they are under lot of pressure to support existing deployments and to deliver new functionality or capabilities they will
not allow themselves to be distracted by engaging in work on the reference model unless there is an obvious pay
off this takes us to the final parallel line of required the reference model could in principle save much effort
by successfully partitioning and coordinating design and construction tasks to avoid duplication and gaps and to ensure the process of
assembly works smoothly with the parts working well together there are three preconditions to enable this to proceed first sufficient
proportion of the software engineers at the coal face of importing subsystems and developing software need to engage using the
reference model when they have questions and improving it when they find the current answers insufficient second enough of the
context in which they are working has to be described at the level at which they work at least information
engineering and technology viewpoints third the third parties providing systems platforms resource providers and technologies such as database and workflow
systems have to engage describing their systems and conforming to agreements cast in the reference model all three of these
lines of development would benefit from improved productivity yielded by good tools these tools should facilitate authoring refining validating and
interrogating the reference model ideally they should also support automated generation of interface and framework code section identifies two commercial
tools but does not find any open source tools of comparable power it is not economically feasible to get such
commercial licenses and follow up training from these vendors in the context of envriplus but in the context of the
wider envri community and the long term lifetime of the environmental ris this would be practical given the scale of
investment needed to construct operate and maintain ris over their extended lifetimes engineering tooling is strategic issue in ris management
the long term utility of design analysis and system assembly is enhanced if there are suitable high level definitions that
are independent of details of specific platform technologies the reference model is potentially good medium for this but it requires
so much investment that it may only pay off if it also meets the practical needs of those building the
infrastructure but this is precisely what odp was designed for wholehearted commitment is needed to reach the thresholds where its
benefits are felt by all of those planning designing building and maintaining infrastructure for the ris it is an open
question whether this can be achieved with the envriplus resources assessing the review of compute storage and network provision section
considers the provision of ict resources is essential to enable every step of the data lifecycle every part of scientific
method development from teleconferencing about the first idea to the final polished and optimised formalisation as packaged workflow for assuring
sufficient persistence for all data metadata software and their relationships and for supporting the human computer interactions of all practitioners
in the geographically distributed communities these resources build on globally and nationally provided underpinnings such as the internet and span
all the way to the computers laptops and mobile devices individuals use section views this digital ecosystem environment the platform
on which we build figure from the viewpoint of pan european organisations and focuses on three aspects digital communication the
support for are data movement computer computer interaction beyond very local connections and human computer interaction covering anything from remote
field or marine observers to experts in presenting results in 3d video cave good proportion of this happens under the
aegis of geant that coordinates the interconnection of national network provisions specialised arrangements are needed for the remainder the combination
needs to be delivered as consistent functionality with standard interfaces even though the non functional parameters vary greatly fortunately the
established internet standards and protocols deliver this consistency almost everywhere computation may take place at every step on the data
lifecycle may be routine process that is applied to every batch of data or may be demanding one off simulation
run or massive data analysis the provision for many of these computations is met by home institution resources provided to
practitioners the high throughput computing is met by local or regional clusters or by the pan european cloud providers listed
in section similarly the hpc facilities may be local regional or under the aegis of prace then specialised computational platforms
are needed for some data intensive workload patterns for productivity of researchers developers and data handling experts presenting these facilities
in coherent and consistent manner would be very helpful that would also open up avenues of optimisation selecting targets that
reduced costs and protect investment in computer based methods as they would no longer be tightly tied to the platforms
that are evolving unfortunately today there do not appear to be standards for job submission scheduling interaction with running jobs
and provenance collection that will yield the required consistency the protocols intermediate software authorisation and authentication diagnostic interfaces and paths
to get support vary significantly over this range in some cases for valid reasons storage underpinning all of the lifecycle
are arrangements for transient or persistent storage of all of the artefacts involved metadata data and software the choices of
provision are determined by factors such as data volume access patterns target persistence duration and speed of required operations cost
and energy consumption such storage only preserves the bits to an understood reliability it needs to be interfaced by software
that handles bundles of data such as files sets of files and scientific databases it also needs to maintain relationships
between such entities file name query or some metadata normally held in database with bundles such as set of files
the primary pan european organisation delivering storage and covering many patterns of use is eudat unfortunately it has far less
maturity and assured longevity than the providers of the previous two essential ict components digital communications and computation this is
strange accident of history it is an acute concern in these days of data driven science and particularly for envriplus
focusing on the data lifecycle the predominant viewpoint of today technological provision of storage systems is to focus on the
file this omits the opportunity to exploit significant structures often latent within or between files the scientific databases are attempting
to retain the good properties of files but also handle such structure well even if we only consider the file
systems it is unclear how consistently functionality is provided and supported across multiple providers these pan european resource providers many
commercial providers and some of the major institutional providers prace and other hpc sites and national environmental services also contribute
to other important factors affordable sustainability and support being the primary example see section this benefit derives from several contributions
for example provision or shared support of significant subsystems significant subsystems may be set up tuned to current loads mapped
to suitable lower level platforms and administered by the collaborating resource provider for example the many file handling and metadata
handling services provided by eudat or the job submission and workload management services supported by cloud providers incorporating and building
on their supplied systems substantially reduces the ri and envriplus specific software that needs to be maintained and supported in
the long run learning about and understanding how to use subsystems provided by others can yield substantial savings the maintenance
costs are amortised over larger community if envriplus coordinates the ris so that many of them are building on the
same subsystem this benefits both parties the resource provider or vendor gains large boost to its customer base from modest
amount of mutual adaption and customer understanding envriplus presents set of ris and their infrastructure as one customer with reduced
cost for customer acquisition and initial training the envriplus community gains the advantage of maintenance and advice from an organisation
experienced in maintaining and advising on the subsystem this is an important step in the sustainability of infrastructures see section
an absolute essential if the ri communities are to depend on the infrastructure consequently developing list of suitable subsystems to
adopt is high priority envri plus task see list item in section this should be approach incrementally and be guided
by architectural section and reference model see section considerations as well as requirements see section engineering and integration for each
subsystem or platform layer in use great deal of effort is needed choosing the lower layers mapping and configuring the
installations and making the parts work together this takes great deal of engineering skill and multiple categories of specialist knowledge
and system administration skills the providers of apache spark have already developed effective integration with many other commonly required systems
such as python mongo db and as another example pegasus and dispel4py work well together exploit mapping to apache storm
and have deployment scripts that set this up by deploying docker images on cloud infrastructure filgueira 2016b external organisation who
provide major subsystem will have done this work drawing on skills and knowledge they have built through experience or being
able to bring in the necessary expertise justified by their larger user or customer base once such systems are operational
they need tuning to carry the presented workloads then need capacity planning and they need optimisation to reduce energy consumption
and operational costs again the increased user base and organisational longevity makes this much more achievable than it is for
the individual ri or small group of ris maintenance as explained in section there are three drivers which make maintenance
essential changes in the digital context that must be accommodated or should be exploited extensions to functionalities and capabilities or
correcting latent errors the first two of these dominate and will certainly be important for every ri again the organisations
with larger customer bases will be able to afford to do this well and benefit from larger pool of skills
and engineering knowledge conversely if the ris adopt this route they will benefit from advances other communities have initiated user
and developer support good user and developer support requires training material on line notes webinars re playable sessions frequently asked
questions compelling exemplars etc that take significant effort to produce refine and tailor for the various audiences in table larger
customer base makes this much more feasible and increases the chance of good quality for particular group at particular stage
of engaging with the subsystem connection to services the set of subsystems and the base platform delivered by provider organisation
still needs to be used from non local devices and to support wide set of interactions from people and computers
establishing frameworks for this with virtual research environments or virtual laboratories that suit ri communities may draw heavily on interaction
and user arrangements the provider has developed for example the authentication and identification provisions developed by egi or geant may
be sufficient for some categories of user community similarly external connections with other services for bulk data transport at low
cost or for synchronisation with minimum delay may already be an established arrangement some forms of automated mapping to deploy
required multi node computing environments or to handle particular forms of workflow may be re usable and save infrastructure or
scientific method developers much work the above list shows that there are potentially substantial benefits from working with some of
these suppliers and with using some of the subsystems they offer but it is impractical to use too many in
one infrastructure they may not fit well together and the resulting infrastructure would be excessively complex analysis of the suitable
compositions should proceed by developing the engineering and technology viewpoints in the envri reference model and then using this as
framework to select candidate list of subsystem and provider bedfellows to best host ri requirements in the interim use cases
should investigate specific collaborations in order to increase the background knowledge available when that selection is made making critical decisions
about software subsystems the long term impact from decisions about which platforms and subsystems to use as an ri infrastructure
is designed and constructed are so significant that decisions should be taken very carefully however they are often taken coincidentally
an individual or agile development team starts using technology because it is familiar is already used in an example they
are developing from or it is the first that comes to hand this is appropriate during agile co development and
when try to get prototype running quickly however that needs to be de coupled from longer term commitments the complex
set of aspects affecting such decisions are set out in section as in major construction projects it is often the
architect who has to identify such crucial questions and ensure that they are answered by suitably qualified and constituted groups
representing the clients at present and in the future an example was also illustrated at the end of section factors
such as comparing the up front costs financial staff training installation effort disruption to current working practices against the long
term costs energy consumption platform costs staff time for maintenance and user support licenses and service costs etc depend very
much on time scales target availability and reliability and required usability these are policy matters as is the judgement of
the impact of ict delays versus cost or rapidity of processing and responding to user versus utilisation of platform of
great concern is whether the user community will adopt features they could benefit from whether staff already performing many recurrent
jobs will take on extra ones these are policy issues that need clarification they may only emerge when decisions need
to be made consequently there are at least the following factors that affect the quality of ict decisions the quality
and clarity of the policy framework and procedures for revising it if necessary the final decision making body who oversees
it who establishes its members and gives them authority what is its scope does it have the right mix of
experts does it represent all relevant constituencies if it is on per ri basis how do the inter ri factors
get assessed how prompt are its decisions how binding are they what resources does it have an appropriate decision making
procedure how are the important decisions recognised brought to the fore and clarified who gets chance to make input what
investment is made in evidence gathering through investigations trials benchmarks and systematic comparisons how easily can it obtain advice from
experts for example can it employ consultants or make potential commitments to suppliers in exchange for them supporting the investigations
how are these information gathering exercises organised so much time of so many people researchers and all the other roles
see table will be wasted or saved depending on the quality of these decisions that it is well worth investing
significant staff time making the decisions carefully the decisions may be partitioned into tractable steps these steps interact significantly so
they are potentially intractable if taken all together some of the providers of resources may offer bundle of the choices
so that selecting them pushes you towards particular choice on many points we illustrate the idea of steps by working
from the lowest levels of the platform upward computational provision the majority of this is provided by using standard computing
nodes organised as clusters or clouds some provision is needed for specialised loads such as those requiring high speed interconnection
between nodes and those requiring very high data or those benefitting from very large shared memory questions arise about how
this provision is organised and paid for whether its location is important for regulatory or data transport reasons what operating
systems virtualisation methods resource allocation methods and monitoring methods are supported what file systems and database systems are included how
well is data transport on and off sites supported and what does it cost what are the local storage provisions
what workload submission mechanisms does it support for batch jobs for complex graphs of tasks in workflows for sustained services
and for interactive use what support is there for diagnostics workload analysis resource planning elasticity and provenance capture what forms
of usage records and accounting does it support further discussions about some systems can be found in simmhan and fox
energy costs should be taken into account two examples are wilde and siew the provision of computation that matches environmental
science requirements at least at the hpc end of the spectrum is considered by frank storage provision what scales of
storage and rates of data transport are provided at what cost can these be expanded what is the reliability of
persistent storage what underlying mix of technologies does the system use does it automatically handle this mix presenting simple stable
interfaces what data organisational systems are well mapped to these underpinnings distributed file systems scalable database systems composite dta management
services those providing high volume catalogues of metadata bound to data with or without pids what forms of usage records
and accounting does it support digital communications what range of geographical regions categories of users and categories of organisation does
the service cover what models of protection and resource sharing does it use how are gateways with other digital communication
systems arranged what protocols does it support what are the ranges of bandwidth and latency it offers for the data
trips relevant to the ris concerned what forms of usage records and accounting does it support some discussion can be
found in masson authentication authorisation accounting and identification aaai what range of users do the aaai mechanisms support for example
from members of the public to administrators controlling access to major or sensitive resources what modes of identification does it
handle does this cover all of the community requirements with minimum disruption to existing practices what range of trust is
associated with the authorities allocating credentials how many different trust relationships have to be taken into account can sensitive resources
and roles be restricted to people granted authenticated identity by sufficiently trusted authority to what extent are roles and groups
managed can authorisation take into account properties of the data such as its source or date of creation can permitted
actions take into account the authority as well as role for example members of the public might be restricted to
maximum of minute on cores and gb result delivery whereas project or ri authorised research might by default have limit
of hour cores and delivery of up to tb of data exceptional competition winners who had convinced peers of the
value of their planned work might submit hour core jobs yielding pb of data frameworks accommodating such range may be
needed data intensive middleware what kind of middleware will best suit the workload patterns apache storm apache spark pilot jobs
turilli or data intensive workflows pegasus handled the ligo data for gravity wave discovery deelman or simmhan how well will
it scale given sufficient nodes how well does it parallelise automatically database systems what dbms does the community already use
and how deeply are they bound to its idiosyncrasies what models does candidate dbms support relational xml sql and nosql
rdf scientific how well does it support development and production how does it scale haw well does it exploit new
technologies for storage and new models of computation provision does it provide time stamped query support participant such as bgs
uses oracle databases to underpin many complex and very large catalogues and information services that meet demanding targets the astrophysics
community ivoa see section and szalay pioneered very large scale sky catalogues that have to manage identities of many more
than or objects with sophisticated metadata high rates of ingest and demanding global user communities their queries also involved significant
computation this has led to whole community of database researchers and demanding users that meets regularly at the extremely large
database conference xldb that should be tracked by the envriplus community this campaign has triggered substantial modifications to traditional dbms
for example in the range of data including files that they handle and in providing non atomic options for large
updates but more significant is the flowering of scientific databases that previously were small research niche these hold time series
matrices and bulk data such as images they also accommodate the representation of uncertainty value ranges and support efficient queries
over all of these data types examples are scidb206 monetdb207 which is organised as column store to accelerate retrieval particularly
of projections that are often used in science and rasdaman208 the latter is already well integrated with geospatial data and
is being considered for eiscat data how ell does the system support distributed query and for what query languages workflow
system what kind of workflow system is needed task oriented tavrena pegasus and knime or stream oriented dispel4py how well
does it help scientists formalise share and develop their methods how well and how automatically does it handle all of
the data management needed movement of data between sites how well does it adapt to load and context how well
does it handle partial failures clean up and resumption with minimum re computation how optimally and automatically does it select
platform targets and map onto them how well does it deliver provenance how good are its diagnostics what forms of
usage records and accounting does it support interaction system what modes and models of user interaction are supported how easy
are these to use how easy are they to develop for how many tool sets and apps already use them
tools the crucial issue is productivity how well do researchers get on when using this system providing them with tools
that meet their needs is key part of enabling their productivity to what extent do the tools meet their needs
how hard is it to learn to use them as an expert in some other tools as novice how well
will they integrate with the planned system for example reporting the information required by the provenance system in form it
can handle improving the productivity of those who design develop run and manage the infrastructure is equally important as that
will enable them to deliver better facilities and more agile support consequently investment in their tools with similar considerations should
be undertaken all of the above are illustrative of the appropriate partitioning and illustrative of the questions that should be
asked for each partition it may be good question for think tank how should the ris partition the decisions about
which software systems to use ancillary questions are to what extent can they do this collaboratively and how will it
be resourced once the partitioning and sequencing is agreed each of the investigations should be launched in some optimised order
each investigation may be sparked off by its own think tank deciding on scope constraints key questions experts needed and
plan for conducting the investigation and forming conclusion the same ancillary questions apply the investigations should also consider sustainability factors
and support issues such as the extent to which the candidate product has an active user community with similar requirements
the resources and expertise the supplier has and so on see section characterisation of task outcomes and implications the overall
findings of requirements gathering and technology review are consistent with the theme plan and indeed with other parts of envriplus
thus the position taken when the project was proposed is largely refreshed and endorsed however there are many detailed findings
that are collated below section we introduce number of categories immediately below and list under those headings the specific tactical
and organisational suggestions that should mainly be considered by theme in section the longer term and strategic issues are collated
in section these should concern those considering the future direction of ris and of the environmental cluster some may have
further reach we recognise here that many of the detailed recommendations emerging from task have relatively short term or localised
relevance examples are enumerated in section these can be categorised into the following groups making best use of task results
the follow up needs to ensure the information represented by this document and the associated wikis is accessible well presented
and effectively communicated and promoted it will be worthwhile investing in maintaining the information to keep it up to date
and in building on the communication networks and mechanisms already established such as the envri community platform209 it is foundation
worth building on but it is also worth maintain it should remain live resource universe of discourse the conceptual frameworks
for supporting the research in each ri and the nature of underpinning technology is complex and full of detail this
presents barriers for immediate usability and implementation of functionality however the longer term frameworks for working practices and the software
that support them are better understood when they are discussed in terms of more abstract viewpoint the envri reference model
provides vocabulary for discussion at this level as far as possible requirements and solutions should be expressed in such terms
to reveal potential commonalities and properties that change relatively slowly the detail should then be built up using the engineering
and technology viewpoints see section to deliver and support user requirements in more sustainable way awareness raising and training there
is widespread recognition of the need to improve and extend understanding and at the same time encourage cross boundary communication
see section the role of theme supported by theme knowledge transfer in this internal campaign will include urgent transformation of
results into material key for understanding potential and issues the key asset of the ri communities and the envriplus project
is great diversity of experience viewpoints and skills every effort should be made to capitalise of this strength by pooling
intellectual effort in series of think tanks that focus on priority issues and bring together experts from across the borders
between disciplines and across the borders between technological viewpoints to create new strategies series of calls for proposals for such
think tanks should be organised usability and take up the adoption of new data handling technology will depend on how
well it is packaged and made available several aspects of this should receive attention this is crucial for sufficient take
up to happen before harmonisation and resource or effort sharing will be achievable shared subsystems and sustainability there are several
important reasons why sharing substantial subsystems often intermediate level software frameworks is valuable the costs of building installing configuring and
supporting these shared capabilities are then shared bringing economies to the ri when they set up run and maintain their
infrastructure sustainability becomes more achievable having relevant experts available to establish tune and support these subsystems becomes achievable if there
are fewer instances to consider underlying software consistency makes harmonisation more feasible and reduces the effort involved in boundary crossing
or moving to new context as section explains scientists and all the other practitioners associated with an ri and its
community quickly become dependent on the software that enables their scientific methods and working practices consequently loss of that software
could be severe blow the envriplus project and the ris have budget for the steps necessary to sustain software see
software sustainability institute ssi typically of software lifetime costs this motivates the need for great care in the choice of
software already well supported and limits to the additional software on which the ris will depend over the next four
years arising from political initiatives of the european research area the esfri forum the irg the european cloud initiative etc
the relationships between the ris and the foundational infrastructure providers egi eudat prace assume great prominence the ris will be
under considerable pressure to learn to outsource their it needs and to work with these providers these providers really have
to learn to adapt and to be agile in meeting ri needs groups of ris with clearly articulated similar requirements
will be in much stronger position for negotiating and developing alliances with those providers as well as with commercial it
consultancies developers and suppliers opportunities for new markets potentially attractive to sme ict suppliers especially software suppliers will be created
through harmonisation when ris act together we then draw attention to aspects that have longer term or more pervasive application
however both are important the short term aspects have to be addressed to meet immediate needs so that practitioners in
the affected domains can make progress in the short term this then builds confidence in interdependencies and technologies that is
essential for sustained investment and collaboration without this researchers will not trust the emerging technology they will avoid dependency and
in consequence fail to reap its full potential once that confidence has been built the longer term issues become critical
they address the strategic questions as to what routes to take to sustain and continue to advance the research without
incurring unaffordable costs some suggestions of strategic issues that may be considered are listed in section they arise from the
issues discussed in sections impact section has drawn together the outcomes of the requirements gathering similarly section introduced four general
issues and then summarised the technology reviews we now consider based on the categories outlined in section the implications for
the envriplus project in section and longer term issues that may concern the ris or the wider community are considered
in section impact on project the short term and focused results from task lead to series of confirmations of current
plans and few issues that require attention and potentially could lead to modified plans items of concurrence are dealt with
lightly or omitted if they have already been stated items provoking further thought and investigation are listed in the order
that they are reported in the above work in consequence they are not in any way prioritised210 the theme and
envriplus management should consider whether these need further attention and if so how to prioritise and resource the follow up
activities making best use of task5 results ensure that the envriplus glossary contains all of the non familiar terms used
in this report this action will be undertaken by the project management and task teams after the spring envri week
establish the wikis used by this report in properly supported context this work is currently underway due to the help
of egi working in conjunction with alex hardisty cu and magdalena brus uhel the expected completion time is may much
of the recent task work has focused on improving the content of this document however the primary reference material delivered
by task will reside in the above wiki it will be necessary to update the wiki with all the insights
useful presentations tables and figures in this report this report itself once it is agreed for publication should also be
made accessible from that wiki in each of its spaces this wiki update is the responsibility of the task team
and should be completed soon after the spring envriplus week the report contains an initial set of references that may
be useful to the whole project and will certainly be needed for future documents written by theme members theme or
the whole of envriplus should set up arrangements for sharing the list via organised wiki pages attention should be drawn
to the report and its conclusions via the envriplus newsletter meeting to plan this has been scheduled for the spring
envri week short documents to pages summarising specific aspects of this report targeted at particular readerships should be developed for
example one to be considered by each work package and one to be considered by strategic planners in the environmental
ri cluster and beyond the plan for this choice of target audiences and commitment to the editorial effort required will
be made during the spring envri plus week external attention and critical review of the conclusions should be obtained by
publishing one or more derived papers these should encourage open debate and refinement on the relevant wiki spaces target venues
include conferences such as ieee science and works eu project events such as egi and eudat and eu h2020 informatics
events journal such as future generations of computer systems fgcs this refinement of the requirements and technology wiki spaces should
be moderated if it is successful the envriplus project steering committee should consider how it can be supported and continued
after the end of envriplus one should not do anything to slow down or distract agile development teams tackling use
case however the gathered requirements and technology reviews should provide useful information as they start if they have time they
should report any deficiencies that are noticed at the end of their campaign they will have deeper knowledge about requirements
and technology if they can contribute key points or synopsis of this to the relevant wiki that would be very
helpful this will be part of keeping this material live and building on it the present requirements gathering and analysis
is an initial solid effort in fast changing field in some topics there is room for improvement of coverage and
precision whereas in others sharp definition has been achieved but they could still be overtaken by external events in the
digital ecosystem in any case the digital world is experiencing period of rapid change that is perturbing the digital ecology
and the patterns of work using its resources as this change is substantial and rapid the envriplus project should commit
to focused effort of update so that it is coordinated has high impact and is efficient we would suggest that
this should happen between m24 and m30 of the project this will need resources and possibly should be shaped by
think tank that ensures that it has substantial contributions from the ris and practitioners working at the coal face of
developing and running infrastructures it may be appropriate to set up think tank to establish its direction and to recruit
balance of viewpoints an integrated view of the technology reviews should be developed as roadmap for the next ten years
of supporting data lifecycles in data driven sciences particularly those falling within the envriplus area of authority this will be
feasible if we take high level view informed by the reference model after allowing time for the baseline information to
be refined this may also be suitable topic for think tank it would require sustained effort to develop this the
think tank might clarify the scope identify key questions make the case for valuing such an effort recruit resources and
balance of experts and initiate the work thus this think tank would need sufficient time and effort itself to delve
deeply into the topics and to form coherent integrated view of the potential roadmap consider making optimisation cross cutting concern
as localised optimisation may simply move costs into different working practices into different stages in the data lifecycle or into
different subsystems within an infrastructure universe of discourse the relationships between the requirements gathered and the reference model should be
studied and clarified the focus of task the technology reviews should be input to initial versions of the engineering and
technology viewpoints this may reveal structural issues in the reference model or areas of engineering and technology that need to
be better understood again planned in task the cross cutting architectural review see section page should be analysed by casting
critical parts of its proposals in terms of the rm as the architectural review is intended to shape the systems
built by ris or the kits from which they are built and their interworking and this is precisely the role
of the reference model review the use of workflow notations and apis in the current and planned infrastructures to ensure
that as far as possible they are cast in higher level terms to avoid undue lock in to particular technologies
and representation details awareness raising and training the requirements gathering discovered widespread interest in further awareness raising and training to
help those engaged in envriplus and in associated ris this would help participants better understand the existing plans and implementation
strategies and their potential benefit for users and infrastructure builders topics range across all of the stages of the data
lifecycle and courses could target each of the roles see table the teams engaged in theme should actively engage in
this including communicating about topics that need explaining as well as topics in which they are already expert this should
be conducted as webinar series to reach the distributed community it should also involve multiple disciplines and multiple roles as
means of stimulating boundary crossing webinars and or their supporting materials should become available via the envri community platform209 and
training platform see also item in this list above which may identify initial foci and prepare some of the relevant
material the understanding is greatly accelerated and participants are convinced if there are working examples of good solutions for participants
to try in remote supported hands on sessions this means that effort should be invested in forming easily presented and
understood exemplars of developing services and functionality with corresponding illustration as to how they benefit the research goals or those
supporting the research communities as above these should become available via the training and community platforms visibility can be promoted
by promoting ad hoc think tanks of engaged scientists and infrastructure operators especially to initiate cross border cooperation calls for
proposals on topics for ad hoc think tanks should be considered by giving selected think tank proposals high publicity profile
their example role is expected to strongly contribute to convincing the wider community it also is mechanism to enhance the
joint envriplus cooperation even after the end of this project the above two activities should provide an opportunity to refresh
and deepen envriplus understanding of requirements and of all the practitioners working with data in the ris it may help
transform the initial results from each agile task force into more widely applicable solutions usability and take up expectations about
the quality of interfaces and tools for managing and analysing data are rising rapidly due to commercial investment into good
interaction frameworks for mobile devices and mobile workers it is reasonable to expect that the researchers and the support teams
can also have such good interfaces for their work with environmental and earth sciences data apis should be structured and
shaped appropriately as microservices offering suitable controls and formats to use available interaction frameworks wherever relevant this will help communicate
the value of envriplus data and products to wider audience of potential adopters as well as helping in the internal
communication identified above in alliance with others deliver with high priority the simply packaged functionality that many researchers need for
example from section we may consider packaged service for accessing data bringing together the various parts currently being developed in
simple to use manner this could for example feature shopping basket into which researchers place items datasets found in catalogues
for later retrieval and use and recommender features along the lines of users who retrieved this data also retrieved or
datasets frequently used together cf the functionalities of typical online marketplaces such as amazon and alibaba provision of user and
group workspaces for handling their data under their own control but with easy download via the access facility upload to
any envriplus service and export practical mechanisms for enabling researchers to easily manage their own data collections selected and possibly
copied data from any of the sources they have access to data they have produced intermediate results for methods they
are developing results that are pending publication making this easy has been shown to be very useful in other contexts
schuler wolstencroft context for developing encodings of methods as workflows where users can easily experiment develop test and refine their
ideas once they have validated this to their satisfaction they can submit their work unchanged to production scale facilities to
progress towards production they can move fluently between the experimental context on their laptops and the production context data intensive
or hpc cluster easily and without changing their representation of the formalised method this was provided in verce atkinson it
greatly accelerates the rate of innovation as researchers can make progress themselves and call on other experts only when they
have explored their ideas to their own satisfaction and are now ready for repeated production use of their method at
which point some optimisation by experts may be warranted improve the training culture and tooling of software engineering that facilitates
the work of the engineers building the infrastructures for ris and the ict experts helping fashion and formalise scientific methods
similarly for those overseeing and implementing stages of the data lifecycle particularly curation and publication improving their work practices working
environment and productivity is as important as that of the end users in the long term to achieve this consider
bringing in or at least sharing experience and pooling support effort for tooling such campaigns might benefit from close collaboration
with organisations such as egi eudat and prace as well as with relevant commercial tool and training providers it may
be assisted by working groups of the rda it will need investment of effort by envriplus to reduce costs over
the longer term shared subsystems and sustainability identify small set of middleware components such as data intensive frameworks inter process
communication and nosql or scientific databases that should be part of the common shared infrastructure being built for or supplied
to ris for these vital underpinning software layers locating them centrally in the engineering and technology viewpoints of the envri
reference model makes it clear to everyone the role they play in an ri section choices should take into account
sharing responsibility with other organisations and the sustainability issues discussed in section consider the priorities and scheduling needed to establish
these in processing environments there is widespread need for workflow systems reported in many areas for automating and refining all
of the data handling steps in the data lifecycle and for implementing scalable and repeatable scientific methods for research and
the applications of research at present many different technologies and systems are in use envriplus should help the ris and
their communities focus on small number of workflow systems and then establish shared maintenance development and support to sustain those
for as long as required they will need to work well with the layers chosen for item above and similar
criteria apply but there are additional criteria here as it is necessary to cover the different patterns of data intensive
workload and to seek good tooling for all of the roles involved envriplus should also help the research communities adopt
mechanism for sharing such as myexperiment de roure so that encoding of methods and parts of methods get re used
rather than re invented they should also help with the introduction of appropriate curation for workflows for example as suggested
in belhajjame consider the extent to which the architectural proposals in section can take into account the longer term potential
for common support for data intensive federations see section if practical steer the architectural developments to facilitate replacement of relevant
parts with common diff kernel consider the extent to which the architectural proposals in section and the more specific topics
take into account sustainability issues identified in section take these issues into account wherever and whenever it is feasible to
do so to save long term maintenance and support costs impact on stakeholders this section is aimed at the strategists
in the research infrastructures as the recommendations are longer term the impact of taking them into account may not be
significant in the lifetime of envriplus until but they will be significant for ris as they have planned lifetimes of
or more years and their scientists will depend on their ability to sustain as well as develop capabilities shorter term
implications from task are dealt with in section and include initial steps preparing for these long term strategic issues theme
will ensure that ri stakeholders are properly consulted as they consider any issues that have been raised there the impact
on stakeholders is restricted to longer term issues so that they are able to consider these in their strategic planning
the topics raised in sections to each lead to strategically significant issues which should concern the ri stakeholders during envriplus
and beyond we conclude by raising the issue as to how decisions about ict choices which will have very long
term impact are made improving interdisciplinary collaboration the background to this is summarised in section the envriplus community should help
drive wider programme to gain recognition of the value of people who are adept at working across boundaries it should
join in the growing campaign to develop training and education opportunities for those who want or need the ability to
collaborate across traditional intellectual cultural and role boundaries this should include wide reaching summer schools it is probable that funding
for such campaigns could be gained at this time by association with the popular data science flag but it should
be steered to take broader view of the issues envriplus leadership might at least stimulate proposals for such activity and
endorse those that look well judged think tank might be convened to plot the strategy for an effective campaign early
summer schools could clarify the critical issues and inject understanding into proposals to build pan european educational campaign to substantially
improve europe boundary crossing collaboration capacity leading the formation of global environmental sounding board just as for astrophysics see section
