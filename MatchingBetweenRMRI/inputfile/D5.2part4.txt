in which the object can participate at given instant in time as defined in odp iso iec the data states
and their changes as effects of actions are illustrated in data states in their lifecycle data may have certain states
raw the primary results of observations or measurements identified data which has been assigned unique identifier annotated data that are
connected to concepts describing their meaning qa assessed data that have undergone checks and are connected with descriptions of the
results of those checks assigned metadata data that are connected to metadata which describe those data finally reviewed data that
have undergone final review and therefore will not be changed any more mapped data that are mapped to certain conceptual
model published data that are presented to the outside world processed data that have undergone processing evaluation transformation note the
state raw refers to data as received into the ict elements of the research infrastructure some pre processing may or
may not have been carried out closer to where measurements and observations were made these states are referential states the
instantiated chain of data lifecyle can be expressed in data provenance unique identifier uid with reference to given type of
data objects unique identifier uid is any identifier which is guaranteed to be unique among all identifiers used for those
type of objects and for specific purpose there are main generation strategies serial numbers assigned incrementally random numbers selected from
number space much larger than the maximum or expected number of objects to be identified although not really unique some
identifiers of this type may be appropriate for identifying objects in many practical applications and are with abuse of language
still referred to as unique names or codes allocated by choice which are forced to be unique by keeping central
registry the above methods can be combined hierarchically or singly to create other generation schemes which guarantee uniqueness in many
cases single object may have more than one unique identifier each of which identifies it for different purpose for example
single object can be assigned with the following identifiers global unique for higher level community local unique for the subcommunity
the critical issues of unique identifiers include but not limited to long term persistence without efficient management tools uids can
be lost resolvability without efficient management tools the linkage between uid and its associated contents can be lost backup copy
of persistent data so it may be used to restore the original after data loss event mapping rule configuration directives
used for model to model transformation mapping rules can be transformation rules for arithmetic values mapping from one unit to
another from linear functions like to multivariate functions ordinal and nominal values transforming classifications according to classification system to classification
system data descriptions metadata or semantic annotation or qa annotation parameter names and descriptions can be method names and descriptions
sampling descriptions data provenance metadata that traces the origins of data and records all state changes of data during their
lifecycle and their movements between storages creation of an entry into the data provenance records triggered by any actions typically
contains date time of action actor type of action data identification data provenance system is an annotation system for managing
data provenances usually unique identifiers are used to refer the data in their different states and for the description of
the different states service description description of services and processes available for reuse the description is needed to facilitate usage
the service description usually includes reference to service or process making it available for reuse within research infrastructure or within
an open network like the internet usually such descriptions include the accessibility of the service the description of the interfaces
the description of behavior and or implemented algorithms such descriptions are usually done along service description standards wsdl web service
description language within some service description languages semantic descriptions of the services and or interfaces are possible sawsdl semantic annotations
for wsdl institution in the context of environmental ris an institution is any organisation participating in the ri within any
of the communities which conform that ri person human actor member of an institution which may undetake one or more
roles within community project in the context of environmental ris the project is collaborative enterprise planned to facilitate the acquisition
curation publishing processing and use of researcg data community collaboration which consists of set of roles agreeing their objective to
achieve stated business purpose role role is collection of iv actions that can be performed any number of times concurrently
or successively iv information object instances information object instances are used to define valid instances of information objects as explained
earlier an information object can have several state transitions an information object instance is iv information object instances iv information
object instances notation model of an information object at particular state the diagram on the right shows examples of different
object instances the main difference with information object is that the status of the information object instances is assigned value
from the list of allowed states information objects instances are needed for two purposes to show the data state changes
as effects of actions to show the relations between valid states of related data types for instance that reaching the
published requires specific series of previous states which can be traced for qa and provenance validation the diagram also includes
two types of conceptual models local conceptual model and global conceptual model global conceptual model set of concepts accepted by
data sharing community global conceptual models contain global concepts examples of these types of models are global thesauri like gemet
eurovoc agrovoc or global ontologies like gene ontology local conceptual model set of concepts locally agreed by limited user community
such as the members of research institution local conceptual model contains concepts which have specific meaning according to the community
using them for instance local definitions of person institute or data conceptual model can be local or global depending on
the size of the community which commits to it local conceptual models can contain concepts borrowed from global conceptual models
alternatively mapping rules can be established to determine equivalences between global and local concepts iv states the envri rm iv
defines data states and metadata states as the set of attributes which determine the actions that can be performed over
given information object the state changes together with the iv information actions can be used to model the behaviour of
data as it is managed by the ri the diagram below shows the states of three iv objects measurement result
persistent data and metadata and their relationships iv state diagrams iv state diagrams notation this diagram shows all the possible
states for each of the iv objects the first iv object left is measurement result object the object is created
from performmeasurementorobservation action the measurement result has only one state raw the second iv object middle is persistentdata object the
storedata action triggers the creation of the new persistent data object in the ri which has raw state this object
can have six states the transitions shown in the figure indicate possible transitions which can occur during the lifetime of
the object the third iv object right is metadata object the addmetadata action triggers the creation of the new metadata
object in the ri this object can have three states however the transitions from registered to published is not directly
triggered the diagram indicates that publish metadata must occur simultaneously with publish data in the diagram the filled circle indicates
starting point or junction used as starting point filled circle indicates pseudo state which represents the start of the lifetime
of an object instance used as junction the filled circle indicates pseudo state where paths merge or split the rectangles
with label in the upper left corner are used to indicate the object whose states are represented the rectangles with
rounded corners are used to indicate states each arrow indicates transition between states the label of the arrow indicates the
activity that triggers the transition the bar figure in the diagram indicates pseudo state that can represent fork or merge
of paths the type of diagram presented is an uml state machine diagram simple use example the diagram shows the
series of actions applied change the state of the iv object until it reaches published state in this diagram only
the persistent data object is shown the diagram is linear and depicts contains only subset of the allowed states and
actions this subset can vary from one ri to another ris can model their data lifecycles using state machine diagrams
the states described in the iv states diagram indicate set of possible paths to follow when representing the data lifecycle
the instances developed by ris may chose the states they need to include to represent their corresponding data lifecycles ris
can also include additional states which help them better represent their data processing flows iv state diagrams simple use example
notation advanced use example the diagram shows the lifecycles of five iv objects and the way in which they relate
to each other the example is more complex but it is still linear and easy to follow reading the diagram
from the left to right and top to bottom it is possible to describe the lifecycles of the five iv
objects the first object is called survey the diagram indicates that survey is type of measurementresult when the survey object
is stored new object is created surveydata type of persistentdata object the surveydata object is then identified after identifying back
up copy of the object named securedstoreddata is created once the securedstoreddata is stored and identified the state of the
surveydata object changes to backedup after backup the ri adds metadata to the secureddata object the surveymetadata object is created
and registered this causes the change in the state of the surveydata object to assignedmetadata subsequently the object is qa
assesed and reviewed once the surveydata is finally reviewed the ri creates new persistentdata object publishableresult which results from the
processing of the surveydata object the published result data object is then qa assesed and finally reviewed the final pair
of activities publish both the publishableresult and the surveymetadata objects in this example most of the states and actions are
still subsets from the ones originally introduced on he iv state diagram the only exception is the backedup state this
is still valid ris can adapt the diagrams to their particular needs adding states and actions to better illustrate their
data lifecycles as close to reality as possible iv state diagrams advanced use example iv state diagrams advanced use example
notation iv information action types iv actions model the processing information objects in the system every action is associated with
at least one object actions cause state changes in the objects that participate in them the figure shows collection of
action types specified in the information viewpoint iv information action types notation iv actions specify investigation design specify measurement or
observation perform measurement or observation store data carry out backup final review publish data add metadata annotate metadata register metadata
publish metadata query metadata build conceptual models setup mapping rules annotate data annotate action resolve annotation perform mapping do data
mining query data assign unique identifier check quality track provenance process data describe service specify investigation design specify design of
investigation including sampling design geographical position of measurement or observation site the selections of observations and measurement sites can be
statistical or stratified by domain knowledge characteristics of site preconditions of measurements specify measurement or observation specify the details of
the method of observations measurements for example it may include the specification of measurement device type and its settings measurement
observation intervals perform measurement or observation measure parameter or observe an event the performance of measurement or observation produces measurement
results store data archive or preserve data in persistent manner to ensure continued accessibility and usability carry out backup replicate
data to an additional data storage so it may be used to restore the original after data loss event long
term preservation is special type of backup final review review the data to be published which will not likely be
changed again the action triggers the change of the data state to be finally reviewed in practices an annotation for
such state change should be recorded for provenance purposes usually this is coupled with archiving and versioning actions publish data
make data public accessible for example this can be done by presenting them in browsable form on the world wide
web by presenting them via special services restful service soap service open grid service ogc service web feature service web
map service sparql endpoint add metadata add additional data according to predefined schema metadata schema this partially overlaps with data
annotations annotate metadata link metadata with meaning concepts of predefined local or global conceptual models this can be done by
adding pointers from concepts within conceptual model to the metadata for instance if concepts are terms of skos thesaurus identified
by uris and published as linked data then annotation amounts to associating metadata with the terms uris register metadata enter
the metadata into metadata catalogue publish metadata make the registered metadata available to the public query metadata send request to
metadata resources to retrieve metadata of interests build conceptual models establish local or global model of interrelated concepts this may
involve the following issues commitment the agreement of larger group of scientists data providers data users should be achieved unambiguousness
the conceptual model should be unambiguously defined readability the model should be readable by both human and machine ontologies for
instance express the meaning of the concepts with the relations to other concepts while being human and machine readable recently
it has increasingly become important to add definitions in human readable language availability the conceptual model must be referenceable and
dereferenceable for long time setup mapping rules specify the mapping rules of data and or concepts these rules should be
explicitly expressed using language that can be processed by software minimal set of mapping rules should include the following data
source data concept for which the mapping is valid target data concept for which the mapping is valid mapping process
the translation and or transformation process validity constraints for the mapping temporal constraints context constraints etc annotate data annotate data
with meaning concepts of predefined local or global conceptual models in practices this can be done by adding tags or
pointer to concepts within conceptual model to the data if the concepts are terms in an skos rdf thesaurus and
published as linked data then data annotation would mean to enter the url of the term describing the meaning of
the data there is no exact borderline between metadata and semantic annotation annotate action perform annotation of an information object
resolve annotation retrieve the reference to the specific set of objects that correspond to set of annotation terms perform mapping
execute transformation rules for values mapping from one unit to another unit or translation rules for concepts translating the meaning
from one conceptual model to another conceptual model translating code lists do data mining execute sequence of metadata data request
interpret result do new request usually this sequence helps to deepen the knowledge about the data classically this sequence can
lead from data to metadata and semantic annotations follow the provenance of data can follow data processing it can be
supported by special software that helps to carry out that sequence of data request and interpretation of results query data
send request to data store to retrieve required data in practice there are two types of data query two step
approach step query search metadata step access data for example when using ogc services it usually first invokes web feature
service to obtain feature descriptions then web map service can be invoked to obtain map images one step approach to
query data by using sql services or sparql endpoints requests can be directly sent to service or distributed by broker
assign unique identifier obtain unique identifier and associate it to the data check quality actions to verify the quality of
data for example it may involve remove noise remove apparently wrong data calculate calibrations quality checks can be carried out
at different points in the chain of data lifecycle quality checks can be supported by software tools for those processes
which can be automated statistic tolerance checks track provenance automatically generate and store metadata about the actions and the data
state changes as provenance instances process data process data for the purposes of converting and generating data products calculations statistical
processes simulation models visualisation alpha numerically graphically geographically data processes should be recorded as provenance instances describe service describe the
accessibility of service or processes which is available for reuse the interfaces the description of behavior and or implemented algorithms
iv information objects lifecycle the specification of the lifecycles of the information objects is described combining iv object instances at
different states and the sequences of allowed actions according to those states the set of models used for describing this
evolution are part of the dynamic schemata in odp the specification of information objects lifecycle is presented in two parts
lifecycle overview overview of information objects state changes as effects of actions lifecycle in detail detailed description of how information
objects changes at each phase of the data lifecycle iv lifecycle overview this section describes the alignment between data processing
in the ri systems and the data lifecycle using information objects and information actions the description is framed against the
phases of the research data lifecycle model the diagram shown on the right provides high level view of the data
lifecycle the rounded rectangles represent iv actions on data and the straight rectangles represent instances of iv objects at different
states the arrow lines link iv actions and iv objects as follows arrows leaving an action connect to iv objects
created by the action while arrows entering an action connect iv objects to actions applied on them the black circle
at the top of the diagram represents the starting point and the double circle at the bottom represents the end
point the types of diagrams used in this section are called activity diagrams uml data acquisition the data acquisition phase
encompasses the actions defined for the observation experimentation storage identification and storage of measurements observations raw data in the diagram
the acquisition phase is represented by the dataacquisition action which produces measurement result data object with the state raw data
curation the data curation phase encompasses the actions that support the long term preservation and use of research data the
main product of this set of actions is persistent data in stable state curated data in the diagram the curation
phase is represented by the datacuration action which produces persistent data object with the state curated iv information object evolution
data and the straight rectangles represent instances of iv objects at different states the arrow lines link iv actions and
iv objects as follows arrows leaving an action connect to iv objects created by the action while arrows entering an
action connect iv objects to actions applied on them the black circle at the top of the diagram represents the
starting point and the double circle at the bottom represents the end point the types of diagrams used in this
section are called activity diagrams uml data acquisition the data acquisition phase encompasses the actions defined for the observation experimentation
storage identification and storage of measurements observations raw data in the diagram the acquisition phase is represented by the dataacquisition
action which produces measurement result data object with the state raw data curation the data curation phase encompasses the actions
that support the long term preservation and use of research data the main product of this set of actions is
persistent data in stable state curated data in the diagram the curation phase is represented by the datacuration action which
produces persistent data object with the state curated iv information object evolution notation in the diagram each phase of the
data lifecycle is represented as an action which produces specific information object in this case the main information object shown
is persistent data the diagram also adds provenance tracking action provenance tracking is an action that can proceed in parallel
during all phases of the data lifecycle the overview of the data lifecycle phases is described as follows note data
curation includes preservation which may require data transformation for example media migration to digital form data publishing the data publishing
phase encompasses the actions that guaranty data access and discovery for entities people and systems outside the ri in the
diagram the publishing phase is represented by the datapublishing action which produces persistent data object with the state published data
processing the data processing phase encompasses the actions that support making use of the ri published data in the diagram
the processing phase is represented by the dataprocessing action which produces persistent data object with the state processed data use
the data use phase is bridge phase which sits between processing and acquisition in this phase the data is used
and may produce new data raw data which can in turn be persisted by an ri in the diagram the
usage phase is represented by the datause action which produces data product object with the state raw in the detailed
description section the actions in the diagram are expanded to present more detailed view of the data lifecycle from the
iv perspective data provenance tracking it is important to track state changes of information objects during their lifecyle as illustrated
in diagram above the provenancetracking action takes place in parallel to the phases of the lifecycle that change the state
of persistent data some of the states changes of information objects as effects of actions are summarised in the following
table as shown in the diagram the outputs of each transition in which new stable state is reached can be
used to produce provenance data for example provenance tracking service may record information objects being processed action types applied and
resulting objects the timestamps for the actions and some additional data and store that as provenance data simplified example of
some provenance tracking points information object applied action types resulting information objects data acquisition persistent data raw persistent data raw
data curation persistent data finallyreviewed metadata registered persistent data finallyreviewed data publishing persistent data published metadata registered metadata published persistent
data published data processing persistent data processed persistent data processed data use data product new form of persistent data raw
the citation of data referencing the actors of involved in production of the data is an example of the use
of data provenance correct interpretation of the data can also depend on reviewing the provenance for instance to ensure origin
of the data matches its intended use iv lifecycle in detail this section expands the overview of the alignment between
the information viewpoint and the data lifecycle the descriptions uses the information objects and information actions to greater extent providing
deeper insight into the processing of information objects by the ri the notation for the diagrams in this section is
as follows the rounded rectangles represent iv actions on data and the straight rectangles represent instances of information objects at
different stages the arrow lines link actions and objects as follows arrows leaving an action connect to iv objects created
by the action while arrows entering an action connect iv objects to actions using them data acquisition data curation data
publishing data processing data use data acquisition the data acquisition phase encompasses the actions defined for the observation experimentation storage
identification and backup of measurements observations raw data the following paragraphs explain the detailed diagram of how the iv actions
can be combined to support data acquisition specify investigation design before measurement or observation can be started the design or
setup must be defined including the working hypothesis and scientific question method of the selection of sites stratified random necessary
precision of the observation or measurement boundary conditions etc for correctly using the resulting data details about their processing and
the parameters defined have to be available if stratified selection of sites according to parameter is done the resulting value
of parameter can not be evaluated in the same way as other results specify measurement or observation after defining the
overall design of measurements or iv data acquisition notation note this example is provided for illustrative purposes the example shows
one of many alternatives for performing data acquisition other iv actions and iv objects can be introduced at this stage
additional actions and objects not described in the iv of the envri rm can also be incorporated observations the measurement
method complying with the design including devices which should be used standards protocols which should be followed and other details
have to be specified the details of the process and the parameters used have to preserved to guarantee correct interpretation
of the resulting data when modelling dependency of parameter of parallel measured wind velocity the limit of detection of the
used anemometer influences the range of values of possible assertions perform measurement or observation after the measurement or observation method
is defined the experiment can be performed producing measurement result which is form of persistent data in raw state store
data the measurement result data is stored this action can be very simple when using measurement device which periodically sends
the data to the data management system but this can also be sophisticated harvesting process or in case of biodiversity
observations process done by humans the storage process is the first step in the lifecycle of data that makes data
accessible in digital form data curation once data is stored the next phase of the data lifecycle is data curation
data curation the data curation phase encompasses the actions that support the the long term preservation and use of research
data the main product of this set of actions is persistent data in stable state annotated data the following paragraphs
explain the detailed diagram of how the iv actions can be combined to support data curation data acquisition the first
action is data acquisition the phase of the data lifecycle that precedes data curation this action produces three iv objects
persistentdata specificationofmeasurementsorobservations and specificationofinvestigationdesign carry out backup as soon as data are available to the ri iv data curation notation
note this example is provided for illustrative purposes the example shows one of many alternatives for performing data curation other
iv actions and iv objects can be introduced at this stage for instance check quality register metadata or publish metadata
actions and objects not described in the iv of the envri rm can also be incorporated backup can be made
independently of the state of the persisted data this can be done locally or remotely by the data owners or
by dedicated data archiving centres assign unique identifier data needs to be uniquely identified for correct retrieval and processing the
unique identifier can be local to the ri or global to be used from outside the ri as such it
can be simple numerical value assigned by the ri dbms or specific pid assigned following the standards of an external
pid provider add metadata this action uses the specifications of investigation and measurements to facilitate the understanding of the associated
persistent data object in addition to this data the ri can add timestamps and other identification data as metadata once
the data is correctly stored and identified and the corresponding metadata has been also created persistent data can be linked
to metadata annotate data data is further enriched with additional metadata which can correspond to specific ontology for the research
field annotate metadata metadata can also be further enriched with additional metadata which can correspond to specific ontology for the
research field build conceptual model the building of local conceptual model mirrors the wider research community efforts to build global
conceptual model in this set of activities concept are added to the local conceptual model of the ri the conceptual
model is made of the composition of concepts which are used to help people know understand or simulate subject the
model represents the pairing of data and metadata using semantic annotations creates local concept new metadata object and changes the
state of the persistent data object to annotated global conceptual models are ontologies thesauri dictionaries or hierarchies built by larger
communities than single ri such as gemet dolce sweet this action normally happens outside of the ri main activities trough
feedback mechanisms ris participate in the creation of global conceptual models while developing their own models data publishing once data
have been curated the next phase of the data lifecycle is data publishing data publishing the data publishing phase encompasses
the actions that make the data available for entities people and systems outside the ri the following paragraphs explain the
detailed diagram of how the iv actions can be combined to support data curation note this example is provided for
illustrative purposes the example shows one of many alternatives for performing data curation other iv actions and iv objects can
be introduced at this stage for instance qualityassurance actions and objects not described in the iv of the envri rm
can also be incorporated data curation the first action is data curation the phase of the data lifecycle that precedes
data publishing this action produces four iv objects persistentdata localconceptualmodel localconcept and metadata finally review data persistent data that is
in the process of publishing needs to be reviewed before proceeding to publising it is important to clearly specify what
the finallyreviewed state means in some ris it can mean that those data will never change again the optimum for
the outside user for other ris it might also mean that only under certain circumstances those data will be changed
in this case it is important to know what certain circumstances mean build global conceptual model the construction of global
conceptual model makes sure that there is an appropriate fit between the persistent data to be published and their metadata
including the local conceptual model with other models existing outside the ri the globalconceptualmodel is the representation of how that
outside world looks to the ri iv data publishing notation semantic harmonisation unifies data and knowledge models based on the
consensus of collaborative domain experts to achieve better data knowledge reuse and semantic interoperability this complex activity is performed in
two stages setup mapping rules and perform mapping defined as follows setup mapping rule the global model is used to
generate set of mapping rules to enable linking the ri data and metadata to global semantics this may include simple
conversions such as conversions of units but may also imply more sophisticated transformations like transformations of code lists descriptions measurement
descriptions and data provenance perform mapping this action carries out the linking of data and metadata to one or more
global models publish data mapped data is made available to the outside world the pid is the main identifier of
the data but the data can also be located by querying metadata publish metadata metadata is also mapped and published
to enable more sophisticated data querying data processing once data have been published the next phase of the data lifecycle
is data processing data can be made directly accessible or indirectly direct access means that data request to data server
query data gets the data or an error message as answer indirect access means initially accessing metadata query metadata searching
for fitting data set and then querying on the resulting data set those two steps can be extended further when
intermediate steps are involved the multi step approach is often used for data which are not open making metadata open
but not data itself for queries touching several data sets and or filtering the data like give me all nox
air measurement where o3 exceeds level of ppb the multy step approach can be seen blocker data processing the data
processing phase encompasses the actions that support making use of the ri published data the following paragraphs explain the detailed
diagram of how the iv actions can be combined to support data curation note this example is provided for illustrative
purposes the example shows one of many alternatives for performing data processing other iv actions and iv objects can be
introduced at this stage actions and objects not described in the iv of the envri rm can also be incorporated
data publishing the first action is data publishing the phase of the data lifecycle that precedes data processing this action
produces three iv objects persistentdata globalconceptualmodel and metadata which are used in the data processing phase to access and process
data provenance tracking is the action that keeps log about the the actions and the data state changes as data
evolves through the ri systems the resulting provenance data is form of metadata which may be of interest for referencing
and citing the use of data within and outside the ri query data this action requests specific persisted data from
the ri do data mining this action implies the execution of sequence of metadata data request interpret result request which
automatically produce or find patterns in the data being analysed usually this sequence helps to deepen the knowledge about the
data resolve annotation this action implies finding specific data set from set of semantic annotation and constrains on those annotations
if the annotation is resolved the result should be link or set of links to specific data sets if not
the result is an empty set query metadata this action requests specific persisted data from the ri using metadata as
additional parameters for narrowing down the search query provenance this action requests specific persisted data about the provenance of some
data or metadata this is usually done to determine the origin and validity of data but can also be helpful
for citation and referencing process data the performance of any of the five actions listed before is automatically detected as
form of data processing by the ri system this should result in changing the state of the data to processed
the processed state can mean several things such as the data has been consulted the data has been referenced the
data has been downloaded the data has been used as input for an external process etc data use once data
have been processed the next phase of the data lifecycle is data use which to some extent overlaps with processing
provenance tracking as described in the overview the provenance tracking action tracks all changes in the states of persistent data
this is an important action which has wide use inside and outside the ri data use the data use phase
is bridge phase which sits between processing and acquisition in this phase the data is used and may produce new
data raw data which can in turn be persisted by an ri the actions that act on data at this
point can be iv data processing notation provided by same ri exposing the data or by external entities ris or
other in the use phase the ri system is open to the outside world users persons or external systems can
use the services provided to produce new data products data publishing the first action is data publishing the phase of
the data lifecycle that precedes data processing this action produces three iv objects persistentdata globalconceptualmodel and metadata which are used
in the data use phase to access and process data provenance tracking provenance tracking keeps log about the the actions
and the data state changes as data evolves through the ri systems the resulting provenance data is form of metadata
which may be of interest for referencing and citing the use of data within and outside the ri iv data
use provided by same ri exposing the data or by external entities ris or other in the use phase the
ri system is open to the outside world users persons or external systems can use the services provided to produce
new data products data publishing the first action is data publishing the phase of the data lifecycle that precedes data
processing this action produces three iv objects persistentdata globalconceptualmodel and metadata which are used in the data use phase to
access and process data provenance tracking provenance tracking keeps log about the the actions and the data state changes as
data evolves through the ri systems the resulting provenance data is form of metadata which may be of interest for
referencing and citing the use of data within and outside the ri iv data use notation note this example is
provided for illustrative purposes the example shows one of many alternatives for performing data processing other iv actions and iv
objects can be introduced at this stage actions and objects not described in the iv of the envri rm can
also be incorporated in the diagram and associated descriptions below cite data convert data produce model and visualise data are
some examples of these types of actions data processing data processing produces persistent data iv objects query data this action
requests specific persisted data from the ri do data mining this action implies the execution of sequence of metadata data
request interpret result request which automatically produce or find patterns in the data being analysed usually this sequence helps to
deepen the knowledge about the data resolve annotation this action implies finding specific data set from set of semantic annotation
and constrains on those annotations if the annotation is resolved the result should be link or set of links to
specific data sets if not the result is an empty set query metadata this action requests specific persisted data from
the ri using metadata as additional parameters for narrowing down the search query provenance this action requests specific persisted data
about the provenance of some data or metadata this is usually done to determine the origin and validity of data
but can also be helpful for citation and referencing cite data produce reference to persistent data or metadata convert data
converting and generating data products for instance translating to different format produce model creation of statistical models simulation models or
summaries with the data provided visualise data creating visual models which display data alpha numerically graphically or geographically data acquisition
use of data has the potential for creating data products which may need to be persisted re initiating the data
lifecycle for this reason the the last action after data use actions is data acquisition iv information management constraints the
iv of the envri rm provides the means for specifying constrains which describe the set of rules governing data management
the set of models used for describing constraints are part of the static schemata in odp in the envri rm
information management constraints establish mechanisms to avoid loss of data around measurements and observations provide information about the meaning of
data make data and metadata available for external use the iv of the envri rm provides three types of management
constraints data collection constraints data integration constraints data publication constraints data collection constraints the constraints applied to data collection are
illustrated in the figure below the application of these constrains helps to avoid data loss or wrong interpretation observing these
three rules together ensures that data can be correctly interpreted and reduces the risk of data loss this is because
the rules guarantee that the original data can be retrieved and interpreted correctly though the lifetime of the information objects
derived from them the rules guarantee the availability of the rationale for collecting 1st rule the details about the how
collection proceeded 2nd rule and the original data collected iv data collection constraints data integration constraints the constraints for data
integration are illustrated in the following figure data integration constraints support the correct interpretation of data helping external data users
correctly interpret and map the semantics of data the observation of these rules makes possible integrating data within ri and
the outside world this requires adding special type of metadata local conceptual model and then mapping the data to global
conceptual model mapping data to global semantics may include simple tasks such as conversions of units but can also need
sophisticated transformations such as code lists cross referencing or measurement descriptions and data provenance data integration constraints data publication constraints
constraints for data publication are illustrated in the following figure the constrains specify conditions necessary for preparing the data to
be publicly accessed data publication constraints data publication constraints computational viewpoint research infrastructure ri provides context in which investigators can
interact with scientific data in principled manner to provide this context an ri must support portfolio of possible research interactions
these interactions can be realised by binding together different services via standard operational interfaces the computational viewpoint cv accounts for
the major computational objects that can be found within an environmental research infrastructure as well as the interfaces by which
they can be invoked and by which they can invoke other objects in the infrastructure each object encapsulates functionality that
should be implemented by service or other resource in compliant ri binding of computational objects together via compatible interfaces creates
network of interactions that allows an ri to support the data related activities of its target research community the description
of the cv is divided in three parts objects supp ort of data lifecycle and integration points objects present computational
objects according generic architecture of the ris subsystems presents examples of how components are integrated for supporting the data lifecycle
into five different subsystems integration points defined to support the movement of research data between phases note before proceeding the
reader may wish to study the pages on how to read the computational viewpoint and how to use the computational
viewpoint cv objects the archetype of modern environmental research infrastructure has brokered service oriented architecture core functionality is encapsulated within
number of key resources which can be accessed by means of externally facing gateway services interaction by external agents with
internal resources is overseen by one or more brokers often closely integrated with the gateway charged with validating requests and
providing where needed an interoperability layer between otherwise heterogeneous components the computational viewpoint of the envri rm the computational viewpoint
defines computational objects cv object and interfaces cv interfaces which enable their interaction the diagram below shows the main elements
of the cv and their relationships each ellipse contains concept the arrows connecting the concepts are directed and indicate the
relationship between concepts the label of the link indicates the type of relationship from this the diagram indicates that cv
object provides cv interface as indicated by the pro vides relationship similarly cv object can create another cv object as
indicated by the caninstantiate elationship in this same way cv interface can fit another cv interface this is indicated the
fits relationsh ip computation viewpoint components and their relationships provides set of models which can help in the design implementation
maintenance and evolution of the systems and services that ris provide for accessing data the cv prescribes number of types
of computational object for which there should be instances present in or around research infrastructure in order to ensure that
particular key functions are supported the grouping of cv objects into sets is based on the software architecture that is
expected to be implemented when providing access to data and other related resources during the research data lifecycle evident in
the ris analysed as part of the envri and envriplus projects consequently the presentation of cv objects is arranged as
five sets corresponding to each of the architectural layers of ri systems note however that this should not be read
as prescriptive and that other groupings are possible presentation objects computational objects that facilitate access to ris by human users
broker objects computational objects that act as intermediaries for access to data held within the data store and facilitate performing
semantic interpretation and routing of queries service objects computational objects that offer programmatic access to distributed systems and resources internal
and external component objects computational objects that provide access to back end objects back end objects computational objects that encompass
the ri systems and resources for accessing research data and derived data products the set of cv components included in
the envri rm comprises the computational objects that are common to many ris the set is not closed so each
ri can include the additional components they require to completely model their systems the set does not contain compulsory items
so each ri can exclude objects and interfaces that are not relevant for them note before proceeding the reader may
wish to study the pages on andhow to read the computational viewpoint how to use the computational viewpoint cv objects
notation cv presentation objects cv presentation objects are the entry points for human users to the systems and services provided
to access research data and their derived products in the envri rm complex interactions between the components facilitating data use
and other components are mediated by virtua laboratories these objects are deployed by science gateways in order to provide persistent
context for such interactions between groups of users and components within the ri the reference model recognises the following specific
sub classes of laboratory field laboratories so named because they interact with raw data sources in the field are used
to interact with the data acquisition components allowing researchers to deploy calibrate and un deploy instruments as part of the
integrated data acquisition network used by an infrastructure to collect its primary raw data field laboratories have the ability to
instantiate new instrumen controllers from the data acquisition set experiment laboratories are used to interact both with curated data and
data processing facilities allowing researchers to deploy datasets for processing and acquire results from computational experimentation semantic laboratories are used
to interact with the semantic models used by research infrastructure to interpret datasets and characteristic meta data cv presentation objects
notation regardless of provenance all laboratories must interact with aa ai service in order to authorise requests and authenticate users
of the laboratory before they can proceed with any privileged activities science gateway community portal for interacting with an infrastructure
science gateway object encapsulates the functions required to interact with research infrastructure from outside with the objects provided for data
acquisition data curation data brokering and data processing science gateway should be able to provide virtual laboratories for authorised agents
to interact with and possibly configure many of the science functions of research infrastructure science gateway is also known as
virtual research environment science gateway object can instantiate any number of virtual laboratory objects virtual laboratory community proxy for interacting
with ri components virtual laboratory object encapsulates interaction between user or group of users and subset of the science functions
provided by research infrastructure its role is to bind aaai service with potentially any number of other infrastructure objects virtual
laboratory object must provide at least one interface authorise action client is used to retrieve authorisation for any restricted interactions
with the data acquisition components specific sub classes of virtual laboratory should be defined to interact with the infrastructure in
different ways the envri rm defines the field laboratory object for interaction with the data acquisition components field laboratory community
proxy for interacting with data acquisition instruments sub class of virtual laboratory object encapsulating the functions required to access calibrate
deploy or withhold instruments during the data acquisition phase field laboratory is created by science gateway in order to allow
researchers in the field to interact with the data acquisition objects deployment of an instrument entails the deployment of an
instrument controller by which the instrument can be interacted with field laboratory object can instantiate any number of instrument controller
objects field laboratory should provide at least two operational interfaces in addition to those provided by any virtual laboratory calibrate
instrument client is used to calibrate the reading of data by instruments based in principle on scientific analysis of data
output this interface can also be used to monitor activity on given instrument update registry client is used to register
and or withdraw instruments used for data acquisition the degree of freedom with which field laboratory interacts with other data
acquisition objects is contingent on the nature of the research infrastructure and policed by object as defined for all user
laboratories aaai service experiment laboratory community proxy for conducting experiments within research infrastructure sub class of virtual laboratory object encapsulating
the functions required to schedule the processing of curated and user provided data in order to perform some task analysis
data mining modelling simulation etc an experiment laboratory is created by science gateway to allow researchers interaction with data held
by research infrastructure in order to achieve some scientific output an experiment laboratory should provide at least three operational interfaces
data request client is used to make requests of the research infrastructure pertaining to curated datasets process request client is
used to make requests of the research infrastructure pertaining to data processing translate request client is used to invoke semantic
broker where some mapping between different semantic domains is deemed necessary semantic laboratory community proxy for interacting with semantic models
sub class of virtual laboratory object encapsulating the functions required to update semantic models such as ontologies used in the
interpretation of curated data and infrastructure metadata semantic laboratory is created by science gateway in order to allow researchers to
provide input on the interpretation of data gathered by research infrastructure semantic laboratory should provide at least one operational interface
in addition to those provided by any virtual laboratory update model client is used to update semantic models associated with
research infrastructure semantic data request client is used to make requests of the research infrastructure about metadata and annotations referring
to data stored by the data set cv broker objects broker objects act as intermediaries for access to data held
within the data store and facilitate performing semantic interpretation and routing of queries for this brokers keep registries of service
objects to which actions are routed whenever possible advanced brokers which make use of metadata should be preferred to hard
coded brokers data broker objects act as intermediaries for access to data held within the data store semantic broker objects
perform semantic interpretation brokers are responsible for verifying the agents making access requests and for validating those requests these brokers
can be interacted with directly via virtual laboratories such as experime cv broker objects notation nt laboratories for general interaction
with data and processing services and semantic laboratories by which the community can update semantic models associated with the research
infrastructure data broker broker for facilitating data access upload requests data broker object intercedes between the data publishing objects and
the data curation objects collecting the computational functions required to negotiate data transfer and query requests directed at data curation
services on behalf of some user it is the responsibility of the data broker to validate all requests and to
verify the identity and access privileges of agents making requests it is not permitted for an outside agency or service
to access the data stores within research infrastructure by any means other than via data broker data brokers are not
responsible for brokering the collection of raw data from the data acquisition objects as this is handled more efficiently by
an acquisition service data broker should provide four operational interfaces data request server provides functions for requesting the import or
export of datasets the querying of data or the annotation of data within research infrastructure annotate data client is used
to request annotation of data held within the data curation objects of research infrastructure prepare data transfer client is used
to negotiate data transfers with the data curation objects of research infrastructure query data client is used to forward queries
onto the data curation objects of research infrastructure and receive the results semantic broker broker for establishing semantic links between
concepts and bridging queries between semantic domains semantic broker intercedes where queries within one semantic domain need to be translated
into another to be able to interact with curated data it also provides the functionalities required to update the semantic
models used by an infrastructure to describe data held within semantic broker should provide two operational interfaces translate request server
provides functions for translating requests between two semantic domains update model server provides functions for updating semantic models associated with
research infrastructure cv service objects cv service objects offer programmatic access to distributed systems and resources internal and external this
allows building ris using both internal and external sourced components the service layer includes the main services that enable data
access processing and transformation used in different phases of the research data lifecycle the acquisition services responsible for ensuring that
any data is delivered into the infrastructure in accordance with current policies the annotation service concerned with the updating of
records such as datasets and catalogues in response to user annotation requests the aaai service handles authorisation requests and authentication
of users before they can proceed with any privileged activities the catalogue service concerned with the cataloguing of metadata and
other characteristic data associated with datasets stored within the infrastructure the coordination service delegates all processing tasks sent to particular
execution resources coordinates multi stage workflows and initiates execution the data transfer service concerned with the movement of data into
and out of the infrastructure the pid service provides globally readable persistent identifiers pids to infrastructure entities mainly datasets that
may be cited by the community cv service objects notation acquisition service oversight service for integrated data acquisition an acquisition
service object encapsulates the computational functions required to monitor and manage network of instruments an acquisition service can translate acquisition
requests into sets of individual instrument configuration operations as appropriate an acquisition service should provide at least three operational interfaces
update registry server provides functions for registering and deregistering instruments within the data acquisition phase configure controller client is used
to configure data collection and other configurable factors on individual instruments prepare data transfer client is used to negotiate data
transfers to data curation objects annotation service oversight service for adding and updating records attached to curated datasets an annotation
service object collects the functions required to annotate datasets and collect observations that can be associated with the various types
of data managed within research infrastructure an annotation service should provide three operational interfaces annotate data server provides functions for
requesting the annotation of existing datasets or the creation of additional records such as qualitative observations made by researchers update
catalogues client is used to update catalogues or catalogue information managed by catalogue service update records client is used to
update annotation records of existing datasets curated within one or more data stores aaai service oversight service for authentication authorisation
and accounting of user requests to the infrastructure an aaai service object encapsulates the functions required to authenticate agents authorise
any requests they make to services within research infrastructure and track their actions generally any interaction occurring via science gateway
object or virtual laboratory object will only proceed after suitable transaction with an aaai service object has been made an
aaai service should provide at least one operational interface authorise action server provides functions to verify and validate proposed actions
providing authorisation tokens for example where required catalogue service oversight service for cataloguing curated datasets catalogue service object collects the
functions required to manage the construction and maintenance of catalogues of metadata or other characteristic data associated with datasets including
provenance and persistent identifiers stored within data stores registered data catalogue is itself dataset and can therefore be accessed and
queried exactly as any other dataset catalogue service should provide four operational interfaces export metadata server provides functions for gathering
metadata to be exported with datasets extracted from the data curation store objects data stores query data server provides functions
for querying data held by the infrastructure including the retrieval of datasets associated with given persistent identifier update catalogues server
provides functions for harvesting meta data from datasets in order to derive or update data catalogues query resource client is
used to retrieve data from data stores coordination service oversight service for data processing tasks deployed on infrastructure execution resources
coordination service should provide at least three operational interfaces process request server provides functions for scheduling the execution of data
processing tasks this could require executing complex workflows involving many parallel sub tasks coordinate process client is used to coordinate
the execution of data processing tasks on execution resources presented by process controllers prepare data transfer client is used to
move data into and out of the data store objects in order to register new results or in preparation for
the generation of such results data transfer service oversight service for the transfer of data into and out of the
data store objects data transfer service object encapsulates the functions required to integrate new data into the ri and export
that integrated data on demand the data transfer service is responsible for setting up data transfers including any repackaging of
datasets necessary prior to delivery data transfer object can create any number of new data transporter objects the actual coordination
of data transfers is handled by data transporter objects the data transfer service is responsible for specifying the behaviour of
given transporter data transfer service should provide one operational interface prepare data transfer server provides functions for negotiating and scheduling
data transfer either into or out of the data stores of ri pid service external service for persistent identifier assignment
and resolution persistent identifiers are generated by global service generally provided by an outside entity supported by the research community
pid persistent identifier service object encapsulates this service and is responsible for providing identifiers for all entities that require them
different versions of artefacts where maintained separately are assumed to have different identifiers but those identifiers can share common root
such that the family of versions of given artefact can be retrieved in one transaction or only the most recent
or otherwise dominant version is returned pid service should provide at least two operational interfaces acquire identifier server provides persistent
identifier for given entity resolve identifier server resolves identifiers referring agents to the identified entity in practice science gateway providing
access to the entity cv component objects cv component objects offer programmatic access to the actual ri systems and resources
the back end objects this allows providing intermediate fa ades for systems and resources which may be inter changed or
replaced as needed data store controllers provide access to data stores that may have their own internal data management regimes
instrument controllers encapsulate the accessible functionalities of instruments and other raw data sources out in the field process controllers represent
the computational functionality of registered execution resources data transporters are provided for managing the movement of data from one part
of research infrastructure to another raw data collectors manage the movement of data from one or more data acquisition objects
to one or more data store objects data importers manage the movement of data from external sources such as user
originated datasets and derived datasets from data processing to one or more cv component objects notation data stores objects data
exporters manage the movement of data from one or more data store objects to external destinations such as user machine
or downstream service gathering data from the research infrastructure data store controller data store supporting data preservation data stores record
data collected by the infrastructure providing the infrastructure primary resources to its community data store controller encapsulates the functions required
to store and maintain datasets and other data artefacts produced within data store of the ri as well as to
provide access to authorised agents data store controller should provide three operational interfaces update records server provides functions for editing
data records within data store as well as preparing data store to ingest new data through its import stream interface
described below query resource server provides functions for querying the data held in data store retrieve data server provides functions
to negotiate the export of datasets from data store data store controller should provide two stream interfaces import data for
curation consumer receives data packaged for curation within the associated data store export curated data producer is used to deliver
data stored within the associated data store to another service or resource instrument controller an integrated raw data source an
instrument is considered computationally to be source of raw environmental data managed by an acquisition service an instrument controller object
encapsulates the computational functions required to calibrate and acquire data from an instrument instrument is logical entity and may to
multiple physical entities deployed in the real world should they act in tandem sufficiently closely to justify being treated as
one data source any instrument represented by an instrument controller should however be considered independently configurable and monitorable from other
instruments managed by the same acquisition service an instrument controller should provide three operational interfaces calibrate instrument server provides functions
to calibrate the reading of data by an instrument if possible configure controller server provides functions to configure how and
when an instrument delivers data to data store retrieve data server provides functions to directly request data from an instrument
an instrument controller should provide at least one stream interface deliver raw data producer is used to deliver raw data
streams to designated data store process controller part of the execution platform that controls the deployment of processing components and
the assignment of processing tasks process controller object encapsulates the functions required for using an execution resource generically any computing
platform that can host some process as part of any infrastructure workflow process controller should provide at least three operational
interfaces coordinate process server provides functions for controlling the execution resource associated with given process controller retrieve data server provides
functions for retrieving data from an execution resource update records server provides functions for modifying data on an execution resource
including preparing the resource for the ingestion of bulk data delivered through its stage data stream interface process controller should
provide at least two stream interfaces stage data consumer is used to acquire data sent from the data store objects
of research infrastructure needed as part of some process deliver dataset producer is used to deliver any new data produced
for integration into the data curation store objects of research infrastructure data transporter generic binding object for data transfer interactions
data transporter binding object encapsulates the coordination logic required to deliver data into and out of the data stores of
ri data transporter object is created whenever data is to be streamed from one locale to another data transporter is
configured based on the data transfer to be performed but must have at least the following two interfaces update records
client is used to inform downstream resources about impending data transfers retrieve data client is used to request data from
given data source raw data collector binding object for raw data collection sub class of data transporter binding object encapsulating
the functions required to move and package raw data collected by acquisition objects raw data collector should provide at least
two operational interfaces in addition to those provided by any data transporter acquire identifier client is used to request new
persistent identifier to be associated with the data being transferred generally identifiers are requested when importing new data into an
infrastructure update catalogues client is used to update or initiate the update of data catalogues used to describe the data
held within an infrastructure to account for new datasets raw data collector must also provide two stream interfaces through which
to pass data deliver raw data consumer is used to collect raw data sent by instruments data acquisition objects import
data for curation producer is used to deliver repackaged raw data to data store objects data importer binding object for
importing external datasets sub class of data transporter binding object encapsulating the functions required to move and package external datasets
from outside sources into the ri data importer should provide at least two operational interfaces in addition to those provided
by any data transporter acquire identifier client is used to request new persistent identifier to be associated with the data
being transferred generally identifiers are requested when importing new data into an infrastructure update catalogues client is used to update
or initiate the update of data catalogues used to describe the data held within an infrastructure to account for new
datasets data importer must also provide two stream interfaces through which to pass data deliver dataset consumer is used to
retrieve external datasets stored in external data stores outside of the ri import data for curation producer is used to
deliver repackaged datasets to one or more data stores within the ri data exporter binding object for exporting curated datasets
sub class of data transporter binding object encapsulating the functions required to move and package curated datasets from the data
curation objects to an outside destination data exporter should provide at least one operational interface in addition to those provided
by any data transporter export metadata client is used to retrieve any additional metadata to be associated with the data
being transferred generally metadata is exported alongside datasets being exported from the infrastructure where data is repackaged to be more
self describing data exporter must also provide two stream interfaces through which to pass data export curated data consumer is
used to retrieve curated datasets stored within data stores deliver dataset producer is used to deliver repackaged curated data to
designated external data store outside of the ri cv back end objects back end objects are computational objects which encompass
the ri systems and resources provided for acquiring preserving publishing and processing research data and derived data products sensor network
is network consisting of distributed sensors which monitor physical or environmental conditions sensor is converter that measures physical quantity and
converts it into signal which can be read by an observer or by an electronic instrument storage system is systems
that manages the storage and retrieval of data and metadata file management system is storage systems that manages the storage
and retrieval of data as files in computer system database management system is storage systems that manages the storage and
retrieval of data and metadata into logically structured repositories service registry is an information system for registering services cv back
end objects notation cv objects and subsystems the include five subsystem roles whichscience viewpoint rolessupport each of the phases of
the data lifecycle in this section the models of those subsystems are developed further using computational viewpoint components the five
subsystems defined are data acquisition data curation data publishing data processing data use cv ri subsystems notation note before proceeding
the reader may wish to study the pages on andhow to read the computational viewpoint how to use the computational
viewpoint cv data acquisition the basis for environmental research is the observation and measurement of environmental phenomena the archetypical environmental
research infrastructure provides access to data harvested from an extended network of sensors instruments and other contributors deployed in the
field the following examples present the acquisition of data from instruments and from external data sources data acquisition from sensors
the diagram shows the organisation of five cv objects as part of an ri which are used for collecting data
from an instrument the instrument controller could be simple device collecting data from single sensor or complex device managing the
collection of data for sensor network data acquisition subsystem collect data notation acquisition is manipulated via field laboratories community proxies
by which authorised agents can add and remove instruments from the network by registering and de registering instrument controllers as
well as calibrate instrument readings where applicable in accordance with current community best practice data acquisition is computationally described as
set of instrument controllers encapsulating the accessible functionalities of instruments and other raw data sources out in the field monitored
and managed by one or more acquisition services responsible for ensuring that any data is delivered into the infrastructure in
accordance with current policies acquisition services invoke data transfer services which instantiate the appropriate raw data collector which retrieves data
from the instrument controller the four unlinked interfaces of the raw data collector will be linked to appropriate objects of
the data curation subsyste data acquisition from external resources the diagram shows the organisation of six cv objects which are
used for collecting data from an external resource the external resource could be another ri user uploaded data or public
data store the external resource could also be an interface for user observations provided by the ri for instance for
citizen observers data acquisition subsystem import data data acquisition subsystem import data notation the six components used to model data
acquisition from external resources the external resource is data source not necessarily integrated into the infrastructure providing data to data
stores acquisition is manipulated via virtual laboratory community proxy by which authorised agents can submit data to the ri the
virtuallaboratory invokes aaai service to retrieve the appropriate credentials for accessing the external resource data exporter and the internal data
importer after obtaining the credentials the virtual laboratory invokes data broker which in turn contacts data transferservices which instantiates the
appropriate data exporter and data importer objects and coordinates the transfer of data the four unlinked interfaces of the data
importer will be linked to appropriate objects of the data curation subsystem cv data curation one of the primary responsibilities
of an environmental research infrastructure is the curation of the significant corpus of acquired data and derived results harvested from
the data acquisition phase of the data lifecycle data processing and community contributions scientific data must be collected catalogued and
made accessible to all authorised users the accessibility requirement in particular dictates that infrastructures provide facilities to ensure easy availability
of data generally by replication for optimised retrieval and failure tolerance publishing of persistent identifiers to aid discovery and cataloguing
aiding discovery and allowing more sophisticated requests to be made over the entirety of curated data the following examples present
two of the main functionalities of the data curation subsystem data preservation and data annotation data preservation the diagram shows
the organisation of five cv objects which participate in the preservation of research data the data transporter in the diagram
could be replaced by raw data collector or data importer object and the change would not affect the integrity of
the system consequently this configuration supports both types of data acquisition described in the data acquisition subsystem section in the
example there is no presentation object which implies that the data preservation process is automated the data transporter modelled in
the diagram is complex device which at the same time invokes the pid service the catalogue service and the data
store controller the pid service is invoked to acquire unique identifier for the incoming data set the catalogue service is
invoked to store the metadata associated with the incoming data set the data store controller is invoked to store the
incoming data set along with its persistent identifier and liked to its associated metadata data curation subsystem data preservation data
curation subsystem data preservation notation data annotation the diagram shows the organisation of five cv objects which participate in the
annotation of research data this task is carried with the oversight of user or on request from user this is
why the presentation object semantic laboratory is included the semantic laboratoryinvokes semantic broker which in turn invokes the annotation service
the annotation service provides two functionalities annotation and updating of the conceptual model both the annotation and conceptual model are
